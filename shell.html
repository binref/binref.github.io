<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.11.6" />
<title>the refinery.shell documentation</title>
<meta name="description" content="On a Mission to Refine Binaries" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/tomorrow-night-bright.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{background-color:#0D0D0D;color:#EEEEEE;font-size:16pt}@font-face{font-family:"FixedSysEx";src:local('Fixedsys Excelsior 3.01-L2'),local('Fixedsys Excelsior 3.01'),local('FixedSysEx'),url(FixedSysEx.ttf) format('truetype')}code,pre,body,html{font-family:FixedSysEx,monospace}b,strong{font-weight:normal}#content{padding:20px}#sidebar{padding:1vw;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #EEEEEE;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}hr{display:none}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#EE8080;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#EEEEEE}// .title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#BB4040}pre code{background:#000000;display:block;padding:1px 0px 4px 0px;line-height:100%}code{background:#000000;padding:1px 0px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#000000;border:0;border-top:1px solid #EEEEEE;border-bottom:1px solid #EEEEEE;margin:1em 0;padding:1ex;overflow-x:auto}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index ul{list-style-type:square;padding:0}// #index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 10px}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#000000;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#0D0D0D}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#EEEEEE;border-left:5px solid #EEEEEE;padding-left:1em}.inheritance em{font-style:normal}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1ch}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition.note,.admonition.info,.admonition.todo,.admonition.versionadded,.admonition.important,.admonition.tip,.admonition.hint{background:#054000}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#998800}.admonition.error,.admonition.danger,.admonition.caution{background:#300000}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:1vw}main{display:flex;flex-direction:row;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #EEEEEE;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>refinery.shell</code></h1>
</header>
<section id="section-intro">
<h1 id="shell-like-unit-interface">Shell-Like Unit Interface</h1>
<p>Any unit from the <code><a title="refinery" href="index.html">refinery</a></code> module can also be imported from this module. When imported from here,
the units are initialized differently: They can be given string arguments as they would receive on
the command line. For example:</p>
<pre><code>&gt;&gt;&gt; from refinery.shell import *
&gt;&gt;&gt; emit('ABC', 'DEF') [ pop('t') | xor('var:t') | pack('-R') ] | str
'575'
</code></pre>
<p>This especially gives easier access to the powerful <code><a title="refinery.lib.meta" href="lib/meta.html">refinery.lib.meta</a></code> variables and the entire
multibin format expressions, see <code><a title="refinery.lib.argformats" href="lib/argformats.html">refinery.lib.argformats</a></code>.</p>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/shell.py#L1-L50" class="git-link">Browse git</a>
</summary>
<pre><code class="python">&#34;&#34;&#34;
# Shell-Like Unit Interface

Any unit from the `refinery` module can also be imported from this module. When imported from here,
the units are initialized differently: They can be given string arguments as they would receive on
the command line. For example:

    &gt;&gt;&gt; from refinery.shell import *
    &gt;&gt;&gt; emit(&#39;ABC&#39;, &#39;DEF&#39;) [ pop(&#39;t&#39;) | xor(&#39;var:t&#39;) | pack(&#39;-R&#39;) ] | str
    &#39;575&#39;

This especially gives easier access to the powerful `refinery.lib.meta` variables and the entire
multibin format expressions, see `refinery.lib.argformats`.
&#34;&#34;&#34;
from __future__ import annotations

from functools import WRAPPER_ASSIGNMENTS, wraps

from refinery import __unit_loader__

with __unit_loader__:
    __all__ = sorted(__unit_loader__.units, key=lambda x: x.lower())


__WRAP_UPDATE = ()
__WRAP_ASSIGN = WRAPPER_ASSIGNMENTS + (&#39;__firstlineno__&#39;,)


class __pdoc3__:
    def __class_getitem__(cls, *_):
        return &#39;&#39;


def __getattr__(name):
    with __unit_loader__:
        unit = __unit_loader__.resolve(name)

    if unit is None:
        raise AttributeError(name)

    class _unit(unit):
        def __new__(cls, *args, **kwargs):
            return unit.assemble(*args, **kwargs)

    wrapped_unit = wraps(unit, updated=__WRAP_UPDATE, assigned=__WRAP_ASSIGN)(_unit)
    return wrapped_unit


def __dir__():
    return __all__</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Units</h2>
<dl>
<dt id="refinery.shell.a3x"><code class="flex name class">
<span>class <span class="ident">a3x</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts embedded resources from compiled AutoIt scripts and decompiles the embedded script
bytecode. The unit also works on compiled AutoIt executables.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/a3x.py#L942-L1034" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class a3x(PathExtractorUnit):
    &#34;&#34;&#34;
    Extracts embedded resources from compiled AutoIt scripts and decompiles the embedded script
    bytecode. The unit also works on compiled AutoIt executables.
    &#34;&#34;&#34;

    def unpack(self, data: bytearray):
        view = memoryview(data)
        cursor = 0
        errors: dict[int, Exception] = {}
        script_count = 0
        truncated: set[A3xRecord] = set()
        intact: set[A3xRecord] = set()

        def _package(records: Iterable[A3xRecord]) -&gt; Generator[UnpackResult]:
            for k, record in enumerate(records, 1):
                self.log_info(F&#39;record {k} type:&#39;, record.type)
                self.log_info(F&#39;record {k} path:&#39;, record.src_path)
                if record.path is None:
                    continue
                yield UnpackResult(
                    record.path,
                    record.extract,
                    srcpath=record.src_path,
                    created=record.created.isoformat(&#39; &#39;, &#39;seconds&#39;),
                    written=record.written.isoformat(&#39; &#39;, &#39;seconds&#39;),
                )

        while cursor &lt; len(view):
            self.log_debug(F&#39;searching at offset 0x{cursor:08X}&#39;)
            nc = data.find(A3xScript.MAGIC, cursor)
            if nc &gt;= 0:
                cursor = nc
            else:
                rp = data.find(A3xRecord.MAGIC, cursor) - A3xScript.WIDTH
                if rp &lt;= cursor:
                    break
                cursor = rp
            try:
                script = A3xScript(view[cursor:])
            except Exception as E:
                errors[cursor] = E
                cursor += 1
                continue
            else:
                valid = script.has_valid_magic()
                if valid:
                    _m = &#39;correct&#39;
                else:
                    _m = &#39;invalid&#39;
                if not script.body:
                    cursor += A3xScript.WIDTH
                    if not script.has_valid_magic():
                        cursor += len(A3xRecord.MAGIC)
                    continue
                if script.truncated:
                    _a = &#39;truncated&#39;
                    truncated.update(script.body)
                else:
                    script_count += 1
                    _a = &#39;intact&#39;
                    intact.update(script.body)
                self.log_info(
                    F&#39;{_a} script of type&#39;, script.type,
                    F&#39;and length 0x{len(script):08X}&#39;,
                    F&#39;with {len(script.body)} records and {_m} magic:&#39;,
                    script.magic
                )
                cursor += len(script)
                if script.truncated:
                    if not script.has_valid_magic():
                        cursor += len(A3xRecord.MAGIC)
                    continue

            yield from _package(script.body)

        remaining = truncated - intact
        if remaining:
            self.log_warn(&#39;emitting records from truncated scripts&#39;)
            yield from _package(remaining)
            return
        elif truncated:
            self.log_debug(&#39;good news: intact scripts contained all records from truncated scripts&#39;)
        if script_count == 0:
            error = None
            for offset, error in errors.items():
                self.log_warn(F&#39;error at offset 0x{offset:08X}:&#39;, error)
            if error:
                raise error

    @classmethod
    def handles(cls, data) -&gt; bool | None:
        return buffer_contains(data, A3xScript.MAGIC) or buffer_contains(data, A3xRecord.MAGIC)</code></pre>
</details>
</dd>
<dt id="refinery.shell.a85"><code class="flex name class">
<span>class <span class="ident">a85</span></span>
</code></dt>
<dd>
<section class="desc"><p>Ascii85 encoding and decoding, the predecessor variant of Base85 with a different alphabet.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/a85.py#L9-L24" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class a85(Unit):
    &#34;&#34;&#34;
    Ascii85 encoding and decoding, the predecessor variant of Base85 with a different alphabet.
    &#34;&#34;&#34;
    def reverse(self, data):
        return base64.a85encode(data)

    def process(self, data):
        if re.search(BR&#39;\s&#39;, data) is not None:
            data = re.sub(BR&#39;\s+&#39;, B&#39;&#39;, data)
        return base64.a85decode(data)

    @classmethod
    def handles(cls, data):
        from refinery.lib.patterns import formats
        return formats.a85s.value.bin.fullmatch(data) is not None</code></pre>
</details>
</dd>
<dt id="refinery.shell.add"><code class="flex name class">
<span>class <span class="ident">add</span></span>
<span>(</span><span>*argument, bigendian=False, blocksize=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Add the given argument to each block.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/blockwise/add.py#L6-L13" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class add(BinaryOperationWithAutoBlockAdjustment):
    &#34;&#34;&#34;
    Add the given argument to each block.
    &#34;&#34;&#34;
    @staticmethod
    def operate(a, b): return a + b
    @staticmethod
    def inplace(a, b): a += b</code></pre>
</details>
</dd>
<dt id="refinery.shell.adler32"><code class="flex name class">
<span>class <span class="ident">adler32</span></span>
<span>(</span><span>reps=1, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the Adler32 hash of the input data.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/hash/checksums.py#L19-L24" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class adler32(HashUnit):
    &#34;&#34;&#34;
    Returns the Adler32 hash of the input data.
    &#34;&#34;&#34;
    def _algorithm(self, data) -&gt; bytes:
        return zlib.adler32(data).to_bytes(4, &#39;big&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.aes"><code class="flex name class">
<span>class <span class="ident">aes</span></span>
<span>(</span><span>key, *, iv=b'', padding=None, mode=None, raw=False, little_endian=False, segment_size=0, tag=(), aad=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>AES encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/aes.py#L9-L12" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class aes(StandardBlockCipherUnit, cipher=PyCryptoFactoryWrapper(AES)):
    &#34;&#34;&#34;
    AES encryption and decryption.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="refinery.shell.alu"><code class="flex name class">
<span>class <span class="ident">alu</span></span>
<span>(</span><span>operator, *argument, seed=0, prologue='', epilogue='', inc=False, dec=False, cbc=False, ctr=False, bigendian=False, blocksize=1, precision=-1)</span>
</code></dt>
<dd>
<section class="desc"><p>The arithmetic-logical unit. It allows you to specify a custom Python expression where the following
variables are allowed:</p>
<ul>
<li>the variable <code>A</code>: same as <code>V[0]</code></li>
<li>the variable <code>B</code>: current block</li>
<li>the variable <code>E</code>: block value of encoded input (not changed after update)</li>
<li>the variable <code>N</code>: number of bytes in the input</li>
<li>the variable <code>K</code>: current index in the input</li>
<li>the variable <code>S</code>: the internal state value</li>
<li>the variable <code>V</code>: the vector of arguments</li>
<li>the variable <code>I</code>: function that casts to a signed int in current precision</li>
<li>the variable <code>U</code>: function that casts to unsigned int in current precision</li>
<li>the variable <code>R</code>: function; <code>R(x,4)</code> rotates x by 4 to the right</li>
<li>the variable <code>L</code>: function; <code>L(x,4)</code> rotates x by 4 to the left</li>
<li>the variable <code>M</code>: function; <code>M(x,8)</code> picks the lower 8 bits of x</li>
<li>the variable <code>X</code>: function that negates the bits of the input</li>
</ul>
<p>(The rotation operations are interpreted as shifts when arbitrary precision is used.)</p>
<p>Each block of the input is replaced by the value of this expression. Additionally, it is possible to
specify prologue and epilogue expressions which are used to update the state variable <code>S</code> before and
after the update of each block, respectively.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/blockwise/alu.py#L25-L198" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class alu(ArithmeticUnit):
    &#34;&#34;&#34;
    The arithmetic-logical unit. It allows you to specify a custom Python expression where the following
    variables are allowed:

    - the variable `A`: same as `V[0]`
    - the variable `B`: current block
    - the variable `E`: block value of encoded input (not changed after update)
    - the variable `N`: number of bytes in the input
    - the variable `K`: current index in the input
    - the variable `S`: the internal state value
    - the variable `V`: the vector of arguments
    - the variable `I`: function that casts to a signed int in current precision
    - the variable `U`: function that casts to unsigned int in current precision
    - the variable `R`: function; `R(x,4)` rotates x by 4 to the right
    - the variable `L`: function; `L(x,4)` rotates x by 4 to the left
    - the variable `M`: function; `M(x,8)` picks the lower 8 bits of x
    - the variable `X`: function that negates the bits of the input

    (The rotation operations are interpreted as shifts when arbitrary precision is used.)

    Each block of the input is replaced by the value of this expression. Additionally, it is possible to
    specify prologue and epilogue expressions which are used to update the state variable `S` before and
    after the update of each block, respectively.
    &#34;&#34;&#34;

    @staticmethod
    def _parse_op(definition, default=None):
        if definition:
            return definition
        elif not default:
            raise ValueError(&#39;No definition given&#39;)
        else:
            return default

    def __init__(
        self,
        operator: Param[str, Arg.String(help=&#39;A Python expression defining the operation.&#39;)],
        *argument,
        seed: Param[int | str, Arg.String(&#39;-s&#39;, help=(
            &#39;Optional seed value for the state variable S. The default is zero. This can be an expression &#39;
            &#39;involving the variable N.&#39;))] = 0,
        prologue: Param[str, Arg.String(&#39;-p&#39;, metavar=&#39;E&#39;, help=(
            &#39;Optional expression with which the state variable S is updated before a block is operated on.&#39;))] = &#39;&#39;,
        epilogue: Param[str, Arg.String(&#39;-e&#39;, metavar=&#39;E&#39;, group=&#39;EPI&#39;, help=(
            &#39;Optional expression with which the state variable S is updated after a block was operated on.&#39;))] = &#39;&#39;,
        inc: Param[bool, Arg.Switch(&#39;-I&#39;, group=&#39;EPI&#39;, help=&#39;equivalent to --epilogue=S+1&#39;)] = False,
        dec: Param[bool, Arg.Switch(&#39;-D&#39;, group=&#39;EPI&#39;, help=&#39;equivalent to --epilogue=S-1&#39;)] = False,
        cbc: Param[bool, Arg.Switch(&#39;-X&#39;, group=&#39;EPI&#39;, help=&#39;equivalent to --epilogue=(B)&#39;)] = False,
        ctr: Param[bool, Arg.Switch(&#39;-T&#39;, group=&#39;EPI&#39;, help=&#39;equivalent to --epilogue=S+B&#39;)] = False,
        bigendian=False, blocksize=1, precision=-1
    ):
        for flag, flag_is_set, expression in [
            (&#39;--cbc&#39;, cbc, &#39;(B)&#39;),
            (&#39;--inc&#39;, inc, &#39;S+1&#39;),
            (&#39;--dec&#39;, dec, &#39;S-1&#39;),
            (&#39;--ctr&#39;, ctr, &#39;S+B&#39;),
        ]:
            if flag_is_set:
                if epilogue:
                    raise ValueError(
                        F&#39;Ambiguous specification; epilogue was already set to {epilogue} &#39;
                        F&#39;when {flag} was parsed.&#39;)
                epilogue = expression

        self._index = IndexCounter()

        super().__init__(
            self._index,
            *argument,
            bigendian=bigendian,
            blocksize=blocksize,
            precision=precision,
            seed=seed,
            operator=self._parse_op(operator),
            prologue=self._parse_op(prologue, &#39;S&#39;),
            epilogue=self._parse_op(epilogue, &#39;S&#39;),
        )

    @property
    def _is_ecb(self):
        return not self.args.epilogue and not self.args.prologue

    def _fastblock(self, data):
        raise FastBlockError

    def process(self, data):
        context = dict(metavars(data))
        seed = self.args.seed
        fbits = self.fbits
        fmask = self.fmask

        self._index.init(self.fmask)

        def _expression(definition: str):
            return PythonExpression(definition, *&#39;IBEASMNVRLX&#39;, all_variables_allowed=True, mask=fmask)

        prologue = _expression(self.args.prologue).expression
        epilogue = _expression(self.args.epilogue).expression
        operator = _expression(self.args.operator).expression

        def cast_unsigned(n) -&gt; int:
            return int(n) &amp; fmask

        def cast_signed(n) -&gt; int:
            n = int(n) &amp; fmask
            if n &gt;&gt; (fbits - 1):
                return -((~n + 1) &amp; fmask)
            else:
                return n

        if fbits is INF:
            def rotate_r(n, k): return n &gt;&gt; k
            def rotate_l(n, k): return n &lt;&lt; k
        else:
            def rotate_r(n, k): return (n &gt;&gt; k) | (n &lt;&lt; (fbits - k)) &amp; fmask
            def rotate_l(n, k): return (n &lt;&lt; k) | (n &gt;&gt; (fbits - k)) &amp; fmask

        def negate_bits(n):
            return n ^ fmask

        def mask_to_bits(x, b):
            return x &amp; ((1 &lt;&lt; b) - 1)

        context.update(
            N=len(data),
            I=cast_signed,
            U=cast_unsigned,
            R=rotate_r,
            L=rotate_l,
            X=negate_bits,
            M=mask_to_bits,
        )
        args = [
            self._infinitize_argument(len(data), *self._argument_parse_hook(a))
            for a in self.args.argument]
        if args:
            args = [next(iter(a)) for a in args]
            context[&#39;A&#39;] = args[0]
            context[&#39;V&#39;] = args

        if isinstance(seed, str):
            seed = PythonExpression(seed, &#39;IAMNVRLX&#39;, constants=context, mask=fmask)
        if callable(seed):
            seed = seed(context, N=len(data))

        self._index.init(self.fmask)
        context.update(S=seed)

        def operate(block, index, *args):
            context.update(K=index, B=block, E=block, V=args)
            if args:
                context[&#39;A&#39;] = args[0]
            context[&#39;S&#39;] = eval(prologue, None, context)
            context[&#39;B&#39;] = eval(operator, None, context)
            context[&#39;S&#39;] = eval(epilogue, None, context)
            return context[&#39;B&#39;]

        placeholder = self.operate
        self.operate = operate

        try:
            result = super().process(data)
        finally:
            self.operate = placeholder

        return result

    @staticmethod
    def operate(block, index, *args):
        raise RuntimeError(&#39;This operate method cannot be called.&#39;)

    def inplace(self, block, *args) -&gt; None:
        super().inplace(block, *args)</code></pre>
</details>
</dd>
<dt id="refinery.shell.aplib"><code class="flex name class">
<span>class <span class="ident">aplib</span></span>
</code></dt>
<dd>
<section class="desc"><p>APLib compression and decompression.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/ap.py#L302-L327" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class aplib(Unit):
    &#34;&#34;&#34;
    APLib compression and decompression.
    &#34;&#34;&#34;

    def reverse(self, buf):
        return compressor(buf).compress()

    def process(self, buf):
        view = memoryview(buf)
        size = 0
        if view[:4] == B&#39;AP32&#39;:
            size = int.from_bytes(buf[4:8], &#39;little&#39;)
            if size &gt; 0x80:
                size = 0
            else:
                self.log_info(F&#39;detected aPLib header of size {size}&#39;)
        return decompressor(view[size:]).decompress()

    @classmethod
    def handles(cls, data):
        if len(data) &lt; 2:
            return False
        if data[:4] == B&#39;AP32&#39;:
            return True
        return None</code></pre>
</details>
</dd>
<dt id="refinery.shell.argon2id"><code class="flex name class">
<span>class <span class="ident">argon2id</span></span>
<span>(</span><span>size, salt, iter=1, jobs=1, cost=None, skey=None, more=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Implements Argon2id-based key derivation.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/keyderive/argon2id.py#L7-L43" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class argon2id(Unit):
    &#34;&#34;&#34;
    Implements Argon2id-based key derivation.
    &#34;&#34;&#34;

    def __init__(
        self,
        size: Param[int, Arg.Number(metavar=&#39;n&#39;, help=&#39;number of bytes to generate&#39;)],
        salt: Param[buf, Arg.Binary(metavar=&#39;S&#39;, help=&#39;salt bytes&#39;)],
        iter: Param[int, Arg.Number(metavar=&#39;t&#39;, help=&#39;number of iterations, defaults to {default}&#39;)] = 1,
        jobs: Param[int, Arg.Number(metavar=&#39;p&#39;, help=&#39;parallelism, defaults to {default}&#39;)] = 1,
        cost: Param[int, Arg.Number(metavar=&#39;m&#39;, help=&#39;memory cost in kibibytes, defaults to the minimum of 8192 per job.&#39;)] = None,
        skey: Param[buf, Arg.Binary(metavar=&#39;K&#39;, help=&#39;optional secret key&#39;)] = None,
        more: Param[buf, Arg.Binary(metavar=&#39;X&#39;, help=&#39;optional additional data&#39;)] = None,
    ):
        super().__init__(size=size, salt=salt, iter=iter, skey=skey, jobs=jobs, cost=cost, more=more)

    @Unit.Requires(&#39;cryptography&#39;, [&#39;default&#39;, &#39;extended&#39;])
    def _argon2id():
        from cryptography.hazmat.primitives.kdf.argon2 import Argon2id
        return Argon2id

    def process(self, data):
        m = self.args.cost
        p = self.args.jobs
        S = self.args.salt
        K = self.args.skey
        n = self.args.size
        X = self.args.more
        t = self.args.iter
        K = K and bytes(K) or None
        X = X and bytes(X) or None
        S = bytes(S)
        m = m or 8192 * p
        a2id = self._argon2id(
            salt=S, length=n, iterations=t, lanes=p, memory_cost=m, ad=X, secret=K)
        return a2id.derive(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.asm"><code class="flex name class">
<span>class <span class="ident">asm</span></span>
<span>(</span><span>mode='x32', *, count=None, until=None, no_address=False, no_hexdump=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Disassembles the input data using capstone and produces a human-readable disassembly listing.
It internally uses the <code><a title="refinery.opc" href="index.html#refinery.opc">opc</a></code> unit for this, which is an alternative option if you are
looking for more programmatic disassembly.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/sinks/asm.py#L9-L64" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class asm(opc):
    &#34;&#34;&#34;
    Disassembles the input data using capstone and produces a human-readable disassembly listing.
    It internally uses the `refinery.opc` unit for this, which is an alternative option if you are
    looking for more programmatic disassembly.
    &#34;&#34;&#34;
    def __init__(
        self, mode=&#39;x32&#39;, *, count=None, until=None,
        no_address: Param[bool, Arg.Switch(&#39;-A&#39;, help=&#39;Disable address display.&#39;)] = False,
        no_hexdump: Param[bool, Arg.Switch(&#39;-H&#39;, help=&#39;Disable opcodes hexdump.&#39;)] = False,
    ):
        super().__init__(
            mode=mode,
            nvar=&#39;_name&#39;,
            avar=&#39;_addr&#39;,
            ovar=&#39;_arg&#39;,
            count=count,
            until=until,
            no_address=no_address,
            no_hexdump=no_hexdump,
        )

    def process(self, data):
        insns = list(super().process(data))
        if not insns:
            return

        no_address = self.args.no_address
        no_hexdump = self.args.no_hexdump

        def _hl(x): return len(hex(x))

        args_width = max(len(insn[&#39;_args&#39;]) for insn in insns)
        memo_width = max(len(insn[&#39;_name&#39;]) for insn in insns)
        addr_width = max(_hl(insn[&#39;_addr&#39;]) for insn in insns)

        if no_address:
            addr_width = 0
            memo_width = memo_width + 2

        max_data_bytes_count = max(len(c) for c in insns)

        padding = addr_width + memo_width + args_width + 10
        metrics_opc = HexDumpMetrics(max_data_bytes_count, padding=padding)

        for insn in insns:
            hd = one(hexdump(insn, metrics_opc))
            name = insn.meta.pop(&#39;_name&#39;)
            args = insn.meta.pop(&#39;_args&#39;)
            addr = insn.meta.pop(&#39;_addr&#39;)
            msg = F&#39; {name:&lt;{memo_width}}  {args:&lt;{args_width}}&#39;
            if not no_hexdump:
                msg = F&#39;{msg}  ; {hd}&#39;
            if not no_address:
                msg = F&#39;{addr:0{addr_width}X}: {msg}&#39;
            yield msg.encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.atbash"><code class="flex name class">
<span>class <span class="ident">atbash</span></span>
</code></dt>
<dd>
<section class="desc"><p><a href="https://en.wikipedia.org/wiki/Atbash">https://en.wikipedia.org/wiki/Atbash</a>
Atbash encoding and decoding. Fairly useless in the 21st century, except
for picking out crypto nerds.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/atbash.py#L6-L25" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class atbash(Unit):
    &#34;&#34;&#34;
    https://en.wikipedia.org/wiki/Atbash
    Atbash encoding and decoding. Fairly useless in the 21st century, except
    for picking out crypto nerds.
    &#34;&#34;&#34;

    def process(self, data: bytearray):
        uc = range(B&#39;A&#39;[0], B&#39;Z&#39;[0] + 1)
        lc = range(B&#39;a&#39;[0], B&#39;z&#39;[0] + 1)
        for k, letter in enumerate(data):
            if letter in uc:
                data[k] = uc[~uc.index(letter)]
                continue
            if letter in lc:
                data[k] = lc[~lc.index(letter)]
                continue
        return data

    reverse = process</code></pre>
</details>
</dd>
<dt id="refinery.shell.autoxor"><code class="flex name class">
<span>class <span class="ident">autoxor</span></span>
<span>(</span><span>range=slice(1, 32, None), plaintext=b'', searchpos=slice(0, None, None), alph=False, crib=False, freq=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Assumes input that was encrypted with a polyalphabetic block cipher, like XOR-ing each byte
with successive bytes from a key or by subtracting the respective key byte value from each
input byte. It uses the <code><a title="refinery.xkey" href="index.html#refinery.xkey">xkey</a></code> unit to attack the cipher and attempts to recover the
plaintext automatically.</p>
<p>The unit expects encrypted input which was encrypted byte-wise with a polyalphabetic key. For
both bit-wise and byte-wise addition, it can attempt do determine this key by three methods:</p>
<ol>
<li>Known plaintext cribs: The unit contains a library of file signatures that are expected to
occur at specific offsets. It uses these to attempt a known-plaintext attack against the
input. If a key is found that is at most half the size of such a crib, it is returned.</li>
<li>Known alphabets: For each given key length, the input is split into slices that would have
been encrypted with a single byte for keys of that length. Each such slice undergoes a
character frequency analysis. If the histogram indicates that an alphabet of a small size
was used (i.e. base64), then the unit attempts to determine the key based on this.</li>
<li>Known high frequency glyph: Works if the plaintext contains one letter that occurs with
very high frequency, i.e. zero padding in PE or ELF files, and the space character in text.
Based on this assumption, the unit computes the most likely key. This method will work best
on uncompressed files that were encrypted with a short key.</li>
</ol>
<p>When no option is set, the unit uses all the above methods by default. When at least one of
the methods is selected, it will attempt only selected methods. When a custom plaintext is given,
the other methods are disabled by default.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/misc/autoxor.py#L9-L80" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class autoxor(xkey, docs=&#39;{0}{p}{1}&#39;):
    &#34;&#34;&#34;
    Assumes input that was encrypted with a polyalphabetic block cipher, like XOR-ing each byte
    with successive bytes from a key or by subtracting the respective key byte value from each
    input byte. It uses the `refinery.xkey` unit to attack the cipher and attempts to recover the
    plaintext automatically.
    &#34;&#34;&#34;
    def process(self, data: bytearray):
        fallback: tuple[str, bytes, bytearray] | None = None

        try:
            result = next(self._attack(data))
        except StopIteration:
            result = None
        else:
            key = result.key
            units: list[type[xor] | type[sub]] = []

            if result.xor is not False:
                units.append(xor)
            if result.xor is not True:
                units.append(sub)

            for unit in units:
                self.log_debug(F&#39;attempting {unit.name} for detected key&#39;)

                name = unit.name
                bin = data | unit(key) | bytearray
                mem = memoryview(bin)
                space = B&#39;\0&#39; | unit(0x20) | bytes
                check = get_structured_data_type

                for k in range(0x1000):
                    if t := check(mem[k:]):
                        self.log_info(F&#39;method {name} resulted in non-blob data ({t.mnemonic}) at offset 0x{k:X}; returning buffer&#39;)
                        return self.labelled(bin, key=key, method=name)
                    if k == 0:
                        check = get_executable_type

                if not fallback:
                    fallback = name, key, bin

                if not any(bin):
                    continue

                as_text = bin | unit(space) | bytearray

                try:
                    decoded = as_text.decode(&#39;utf8&#39;)
                except UnicodeDecodeError:
                    is_text = False
                else:
                    import re
                    is_text = bool(re.fullmatch(r&#39;[\s\w!-~]+&#39;, decoded))

                if is_text:
                    self.log_info(&#39;detected likely text input; automatically shifting towards space character&#39;)
                    key = (b&#39;\x20&#39; * len(key)) | unit(key) | bytes
                    return self.labelled(as_text, key=key, method=name)

        if fallback is None:
            self.log_warn(&#39;no key was found; returning original data&#39;)
            return data
        else:
            assert result is not None
            name, key, bin = fallback
            if result.how == self._rt.freq and result.score &lt; 8:
                self.log_warn(
                    F&#39;unrecognized format, no confirmed crib, low score ({result.score:.2f}%); &#39;
                    &#39;the output is likely junk&#39;
                )
            return self.labelled(bin, key=key)</code></pre>
</details>
</dd>
<dt id="refinery.shell.b2f"><code class="flex name class">
<span>class <span class="ident">b2f</span></span>
</code></dt>
<dd>
<section class="desc"><p>Short for "back to front". This unit is a shortcut for <code><a title="refinery.pick" href="index.html#refinery.pick">pick</a></code> with argument <code>::-1</code>:
It will reorder the chunks in the current frame in reverse order.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/pick.py#L143-L149" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class b2f(pick):
    &#34;&#34;&#34;
    Short for &#34;back to front&#34;. This unit is a shortcut for `refinery.pick` with argument `::-1`:
    It will reorder the chunks in the current frame in reverse order.
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__(slice(None, None, -1))</code></pre>
</details>
</dd>
<dt id="refinery.shell.b32"><code class="flex name class">
<span>class <span class="ident">b32</span></span>
</code></dt>
<dd>
<section class="desc"><p>Base32 encoding and decoding.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/b32.py#L8-L35" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class b32(Unit):
    &#34;&#34;&#34;
    Base32 encoding and decoding.
    &#34;&#34;&#34;
    def reverse(self, data):
        return base64.b32encode(data)

    def process(self, data: bytearray):
        before_padding = 0
        for before_padding in range(len(data), 0, -1):
            if data[before_padding - 1:before_padding] != B&#39;=&#39;:
                break
        padding_size = -before_padding % 8
        missing = before_padding + padding_size - len(data)
        if missing &gt; 0:
            self.log_info(F&#39;detected incorrect padding: added {missing} padding characters&#39;)
            data.extend(B&#39;=&#39; * missing)
        if missing &lt; 0:
            self.log_info(F&#39;detected incorrect padding: removed {-missing} padding characters&#39;)
            data[padding_size + before_padding:] = []
        return base64.b32decode(data, casefold=True)

    @classmethod
    def handles(cls, data):
        from refinery.lib.patterns import formats
        if not formats.b32.value.bin.fullmatch(data):
            return False
        return not formats.hex.value.bin.fullmatch(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.b58"><code class="flex name class">
<span>class <span class="ident">b58</span></span>
</code></dt>
<dd>
<section class="desc"><p>Base58 encoding and decoding. It is famously used as an encoding in Bitcoin addresses
because the alphabet omits digits and letters that look similar.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/b58.py#L6-L21" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class b58(base):
    &#34;&#34;&#34;
    Base58 encoding and decoding. It is famously used as an encoding in Bitcoin addresses
    because the alphabet omits digits and letters that look similar.
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__(b&#39;123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz&#39;)

    @classmethod
    def handles(cls, data):
        from refinery.lib.patterns import formats
        return (
            formats.b58.value.bin.fullmatch(data)
            and not formats.hex.value.bin.fullmatch(data)
            and not formats.b32.value.bin.fullmatch(data)
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.b62"><code class="flex name class">
<span>class <span class="ident">b62</span></span>
</code></dt>
<dd>
<section class="desc"><p>Base62 encoding and decoding.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/b62.py#L6-L20" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class b62(base):
    &#34;&#34;&#34;
    Base62 encoding and decoding.
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__(b&#39;0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz&#39;)

    @classmethod
    def handles(cls, data):
        from refinery.lib.patterns import formats
        return (
            formats.b62.value.bin.fullmatch(data)
            and not formats.hex.value.bin.fullmatch(data)
            and not formats.b32.value.bin.fullmatch(data)
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.b64"><code class="flex name class">
<span>class <span class="ident">b64</span></span>
<span>(</span><span>urlsafe=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Base64 encoding and decoding.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/b64.py#L9-L81" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class b64(Unit):
    &#34;&#34;&#34;
    Base64 encoding and decoding.
    &#34;&#34;&#34;
    def __init__(self, urlsafe: Param[bool, Arg.Switch(&#39;-u&#39;, help=&#39;use URL-safe alphabet&#39;)] = False):
        super().__init__(urlsafe=urlsafe)

    def reverse(self, data):
        altchars = None
        if self.args.urlsafe:
            altchars = B&#39;-_&#39;
        return base64.b64encode(data, altchars=altchars)

    def process(self, data: bytearray):
        if not data:
            return data
        if len(data) == 1:
            raise ValueError(&#39;single byte can not be base64-decoded.&#39;)
        data.extend(B&#39;===&#39;)
        altchars = None
        if (B&#39;-&#39; in data or B&#39;_&#39; in data) and (B&#39;+&#39; not in data and B&#39;/&#39; not in data) or self.args.urlsafe:
            altchars = B&#39;-_&#39;
        return base64.b64decode(data, altchars=altchars)

    @Unit.Requires(&#39;numpy&#39;, [&#39;speed&#39;, &#39;default&#39;, &#39;extended&#39;])
    def _numpy():
        import numpy
        return numpy

    @classmethod
    def handles(cls, data) -&gt; bool:
        from refinery.lib.patterns import formats
        if not formats.b64s.value.bin.fullmatch(data):
            return False
        try:
            np = cls._numpy
        except ImportError:
            histogram = set()
            lcase_count = 0
            ucase_count = 0
            digit_count = 0
            other_count = 0
            total_count = len(data)
            for byte in data:
                histogram.add(byte)
                if len(histogram) &gt; 60:
                    return True
                elif byte in range(0x61, 0x7B):
                    lcase_count += 1
                elif byte in range(0x41, 0x5B):
                    ucase_count += 1
                elif byte in range(0x30, 0x40):
                    digit_count += 1
                elif byte in B&#39;\v\f\t\r\n\x20&#39;:
                    total_count -= 1
                else:
                    other_count += 1
        else:
            hist = np.histogram(
                np.frombuffer(memoryview(data), np.uint8), range(0x101))[0]
            lcase_count = sum(hist[k] for k in range(0x61, 0x7B))
            ucase_count = sum(hist[k] for k in range(0x41, 0x5B))
            digit_count = sum(hist[k] for k in range(0x30, 0x40))
            space_count = sum(hist[k] for k in B&#39;\v\f\t\r\n\x20&#39;)
            total_count = len(data) - space_count
            other_count = total_count - (digit_count + ucase_count + lcase_count)

        if any(c &lt; total_count // 64 for c in (lcase_count, ucase_count, digit_count)):
            return False
        if other_count * 2 &gt; total_count:
            return False

        return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.b65536"><code class="flex name class">
<span>class <span class="ident">b65536</span></span>
</code></dt>
<dd>
<section class="desc"><p>Base65536 encoding and decoding.
A relatively esoteric encoding scheme utilizing the UTF-16 / UTF-32 character set.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/b65536.py#L100-L138" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class b65536(Unit):
    &#34;&#34;&#34;
    Base65536 encoding and decoding.
    A relatively esoteric encoding scheme utilizing the UTF-16 / UTF-32 character set.
    &#34;&#34;&#34;
    def reverse(self, data):
        if not data:
            return B&#39;&#39;

        output = MemoryFile()
        length = len(data)
        for x in range(0, length, 2):
            b1 = data[x]
            b2 = data[x + 1] if x + 1 &lt; length else -1
            code_point = _BLOCK_START[b2] + b1
            output.write(chr(code_point).encode())
        return output.getvalue()

    def process(self, data):
        if not data:
            return B&#39;&#39;

        done = False
        output = MemoryFile()
        for ch in data.decode():
            code_point = ord(ch)
            b1 = code_point &amp; ((1 &lt;&lt; 8) - 1)
            try:
                b2 = _B2[code_point - b1]
            except KeyError:
                self.log_info(&#39;Invalid base65536 code point: %d, skipping&#39; % code_point)
                continue
            b = b1.to_bytes(1, &#34;little&#34;) if b2 == -1 else b1.to_bytes(1, &#34;little&#34;) + b2.to_bytes(1, &#34;little&#34;)
            if len(b) == 1:
                if done:
                    raise ValueError(&#39;base65536 sequence continued after final byte&#39;)
                done = True
            output.write(b)
        return output.getvalue()</code></pre>
</details>
</dd>
<dt id="refinery.shell.b85"><code class="flex name class">
<span>class <span class="ident">b85</span></span>
</code></dt>
<dd>
<section class="desc"><p>Base85 encoding and decoding.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/b85.py#L9-L24" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class b85(Unit):
    &#34;&#34;&#34;
    Base85 encoding and decoding.
    &#34;&#34;&#34;
    def reverse(self, data):
        return base64.b85encode(data)

    def process(self, data):
        if re.search(BR&#39;\s&#39;, data) is not None:
            data = re.sub(BR&#39;\s+&#39;, B&#39;&#39;, data)
        return base64.b85decode(data)

    @classmethod
    def handles(cls, data):
        from refinery.lib.patterns import formats
        return formats.b85s.value.bin.fullmatch(data) is not None</code></pre>
</details>
</dd>
<dt id="refinery.shell.b92"><code class="flex name class">
<span>class <span class="ident">b92</span></span>
</code></dt>
<dd>
<section class="desc"><p>Base92 encoding and decoding.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/b92.py#L14-L106" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class b92(Unit):
    &#34;&#34;&#34;
    Base92 encoding and decoding.
    &#34;&#34;&#34;
    def reverse(self, data):
        if not data:
            return B&#39;~&#39;

        reader = StructReader(data, bigendian=True)
        output = MemoryFile()
        while reader.remaining_bits &gt; 0:
            try:
                block = reader.read_integer(13)
            except EOFError:
                count = reader.remaining_bits
                block = reader.read_integer(count)
                self.log_debug(F&#39;reading {count} remaining bits: {block:0{count}b}&#39;)
                shift = 6 - count
                if shift &gt;= 0:
                    block &lt;&lt;= shift
                    self.log_debug(F&#39;encoding block: {block:06b}&#39;)
                    output.write_byte(_B92_ALPHABET[block])
                    break
                block &lt;&lt;= 13 - count
            self.log_debug(F&#39;encoding block: {block:013b}&#39;)
            hi, lo = divmod(block, 91)
            output.write_byte(_B92_ALPHABET[hi])
            output.write_byte(_B92_ALPHABET[lo])
        return output.getvalue()

    def process(self, data):
        if data == B&#39;~&#39;:
            return B&#39;&#39;

        output = MemoryFile()
        buffer = 0
        length = 0

        view = memoryview(data)
        q, r = divmod(len(view), 2)

        if r &gt; 0:
            bits = 6
            tail = _B92_DECODING[data[~0]]
        else:
            bits = 13
            tail = _B92_DECODING[data[~1]] * 91 + _B92_DECODING[data[~0]]
            view = view[:(q - 1) * 2]

        it = iter(view)

        for a, b in zip(it, it):
            block = _B92_DECODING[a] * 91 + _B92_DECODING[b]
            assert length &lt; 8
            buffer &lt;&lt;= 13
            buffer |= block
            length += 13
            size, length = divmod(length, 8)
            assert size &gt; 0
            output.write((buffer &gt;&gt; length).to_bytes(size, &#39;big&#39;))
            buffer &amp;= (1 &lt;&lt; length) - 1

        missing = 8 - length
        shift = bits - missing

        if shift &lt; 8:
            bytecount = 1
        else:
            bytecount = 2
            shift -= 8
            missing += 8

        if shift &lt; 0:
            raise RefineryPartialResult(
                F&#39;Invalid padding, missing {-shift} bits.&#39;,
                output.getvalue())

        buffer &lt;&lt;= missing
        buffer |= tail &gt;&gt; shift
        length += missing
        output.write(buffer.to_bytes(bytecount, &#39;big&#39;))

        if tail &amp; ((1 &lt;&lt; shift) - 1) != 0:
            raise RefineryPartialResult(
                F&#39;Invalid padding, lower {shift} bits of {tail:0{bits}b} are not zero.&#39;,
                output.getvalue())

        return output.getvalue()

    @classmethod
    def handles(cls, data):
        from refinery.lib.patterns import formats
        return formats.b92.value.bin.fullmatch(data) is not None</code></pre>
</details>
</dd>
<dt id="refinery.shell.base"><code class="flex name class">
<span>class <span class="ident">base</span></span>
<span>(</span><span>base=0, strip_padding=False, little_endian=False, strict_digits=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Encodes and decodes integers in arbitrary base.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/base.py#L19-L140" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class base(Unit):
    &#34;&#34;&#34;
    Encodes and decodes integers in arbitrary base.
    &#34;&#34;&#34;
    def __init__(
        self,
        base: Param[isq, Arg.NumSeq(metavar=&#39;base|alphabet&#39;, help=(
            R&#39;Either the base to be used or an alphabet. If an explicit alphabet is given, its length &#39;
            R&#39;determines the base. The default base 0 treats the input as a Python integer literal. If &#39;
            F&#39;a numeric base is given, digits from the alphabet &#34;{_DEFAULT_ALPH_STR}&#34; are used. &#39;))] = 0,
        strip_padding: Param[bool, Arg.Switch(&#39;-s&#39;, help=&#39;Do not add leading zeros to the output.&#39;)] = False,
        little_endian: Param[bool, Arg.Switch(&#39;-e&#39;, help=&#39;Use little endian byte order instead of big endian.&#39;)] = False,
        strict_digits: Param[bool, Arg.Switch(&#39;-d&#39;, help=&#39;Check that all input digits are part of the alphabet.&#39;)] = False,
    ):
        super().__init__(
            base=base,
            strip_padding=strip_padding,
            little_endian=little_endian,
            strict_digits=strict_digits,
        )

    @property
    def _args(self):
        base = self.args.base
        if isinstance(base, int):
            if not base:
                return 0, B&#39;&#39;
            if base in _LARGER_ALPHABETS:
                return base, _LARGER_ALPHABETS[base]
            if base not in range(2, len(_DEFAULT_ALPHABET) + 1):
                raise ValueError(F&#39;base may only be an integer between 2 and {len(_DEFAULT_ALPHABET)}&#39;)
            return base, _DEFAULT_ALPHABET[:base]
        if len(set(base)) != len(base):
            raise ValueError(&#39;the given alphabet contains duplicate letters&#39;)
        return len(base), bytearray(base)

    @property
    def byteorder(self):
        return &#39;little&#39; if self.args.little_endian else &#39;big&#39;

    def reverse(self, data):
        base, alphabet = self._args
        self.log_info(&#39;using byte order&#39;, self.byteorder)
        number = int.from_bytes(data, byteorder=self.byteorder)

        if base == 0:
            return B&#39;0x%X&#39; % number
        if base &gt; len(alphabet):
            raise ValueError(F&#39;Only {len(alphabet)} available; not enough to encode base {base}&#39;)

        log2n = len(data) * 8
        logBn = int(log2n / math.log2(base))
        if base ** logBn &lt;= number:
            logBn += 1
        result = bytearray()
        no_pad = self.args.strip_padding

        for _ in range(logBn):
            number, k = divmod(number, base)
            result.append(alphabet[k])
            if no_pad and number &lt;= 0:
                break

        result.reverse()
        return result

    def process(self, data: bytearray):
        if not data:
            return data
        base, alphabet = self._args
        self.log_debug(F&#39;decoding data using base {base}; alphabet {alphabet!r}&#39;)
        be_lenient = not self.args.strict_digits
        if be_lenient and alphabet.upper() == alphabet:
            lcased = (c + 0x20 if 0x41 &lt;= c &lt;= 0x5a else c for c in data)
            if all(x == y for x, y in zip(data, lcased)):
                data = data.upper()
        if base and base != 64 and be_lenient:
            check = &#39;[^{}]&#39;.format(
                &#39;&#39;.join(F&#39;\\x{c:02x}&#39; for c in sorted(set(alphabet)))).encode(&#39;ascii&#39;)
            if re.search(check, data) is not None:
                stripped = re.sub(check, B&#39;&#39;, data)
                self.log_info(F&#39;stripped {len(data) - len(stripped)} invalid digits from input data&#39;)
                data[:] = stripped
        if len(alphabet) &lt;= len(_DEFAULT_ALPHABET):
            defaults = _DEFAULT_ALPHABET[:base]
            if alphabet != defaults:
                self.log_info(&#39;translating input data to a default alphabet for faster conversion&#39;)
                data_translated = data.translate(bytes.maketrans(alphabet, defaults))
                result = int(data_translated, base)
            else:
                result = int(data, base)
        elif len(alphabet) == 64 and len(data) &gt;= 4:
            import base64
            _b64_alphabet = _LARGER_ALPHABETS[64]
            if alphabet != _b64_alphabet:
                data = data.translate(bytes.maketrans(alphabet, _b64_alphabet))
            return base64.b64decode(data + b&#39;===&#39;, validate=self.args.strict_digits)
        elif len(alphabet) == 85 and len(data) &gt;= 5:
            import base64
            _b85_alphabet = _LARGER_ALPHABETS[85]
            if alphabet != _b85_alphabet:
                data = data.translate(bytes.maketrans(alphabet, _b85_alphabet))
            return base64.b85decode(data)
        else:
            if len(data) &gt; 100_000:
                self.log_warn(&#39;long alphabet &amp; unable to use built-ins; reverting to (slow) fallback.&#39;)
            result = 0
            lookup = {digit: k for k, digit in enumerate(alphabet)}
            for digit in data:
                result *= base
                result += lookup[digit]
        if not base or self.args.strip_padding:
            size, r = divmod(result.bit_length(), 8)
            size += int(bool(r))
        else:
            log2n = int(len(data) * math.log2(base))
            test = 1 &lt;&lt; log2n
            while test &gt; result:
                log2n -= 1
                test &gt;&gt;= 1
            size = log2n // 8 + 1
        return result.to_bytes(size, byteorder=self.byteorder)</code></pre>
</details>
</dd>
<dt id="refinery.shell.bat"><code class="flex name class">
<span>class <span class="ident">bat</span></span>
</code></dt>
<dd>
<section class="desc"><p>Emulates the execution of a batch file. Each command line that would be executed is emitted
as an individual chunk. This can remove simple obfuscation based on expansion of environment
variables.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/bat.py#L7-L17" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class bat(Unit):
    &#34;&#34;&#34;
    Emulates the execution of a batch file. Each command line that would be executed is emitted
    as an individual chunk. This can remove simple obfuscation based on expansion of environment
    variables.
    &#34;&#34;&#34;

    def process(self, data):
        emu = BatchFileEmulator(data)
        for cmd in emu.emulate():
            yield cmd.encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.bitrev"><code class="flex name class">
<span>class <span class="ident">bitrev</span></span>
<span>(</span><span>bigendian=False, blocksize=1)</span>
</code></dt>
<dd>
<section class="desc"><p>Reverse the bits of every block. Any excess bytes at the end of the input that are not
an integer multiple of the block size are ignored.</p>
<p>Unreadable bit reversal operations due to:
<a href="https://graphics.stanford.edu/~seander/bithacks.html#ReverseByteWith64BitsDiv">https://graphics.stanford.edu/~seander/bithacks.html#ReverseByteWith64BitsDiv</a>
<a href="https://graphics.stanford.edu/~seander/bithacks.html#ReverseParallel">https://graphics.stanford.edu/~seander/bithacks.html#ReverseParallel</a></p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/blockwise/bitrev.py#L6-L42" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class bitrev(UnaryOperation):
    &#34;&#34;&#34;
    Reverse the bits of every block. Any excess bytes at the end of the input that are not
    an integer multiple of the block size are ignored.
    &#34;&#34;&#34;
    @staticmethod
    def operate(arg):
        raise RuntimeError(&#39;operate was called before the unit was initialized&#39;)

    def __init__(self, bigendian=False, blocksize=1):
        &#34;&#34;&#34;
        Unreadable bit reversal operations due to:
        https://graphics.stanford.edu/~seander/bithacks.html#ReverseByteWith64BitsDiv
        https://graphics.stanford.edu/~seander/bithacks.html#ReverseParallel
        &#34;&#34;&#34;
        super().__init__(bigendian=bigendian, blocksize=blocksize, _truncate=1)

        if self.bytestream:
            def operate(v):
                return ((v * 0x202020202) &amp; 0x10884422010) % 1023
        elif self.blocksize in (2, 4, 8):
            def operate(v):
                s = self.fbits
                m = self.fmask
                w = v
                while s &gt; 1:
                    s &gt;&gt;= 1
                    m = m ^ (m &lt;&lt; s)
                    w = ((w &lt;&lt; s) &amp; ~m) | ((w &gt;&gt; s) &amp; m)
                return w
        else:
            def operate(v):
                w = v &amp; 0
                for s in range(self.fbits):
                    w |= ((v &gt;&gt; s) &amp; 1) &lt;&lt; (self.fbits - s - 1)
                return w
        self.operate = operate</code></pre>
</details>
</dd>
<dt id="refinery.shell.bitsnip"><code class="flex name class">
<span>class <span class="ident">bitsnip</span></span>
<span>(</span><span>slices=[slice(0, 1, None)], bigendian=False, blocksize=1)</span>
</code></dt>
<dd>
<section class="desc"><p>Pick a certain range of bits from each block of the input. The extracted ranges of bits are
concatenated. Leftover bits that do not form at least one full byte are discarded. Bits are
indexed from least significant at index 0 to most significant in each block. When the unit
operates in big endian mode, the internal bit buffer is shifted left in each step and new bits
are inserted as the least significant portion. Conversely, in default (little endian) mode,
newly extracted bits are added as the now most significant ones. After concatenating all bit
slices into a large integer, this integer is converted into a byte string according to the
given byte ordering.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/blockwise/bitsnip.py#L9-L76" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class bitsnip(BlockTransformationBase):
    &#34;&#34;&#34;
    Pick a certain range of bits from each block of the input. The extracted ranges of bits are
    concatenated. Leftover bits that do not form at least one full byte are discarded. Bits are
    indexed from least significant at index 0 to most significant in each block. When the unit
    operates in big endian mode, the internal bit buffer is shifted left in each step and new bits
    are inserted as the least significant portion. Conversely, in default (little endian) mode,
    newly extracted bits are added as the now most significant ones. After concatenating all bit
    slices into a large integer, this integer is converted into a byte string according to the
    given byte ordering.
    &#34;&#34;&#34;
    def __init__(
        self, slices: Param[list[slice], Arg(help=(
            &#39;Specify start:stop:size, where size can be used to pad or truncate the extracted &#39;
            &#39;bits. If size is omitted, it defaults to (stop-start). If no slice is specified, &#39;
            &#39;it defaults to 0, which corresponds to 0:1:1, i.e. extracting the lowest bit.&#39;)
        )] = [slice(0, 1)],
        bigendian=False, blocksize=1
    ):
        super().__init__(slices=slices, bigendian=bigendian, blocksize=blocksize)

    def process(self, data: bytearray):
        bitsnip_data = 0
        bitsnip_size = 0
        slices: list[tuple[int, int, int]] = []
        maxbits = 8 * self.blocksize
        args: Iterable[slice] = iter(self.args.slices)
        bigendian: bool = self.args.bigendian

        for s in args:
            start = s.start
            stop = s.stop
            if start is None:
                start = 0
            if stop is None:
                stop = maxbits
            elif stop &gt; maxbits:
                raise ValueError(F&#39;the selection {start}:{stop} is out of bounds for the block size {self.blocksize}&#39;)
            if start &gt;= stop:
                continue
            size = stop - start
            mask = (1 &lt;&lt; size) - 1
            size = s.step or size
            slices.append((start, mask, size))

        for item in self.chunk(data):
            for shift, mask, size in slices:
                bits = (item &gt;&gt; shift) &amp; mask
                if bigendian:
                    bitsnip_data &lt;&lt;= size
                    bitsnip_data |= bits
                else:
                    bitsnip_data |= bits &lt;&lt; bitsnip_size
                bitsnip_size += size

        length, remainder = divmod(bitsnip_size, 8)

        if remainder != 0:
            self.log_info(F&#39;discarding {bitsnip_size % 8} bits&#39;)
            if bigendian:
                bitsnip_data &gt;&gt;= remainder
            else:
                bitsnip_data &amp;= (1 &lt;&lt; (8 * length)) - 1

        if bigendian:
            return bitsnip_data.to_bytes(length, &#39;big&#39;)
        else:
            return bitsnip_data.to_bytes(length, &#39;little&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.blabla"><code class="flex name class">
<span>class <span class="ident">blabla</span></span>
<span>(</span><span>key, nonce=b'\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00', rounds=10, discard=0, stateful=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Implements the BlaBla cipher, a 256-bit stream cipher designed by Jean-Philippe Aumasson. It
is similar to ChaCha in design but operates on 64-bit blocks.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/blabla.py#L13-L75" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class blabla(StreamCipherUnit):
    &#34;&#34;&#34;
    Implements the BlaBla cipher, a 256-bit stream cipher designed by Jean-Philippe Aumasson. It
    is similar to ChaCha in design but operates on 64-bit blocks.
    &#34;&#34;&#34;
    key_size = {32}

    def __init__(
        self, key,
        nonce: Param[buf, Arg(help=&#39;The 16-byte nonce. The default are 16 null bytes.&#39;)] = bytes(16),
        rounds: Param[int, Arg.Number(&#39;-r&#39;, help=&#39;The number of rounds, default is {default}.&#39;)] = 10,
        discard=0, stateful=False
    ):
        super().__init__(key=key, nonce=nonce, rounds=rounds, discard=discard, stateful=stateful)

    def keystream(self):
        r = self.args.rounds
        n = self.args.nonce
        k = struct.unpack(&#39;&lt;4Q&#39;, self.args.key)

        try:
            n = struct.unpack(&#39;&lt;2Q&#39;, n)
        except Exception:
            raise ValueError(F&#39;The given nonce has invalid length of {len(n)}, it must be 16 bytes in size.&#39;)

        q = [
            0x6170786593810fab,  # 0x0
            0x3320646ec7398aee,  # 0x1
            0x79622d3217318274,  # 0x2
            0x6b206574babadada,  # 0x3
            *k,                  # 0x4 .. 0x7
            0x2ae36e593e46ad5f,  # 0x8
            0xb68f143029225fc9,  # 0x9
            0x8da1e08468303aa6,  # 0xA
            0xa48a209acd50a4a7,  # 0xB
            0x7fdc12f23f90778c,  # 0xC
            1,                   # 0xD
            *n                   # 0xE .. 0xF
        ]
        while True:
            v = [*q]
            for _ in range(r):
                for a, b, c, d in [
                    (0x0, 0x4, 0x8, 0xC),
                    (0x1, 0x5, 0x9, 0xD),
                    (0x2, 0x6, 0xA, 0xE),
                    (0x3, 0x7, 0xB, 0xF),
                    (0x0, 0x5, 0xA, 0xF),
                    (0x1, 0x6, 0xB, 0xC),
                    (0x2, 0x7, 0x8, 0xD),
                    (0x3, 0x4, 0x9, 0xE),
                ]:
                    v[a] = v[a] + v[b] &amp; _M64
                    v[d] = rotr64(v[d] ^ v[a], 32)
                    v[c] = v[c] + v[d] &amp; _M64
                    v[b] = rotr64(v[b] ^ v[c], 24)
                    v[a] = v[a] + v[b] &amp; _M64
                    v[d] = rotr64(v[d] ^ v[a], 16)
                    v[c] = v[c] + v[d] &amp; _M64
                    v[b] = rotr64(v[b] ^ v[c], 63)
            v = [x + y &amp; _M64 for x, y in zip(q, v)]
            q[0xD] += 1
            yield from struct.pack(&#39;&lt;16Q&#39;, *v)</code></pre>
</details>
</dd>
<dt id="refinery.shell.blowfish"><code class="flex name class">
<span>class <span class="ident">blowfish</span></span>
<span>(</span><span>key, *, iv=b'', padding=None, mode=None, raw=False, little_endian=False, segment_size=0, tag=(), aad=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Blowfish encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/blowfish.py#L9-L12" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class blowfish(StandardBlockCipherUnit, cipher=PyCryptoFactoryWrapper(Blowfish)):
    &#34;&#34;&#34;
    Blowfish encryption and decryption.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="refinery.shell.blz"><code class="flex name class">
<span>class <span class="ident">blz</span></span>
</code></dt>
<dd>
<section class="desc"><p>BriefLZ compression and decompression. The compression algorithm uses a pure Python suffix tree
implementation: It requires a lot of time &amp; memory.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/blz.py#L10-L258" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class blz(Unit):
    &#34;&#34;&#34;
    BriefLZ compression and decompression. The compression algorithm uses a pure Python suffix tree
    implementation: It requires a lot of time &amp; memory.
    &#34;&#34;&#34;
    def _begin(self, data):
        self._src = StructReader(memoryview(data))
        self._dst = MemoryFile(bytearray())
        return self

    def _reset(self):
        self._src.seek(0)
        self._dst.seek(0)
        self._dst.truncate()
        return self

    def _decompress(self):
        (
            signature,
            version,
            src_count,
            src_crc32,
            dst_count,
            dst_crc32,
        ) = self._src.read_struct(&#39;&gt;6L&#39;)
        if signature != 0x626C7A1A:
            raise ValueError(F&#39;Invalid BriefLZ signature: {signature:08X}, should be 626C7A1A.&#39;)
        if version &gt; 10:
            raise ValueError(F&#39;Invalid version number {version}, should be less than 10.&#39;)
        self.log_debug(F&#39;signature: 0x{signature:08X} V{version}&#39;)
        self.log_debug(F&#39;src count: 0x{src_count:08X}&#39;)
        self.log_debug(F&#39;src crc32: 0x{src_crc32:08X}&#39;)
        self.log_debug(F&#39;dst count: 0x{dst_count:08X}&#39;)
        self.log_debug(F&#39;dst crc32: 0x{dst_crc32:08X}&#39;)
        src = self._src.getvalue()
        src = src[24:24 + src_count]
        if len(src) &lt; src_count:
            self.log_warn(F&#39;Only {len(src)} bytes in buffer, but header annoucned a length of {src_count}.&#39;)
        if src_crc32:
            check = zlib.crc32(src)
            if check != src_crc32:
                self.log_warn(F&#39;Invalid source data CRC {check:08X}, should be {src_crc32:08X}.&#39;)
        dst = self._decompress_chunk(dst_count)
        if not dst_crc32:
            return dst
        check = zlib.crc32(dst)
        if check != dst_crc32:
            self.log_warn(F&#39;Invalid result data CRC {check:08X}, should be {dst_crc32:08X}.&#39;)
        return dst

    def _decompress_modded(self):
        self._src.seekrel(8)
        total_size = self._src.u64()
        chunk_size = self._src.u64()
        remaining = total_size
        self.log_debug(F&#39;total size: 0x{total_size:016X}&#39;)
        self.log_debug(F&#39;chunk size: 0x{chunk_size:016X}&#39;)
        while remaining &gt; chunk_size:
            self._decompress_chunk(chunk_size)
            remaining -= chunk_size
        return self._decompress_chunk(remaining)

    def _decompress_chunk(self, size=None):
        bitcount = 0
        bitstore = 0
        decompressed = 1

        def readbit():
            nonlocal bitcount, bitstore
            if not bitcount:
                bitstore = int.from_bytes(self._src.read_exactly(2), &#39;little&#39;)
                bitcount = 0xF
            else:
                bitcount = bitcount - 1
            return (bitstore &gt;&gt; bitcount) &amp; 1

        def readint():
            result = 2 + readbit()
            while readbit():
                result &lt;&lt;= 1
                result += readbit()
            return result

        self._dst.write(self._src.read_exactly(1))

        try:
            while not size or decompressed &lt; size:
                if readbit():
                    length = readint() + 2
                    sector = readint() - 2
                    offset = self._src.read_byte() + 1
                    delta = offset + 0x100 * sector
                    available = self._dst.tell()
                    if delta not in range(available + 1):
                        raise RefineryPartialResult(
                            F&#39;Requested rewind by 0x{delta:08X} bytes with only 0x{available:08X} bytes in output buffer.&#39;,
                            partial=self._dst.getvalue())
                    quotient, remainder = divmod(length, delta)
                    replay = memoryview(self._dst.getvalue())
                    replay = bytes(replay[-delta:] if quotient else replay[-delta:length - delta])
                    replay = quotient * replay + replay[:remainder]
                    self._dst.write(replay)
                    decompressed += length
                else:
                    self._dst.write(self._src.read_exactly(1))
                    decompressed += 1
        except EOFError as E:
            raise RefineryPartialResult(str(E), partial=self._dst.getvalue())
        dst = self._dst.getvalue()
        if decompressed &lt; size:
            raise RefineryPartialResult(
                F&#39;Attempted to decompress {size} bytes, got only {len(dst)}.&#39;, dst)
        if decompressed &gt; size:
            raise RuntimeError(&#39;Decompressed buffer contained more bytes than expected.&#39;)
        return dst

    def _compress(self):
        from refinery.lib.suffixtree import SuffixTree

        try:
            self.log_info(&#39;computing suffix tree&#39;)
            tree = SuffixTree(self._src.getvalue())
        except Exception:
            raise

        bitstore = 0  # The bit stream to be written
        bitcount = 0  # The number of bits in the bit stream
        buffer = MemoryFile(bytearray())

        # Write empty header and first byte of source
        self._dst.write(bytearray(24))
        self._dst.write(self._src.read_exactly(1))

        def writeint(n: int) -&gt; None:
            &#34;&#34;&#34;
            Write an integer to the bit stream.
            &#34;&#34;&#34;
            nonlocal bitstore, bitcount
            nbits = n.bit_length()
            if nbits &lt; 2:
                raise ValueError
            # The highest bit is implicitly assumed:
            n ^= 1 &lt;&lt; (nbits - 1)
            remaining = nbits - 2
            while remaining:
                remaining -= 1
                bitstore &lt;&lt;= 2
                bitcount += 2
                bitstore |= ((n &gt;&gt; remaining) &amp; 3) | 1
            bitstore &lt;&lt;= 2
            bitcount += 2
            bitstore |= (n &amp; 1) &lt;&lt; 1

        src = self._src.getvalue()
        remaining = len(src) - 1
        self.log_info(&#39;compressing data&#39;)

        while True:
            cursor = len(src) - remaining
            rest = src[cursor:]
            if bitcount &gt;= 0x10:
                block_count, bitcount = divmod(bitcount, 0x10)
                info_channel = bitstore &gt;&gt; bitcount
                bitstore = info_channel &lt;&lt; bitcount ^ bitstore
                # The decompressor will read bits from top to bottom, and each 16 bit block has to be
                # little-endian encoded. The bit stream is encoded top to bottom bit in the bitstore
                # variable, and by encoding it as a big endian integer, the stream is in the correct
                # order. However, we need to swap adjacent bytes to achieve little endian encoding for
                # each of the blocks:
                info_channel = bytearray(info_channel.to_bytes(block_count * 2, &#39;big&#39;))
                for k in range(block_count):
                    k0 = 2 * k + 0
                    k1 = 2 * k + 1
                    info_channel[k0], info_channel[k1] = info_channel[k1], info_channel[k0]
                info_channel = memoryview(info_channel)
                data_channel = memoryview(buffer.getvalue())
                self._dst.write(info_channel[:2])
                self._dst.write(data_channel[:-1])
                self._dst.write(info_channel[2:])
                data_channel = bytes(data_channel[-1:])
                buffer.truncate(0)
                store = buffer if bitcount else self._dst
                store.write(data_channel)
            if remaining + bitcount &lt; 0x10:
                buffer = buffer.getvalue()
                if rest or buffer:
                    bitstore &lt;&lt;= 0x10 - bitcount
                    self._dst.write(bitstore.to_bytes(2, &#39;little&#39;))
                    self._dst.write(buffer)
                    self._dst.write(rest)
                elif bitcount:
                    raise RuntimeError(&#39;Bitbuffer Overflow&#39;)
                break
            node = tree.root
            length = 0
            offset = 0
            sector = None
            while node.children and length &lt; len(rest):
                for child in node.children.values():
                    if tree.data[child.start] == rest[length]:
                        node = child
                        break
                if node.start &gt;= cursor:
                    break
                offset = node.start - length
                length = node.end + 1 - offset
            length = min(remaining, length)
            if length &gt;= 4:
                sector, offset = divmod(cursor - offset - 1, 0x100)
            bitcount += 1
            bitstore &lt;&lt;= 1
            if sector is None:
                buffer.write(rest[:1])
                remaining -= 1
                continue
            bitstore |= 1
            buffer.write(bytes((offset,)))
            writeint(length - 2)
            writeint(sector + 2)
            remaining -= length

        self._dst.seek(24)
        dst = self._dst.peek()
        self._dst.seek(0)
        self._dst.write(struct.pack(&#39;&gt;6L&#39;, 0x626C7A1A, 1, len(dst), zlib.crc32(dst), len(src), zlib.crc32(src)))
        return self._dst.getvalue()

    def process(self, data):
        self._begin(data)
        partial = None
        try:
            return self._decompress()
        except ValueError as error:
            if isinstance(error, RefineryPartialResult):
                partial = error
            self.log_warn(F&#39;Reverting to modified BriefLZ after decompression error: {error!s}&#39;)
            self._reset()

        try:
            return self._decompress_modded()
        except RefineryPartialResult:
            raise
        except Exception as error:
            if not partial:
                raise
            raise partial from error

    def reverse(self, data):
        return self._begin(data)._compress()</code></pre>
</details>
</dd>
<dt id="refinery.shell.brotli"><code class="flex name class">
<span>class <span class="ident">brotli</span></span>
</code></dt>
<dd>
<section class="desc"><p>Brotli compression and decompression.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/brotli.py#L6-L20" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class brotli(Unit):
    &#34;&#34;&#34;
    Brotli compression and decompression.
    &#34;&#34;&#34;

    @Unit.Requires(&#39;brotlipy&#39;, [&#39;all&#39;])
    def _brotli():
        import brotli
        return brotli

    def process(self, data):
        return self._brotli.decompress(bytes(data))

    def reverse(self, data):
        return self._brotli.compress(bytes(data))</code></pre>
</details>
</dd>
<dt id="refinery.shell.bruteforce"><code class="flex name class">
<span>class <span class="ident">bruteforce</span></span>
<span>(</span><span>name, length=slice(1, None, None), format=None, alphabet=None, pattern=None, printable=False, digits=False, identifier=False, letters=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Generates all possible combinations of letters in a given alphabet. For each generated string,
one copy of each input chunk is generated and populated with a meta variable containing that
string. This can be used for simple brute forcing checks.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/strings/bruteforce.py#L12-L91" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class bruteforce(Unit):
    &#34;&#34;&#34;
    Generates all possible combinations of letters in a given alphabet. For each generated string,
    one copy of each input chunk is generated and populated with a meta variable containing that
    string. This can be used for simple brute forcing checks.
    &#34;&#34;&#34;
    def __init__(
        self,
        name: Param[str, Arg.String(help=&#39;Name of the meta variable to be populated.&#39;)],
        length: Param[slice, Arg.Bounds(metavar=&#39;length&#39;, help=(
            &#39;Specifies the range of characters to brute force, default is {default}.&#39;
        ))] = slice(1, None),
        format: Param[str, Arg.String(help=(
            &#39;Optional format expression for the output string. The format sequence &#34;{0}&#34; is the &#39;
            &#39;current brute force string, the sequence &#34;{1}&#34; represents the input data.&#39;
        ))] = None,
        alphabet: Param[buf, Arg.Binary(&#39;-a&#39;, group=&#39;ALPH&#39;, help=(
            &#39;The alphabet from which to choose the letters. Entire byte range by default.&#39;
        ))] = None,
        pattern: Param[str, Arg.RegExp(&#39;-r&#39;, group=&#39;ALPH&#39;,
            help=&#39;Provide a regular expression pattern to define the alphabet.&#39;)] = None,
        printable: Param[bool, Arg.Switch(&#39;-p&#39;, group=&#39;ALPH&#39;,
            help=&#39;Equivalent to --pattern=[\\s\\x20-\\x7E]&#39;)] = False,
        digits: Param[bool, Arg.Switch(&#39;-d&#39;, group=&#39;ALPH&#39;,
            help=&#39;Equivalent to --pattern=\\d&#39;)] = False,
        identifier: Param[bool, Arg.Switch(&#39;-i&#39;, group=&#39;ALPH&#39;,
            help=&#39;Equivalent to --pattern=\\w&#39;)] = False,
        letters: Param[bool, Arg.Switch(&#39;-l&#39;, group=&#39;ALPH&#39;,
            help=&#39;Equivalent to --pattern=[a-zA-Z]&#39;)] = False,
    ):
        options = sum(1 for x in [printable, digits, identifier, letters] if x)

        if options &gt; 1 or options and pattern:
            raise ValueError(&#39;Invalid selection.&#39;)

        if printable:
            pattern = b&#39;[\\s\\x20-\\x7E]&#39;
        if digits:
            pattern = b&#39;\\d&#39;
        if identifier:
            pattern = b&#39;\\w&#39;
        if letters:
            pattern = b&#39;[a-zA-Z]&#39;

        super().__init__(
            name=name,
            length=length,
            format=format,
            alphabet=alphabet,
            pattern=pattern,
        )

    def _alphabet(self) -&gt; bytes:
        if (alphabet := self.args.alphabet):
            return alphabet
        else:
            alphabet = bytes(range(0x100))
        if not (pattern := self.args.pattern):
            return alphabet
        if isinstance((regex := Arg.AsRegExp(self.codec, pattern, flags=re.DOTALL)), re.Pattern):
            if (alphabet := B&#39;&#39;.join(regex.findall(alphabet))):
                return alphabet
        raise ValueError(F&#39;Invalid regular expression: {pattern}&#39;)

    def process(self, data: bytearray):
        format_spec: str = self.args.format
        meta = metavars(data)
        name = self.args.name
        kwargs = {name: None}

        for length in integers_of_slice(self.args.length):
            self.log_info(F&#39;generating {length} digits&#39;)
            if not isinstance(length, int) or length &lt; 0:
                raise ValueError(F&#39;Unable to brute force {length} characters.&#39;)
            for string in itertools.product(self._alphabet(), repeat=length):
                string = bytes(string)
                if format_spec:
                    string = meta.format_bin(format_spec, self.codec, [string, data])
                kwargs[name] = string
                yield self.labelled(data, **kwargs)</code></pre>
</details>
</dd>
<dt id="refinery.shell.byteswap"><code class="flex name class">
<span>class <span class="ident">byteswap</span></span>
<span>(</span><span>size=4)</span>
</code></dt>
<dd>
<section class="desc"><p>Reverses the order of bytes in each block. Excess bytes that are not an integer multiple of the block
size are discarded.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/blockwise/byteswap.py#L12-L42" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class byteswap(UnaryOperation):
    &#34;&#34;&#34;
    Reverses the order of bytes in each block. Excess bytes that are not an integer multiple of the block
    size are discarded.
    &#34;&#34;&#34;
    def __init__(self, size: Param[int, Arg.Number(help=&#39;the block size in bytes; the default is {default}.&#39;)] = 4):
        super().__init__(blocksize=size, _truncate=2)

    def inplace(self, block: ndarray) -&gt; None:
        block.byteswap(True)

    operate = NotImplemented

    def process(self, data):
        try:
            return self._fastblock(data)
        except FastBlockError:
            b = self.blocksize
            n = len(data)
            m = n - n % b
            v = memoryview(data)
            if b == 1:
                self.log_warn(&#39;running this unit with a block size of 1 does not have any effect&#39;)
                return data
            for k in range(0, m, b):
                _end = k and k - 1 or None
                data[k : k + b] = v[k + b - 1:_end:-1]
            if m &lt; n:
                del v
                del data[m:]
            return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.bz2"><code class="flex name class">
<span>class <span class="ident">bz2</span></span>
<span>(</span><span>level=9)</span>
</code></dt>
<dd>
<section class="desc"><p>BZip2 compression and decompression.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/bz2.py#L9-L24" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class bz2(Unit):
    &#34;&#34;&#34;
    BZip2 compression and decompression.
    &#34;&#34;&#34;
    def __init__(self, level: Param[int, Arg.Number(&#39;-l&#39;, bound=(1, 9), help=&#39;compression level preset between 1 and 9&#39;)] = 9):
        super().__init__(level=level)

    def process(self, data):
        return bz2_.decompress(data)

    def reverse(self, data):
        return bz2_.compress(data, self.args.level)

    @classmethod
    def handles(cls, data):
        return data[:3] == B&#39;BZh&#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.camellia"><code class="flex name class">
<span>class <span class="ident">camellia</span></span>
<span>(</span><span>key, *, iv=b'', padding=None, mode=None, raw=False, little_endian=False, segment_size=0, tag=(), aad=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Camellia encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/camellia.py#L220-L223" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class camellia(StandardBlockCipherUnit, cipher=BlockCipherFactory(Camellia)):
    &#34;&#34;&#34;
    Camellia encryption and decryption.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="refinery.shell.carve"><code class="flex name class">
<span>class <span class="ident">carve</span></span>
<span>(</span><span>format, unique=False, decode=False, single=False, min=1, max=None, len=None, stripspace=False, longest=False, take=None, utf16=True, ascii=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts patches of data in particular formats from the input.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/pattern/carve.py#L13-L116" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class carve(PatternExtractor):
    &#34;&#34;&#34;
    Extracts patches of data in particular formats from the input.
    &#34;&#34;&#34;
    def __init__(
        self, format: Param[str, Arg.String(metavar=&#39;format&#39;,
            help=F&#39;Specify one of the following formats: {_FORMATS}&#39;)],
        unique: Param[bool, Arg.Switch(&#39;-q&#39;, help=&#39;Yield every match only once.&#39;)] = False,
        decode: Param[bool, Arg.Switch(&#39;-d&#39;, help=&#39;Automatically decode known patterns.&#39;)] = False,
        single: Param[bool, Arg.Switch(&#39;-s&#39;, help=&#39;Only get the biggest match; equivalent to -qlt1&#39;)] = False,
        min=1, max=None, len=None,
        stripspace=False, longest=False, take=None, utf16=True, ascii=True
    ):
        if single:
            take = 1
            longest = True
            unique = True
        try:
            format = formats.from_dashname(format)
        except Exception:
            raise ValueError(F&#39;{format} is not a valid format&#39;)
        super().__init__(
            min=min,
            max=max,
            len=len,
            stripspace=stripspace,
            duplicates=not unique,
            longest=longest,
            take=take,
            ascii=ascii,
            utf16=utf16,
            format=format
        )
        if not decode:
            decoder = NotImplemented
        elif self.args.format in (formats.multiline_string, formats.string):
            from ..encoding.esc import esc
            decoder = esc(unicode=True, quoted=True)
        elif self.args.format == formats.integer:
            from ..encoding.base import base
            decoder = base()
        elif self.args.format in (formats.b16, formats.b16s, formats.hex):
            from ..encoding.hex import hex
            decoder = hex()
        elif self.args.format == formats.hexdump:
            from ..formats.hexload import hexload
            decoder = hexload()
        elif self.args.format == formats.intarray:
            from ..blockwise.pack import pack
            decoder = pack()
        elif self.args.format == formats.strarray:
            from ..encoding.esc import esc
            def _decoder(data: Chunk): # noqa
                return msgpack.packb([
                    m[0] | esc | bytes for m in formats.string.value.bin.finditer(data)])
            decoder = _decoder
        elif self.args.format in (formats.b64, formats.b64s):
            from ..encoding.b64 import b64
            decoder = b64()
        elif self.args.format in (formats.b85, formats.b85s):
            from ..encoding.b85 import b85
            decoder = b85()
        elif self.args.format == formats.b64url:
            from ..encoding.b64 import b64
            decoder = b64(urlsafe=True)
        elif self.args.format == formats.b32:
            from ..encoding.b32 import b32
            decoder = b32()
        elif self.args.format == formats.ps1str:
            from ..encoding.escps import escps
            decoder = escps()
        elif self.args.format == formats.vbastr:
            from ..encoding.escps import escps
            decoder = escps()
        elif self.args.format == formats.hexarray:
            from ..blockwise.pack import pack
            decoder = pack(0x10)
        elif self.args.format == formats.wshenc:
            from ..encoding.wshenc import wshenc
            decoder = wshenc()
        elif self.args.format == formats.uuencode:
            from ..encoding.uuenc import uuenc
            decoder = uuenc()
        elif self.args.format in (
            formats.urlquote,
            formats.urlquote_coarse,
            formats.urlquote_narrow,
        ):
            from ..encoding.url import url
            decoder = url()
        else:
            decoder = NotImplemented
        self.decoder = decoder

    def process(self, data):
        self.log_info(&#39;using pattern:&#39;, self.args.format.str.pattern)
        it = iter(self.matches_filtered(memoryview(data), self.args.format.value.bin))
        if self.decoder is NotImplemented:
            yield from it
        for chunk in it:
            try:
                yield self.decoder(chunk)
            except Exception as E:
                self.log_info(F&#39;decoder failure: {E!s}&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.carve_7z"><code class="flex name class">
<span>class <span class="ident">carve_7z</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts anything from the input data that looks like a 7zip archive file.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/pattern/carve_7z.py#L25-L60" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class carve_7z(Unit):
    &#34;&#34;&#34;
    Extracts anything from the input data that looks like a 7zip archive file.
    &#34;&#34;&#34;
    @Unit.Requires(&#39;py7zr&#39;, [&#39;arc&#39;, &#39;default&#39;, &#39;extended&#39;])
    def _py7zr():
        import py7zr
        return py7zr

    HEADER_SIGNATURE = B&#39;7z\xBC\xAF\x27\x1C&#39;

    def process(self, data: bytearray):
        cursor = 0
        mv = memoryview(data)
        while True:
            start = data.find(self.HEADER_SIGNATURE, cursor)
            if start &lt; cursor:
                break
            self.log_debug(F&#39;found header at offset: 0x{start:08X}&#39;)
            try:
                mf = MemoryFileRecorder(mv[start:])
                self.log_debug(&#39;attempting to read archive&#39;)
                archive = self._py7zr.SevenZipFile(mf)
                self.log_debug(&#39;attempting to test archive&#39;)
                success = archive.test() is not False
            except ImportError:
                raise
            except Exception as error:
                self.log_debug(&#39;parsing archive failed:&#39;, error)
                success = False
            if success:
                self.log_info(F&#39;identified archive of size 0x{mf.max_cursor:08X} at offset 0x{start:08X}&#39;)
                cursor = start + mf.max_cursor
                yield self.labelled(mv[start:cursor], offset=start)
            else:
                cursor = start + 5</code></pre>
</details>
</dd>
<dt id="refinery.shell.carve_der"><code class="flex name class">
<span>class <span class="ident">carve_der</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts anything from the input data that looks like a DER sequence. The carving can be very
slow: The unit will attempt to parse an ASN1 sequence at every offset where a byte with value
0x30 is found, since this can indicate the start of an ASN1 SEQUENCE. It will only consider the
next 10KB of data at this offset, but it nevertheless remains a poor heuristic.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/pattern/carve_der.py#L6-L43" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class carve_der(Unit):
    &#34;&#34;&#34;
    Extracts anything from the input data that looks like a DER sequence. The carving can be very
    slow: The unit will attempt to parse an ASN1 sequence at every offset where a byte with value
    0x30 is found, since this can indicate the start of an ASN1 SEQUENCE. It will only consider the
    next 10KB of data at this offset, but it nevertheless remains a poor heuristic.
    &#34;&#34;&#34;
    @Unit.Requires(&#39;pyasn1&#39;, [&#39;default&#39;, &#39;extended&#39;])
    def _pyasn1parsers():
        from pyasn1.codec.der.decoder import decode
        from pyasn1.codec.der.encoder import encode
        return encode, decode

    def process(self, data: bytearray):
        cursor = 0
        encode, decode = self._pyasn1parsers
        while True:
            try:
                pos = data.index(0x30, cursor)
            except Exception:
                break
            else:
                cursor += 1
            if pos + 1 &lt; len(data) and data[pos + 1] == 0:
                continue
            try:
                sequence = decode(bytes(data[pos:pos + 10_000]))
            except Exception:
                continue
            if not (der := sequence[0]):
                self.log_info(F&#39;0x{pos:08X}: parser returned nothing&#39;)
                continue
            if len(der) &lt; 2:
                self.log_info(F&#39;0x{pos:08X}: parser returned empty sequence&#39;)
                continue
            der = encode(der)
            cursor = pos + len(der)
            yield self.labelled(der, offset=pos)</code></pre>
</details>
</dd>
<dt id="refinery.shell.carve_json"><code class="flex name class">
<span>class <span class="ident">carve_json</span></span>
<span>(</span><span>all=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts anything from the input data that looks like JSON.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/pattern/carve_json.py#L96-L109" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class carve_json(Unit):
    &#34;&#34;&#34;
    Extracts anything from the input data that looks like JSON.
    &#34;&#34;&#34;
    def __init__(
        self, all: Param[bool, Arg.Switch(&#39;-a&#39;, help=(
            &#39;By default, only dictionaries are carved. Specify this flag to also carve lists.&#39;
        ))] = False
    ):
        super().__init__(all=all)

    def process(self, data):
        for start, chunk in JSONCarver(data, dictonly=not self.args.all):
            yield self.labelled(chunk, offset=start)</code></pre>
</details>
</dd>
<dt id="refinery.shell.carve_lnk"><code class="flex name class">
<span>class <span class="ident">carve_lnk</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts anything from the input data that looks like a Windows shortcut (i.e. an LNK file)</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/pattern/carve_lnk.py#L9-L61" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class carve_lnk(Unit):
    &#34;&#34;&#34;
    Extracts anything from the input data that looks like a Windows shortcut (i.e. an LNK file)
    &#34;&#34;&#34;

    @Unit.Requires(&#39;LnkParse3&gt;=1.4.0&#39;, [&#39;formats&#39;, &#39;extended&#39;])
    def _LnkParse3():
        import LnkParse3
        import LnkParse3.extra_factory
        return LnkParse3

    def process(self, data: bytearray):
        pos = 0
        mem = memoryview(data)
        sig = B&#39;\x4C\x00\x00\x00\x01\x14\x02\x00&#39;
        lnk = self._LnkParse3

        while True:
            pos = data.find(sig, pos)
            if pos &lt; 0:
                break
            try:
                parsed = lnk.lnk_file(indata=mem[pos:])
            except Exception:
                pos += 1
                continue
            end = pos + parsed.header.size() + parsed.string_data.size()
            if parsed.has_target_id_list():
                end += parsed.targets.size()
            if parsed.has_link_info() and not parsed.force_no_link_info():
                with suppress(AttributeError):
                    end += parsed.info.size()
            with NoLogging():
                while end &lt; len(mem):
                    extra = lnk.extra_factory.ExtraFactory(mem[end:])
                    try:
                        ec = extra.extra_class()
                    except Exception:
                        break
                    if ec is None:
                        break
                    if &#39;UNKNOWN&#39; in ec().name():
                        break
                    end += extra.item_size()

            terminal_block = mem[end:end + 4]
            if terminal_block != B&#39;\0\0\0\0&#39;:
                self.log_warn(F&#39;detected LNK at offset 0x{pos:X}, but size calculation did not end on a terminal block&#39;)
                continue
            else:
                end += 4
            yield self.labelled(mem[pos:end], offset=pos)
            pos = end</code></pre>
</details>
</dd>
<dt id="refinery.shell.carve_pe"><code class="flex name class">
<span>class <span class="ident">carve_pe</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, path=b'name', recursive=False, keep_root=False, memdump=False, fileinfo=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts anything from the input data that looks like a Portable
Executable (PE) file.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/pattern/carve_pe.py#L13-L89" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class carve_pe(PathExtractorUnit):
    &#34;&#34;&#34;
    Extracts anything from the input data that looks like a Portable
    Executable (PE) file.
    &#34;&#34;&#34;
    def __init__(
        self, *paths, list=False, join_path=False, drop_path=False, path=b&#39;name&#39;,
        recursive: Param[bool, Arg.Switch(&#39;-r&#39;, help=&#39;Extract PE files that are contained in already extracted PEs.&#39;)] = False,
        keep_root: Param[bool, Arg.Switch(&#39;-k&#39;, help=&#39;If the input chunk is itself a PE, include it as an output chunk.&#39;)] = False,
        memdump: Param[bool, Arg.Switch(&#39;-m&#39;, help=&#39;Use the virtual memory layout of a PE file to calculate its size.&#39;)] = False,
        fileinfo: Param[bool, Arg.Switch(&#39;-f&#39;, help=&#39;Use the PE meta information to deduce a file name meta variable.&#39;)] = False
    ):
        super().__init__(
            *paths,
            list=list,
            join_path=join_path,
            drop_path=drop_path,
            path=path,
            recursive=recursive,
            keep_root=keep_root,
            memdump=memdump,
            fileinfo=fileinfo,
        )

    def unpack(self, data):
        cursor = 0
        mv = memoryview(data)

        while True:
            offset = data.find(B&#39;MZ&#39;, cursor)
            if offset &lt; cursor: break
            cursor = offset + 2
            ntoffset = mv[offset + 0x3C:offset + 0x3E]
            if len(ntoffset) &lt; 2:
                return
            ntoffset, = unpack(&#39;H&#39;, ntoffset)
            if mv[offset + ntoffset:offset + ntoffset + 2] != B&#39;PE&#39;:
                self.log_debug(F&#39;invalid NT header signature for candidate at 0x{offset:08X}&#39;)
                continue
            try:
                pe = lief.load_pe_fast(mv[offset:])
            except Exception as err:
                self.log_debug(F&#39;parsing of PE header at 0x{offset:08X} failed:&#39;, err)
                continue

            pesize = get_pe_size(pe, memdump=self.args.memdump)
            pedata = mv[offset:offset + pesize]
            info = {}
            if self.args.fileinfo:
                pe_meta_parser = pemeta()
                try:
                    info = pe_meta_parser.parse_version(pe) or {}
                except Exception as error:
                    self.log_warn(F&#39;Unable to obtain file information: {error!s}&#39;)
                try:
                    info.update(pe_meta_parser.parse_header(pe) or {})
                except Exception:
                    pass
            try:
                path = info[&#39;OriginalFilename&#39;]
            except KeyError:
                try:
                    path = info[&#39;ExportName&#39;]
                except KeyError:
                    path = F&#39;carve-0x{offset:08X}.{magic(pedata).extension}&#39;

            if offset &gt; 0 or self.args.keep_root:
                yield UnpackResult(path, pedata, offset=offset)
                self.log_info(F&#39;extracted PE file of size 0x{pesize:08X} from 0x{offset:08X}&#39;)
            else:
                self.log_info(F&#39;ignored root file of size 0x{pesize:08X} from 0x{offset:08X}&#39;)
                continue

            if not offset or self.args.recursive:
                cursor += pe.optional_header.sizeof_headers
            else:
                cursor += pesize - 2</code></pre>
</details>
</dd>
<dt id="refinery.shell.carve_rtf"><code class="flex name class">
<span>class <span class="ident">carve_rtf</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts anything from the input data that looks like an RTF document.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/pattern/carve_rtf.py#L8-L34" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class carve_rtf(Unit):
    &#34;&#34;&#34;
    Extracts anything from the input data that looks like an RTF document.
    &#34;&#34;&#34;

    def process(self, data: bytearray):
        pos = 0
        mem = memoryview(data)
        sig = re.escape(b&#39;{\\rtf&#39;)

        while True:
            match = re.search(sig, mem[pos:], flags=re.IGNORECASE)
            if match is None:
                break
            pos = pos + match.start()
            end = pos + 1
            depth = 1
            while depth and end &lt; len(mem):
                if mem[end] == 0x7B:  # {
                    depth += 1
                if mem[end] == 0x7D:  # }
                    depth -= 1
                end += 1
            if depth &gt; 0:
                break
            yield self.labelled(mem[pos:end], offset=pos)
            pos = end</code></pre>
</details>
</dd>
<dt id="refinery.shell.carve_xml"><code class="flex name class">
<span>class <span class="ident">carve_xml</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts anything from the input data that looks like XML.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/pattern/carve_xml.py#L121-L128" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class carve_xml(Unit):
    &#34;&#34;&#34;
    Extracts anything from the input data that looks like XML.
    &#34;&#34;&#34;

    def process(self, data):
        for offset, chunk in XMLCarver(data):
            yield self.labelled(chunk, offset=offset)</code></pre>
</details>
</dd>
<dt id="refinery.shell.carve_zip"><code class="flex name class">
<span>class <span class="ident">carve_zip</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts anything from the input data that looks like a zip archive file.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/pattern/carve_zip.py#L49-L89" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class carve_zip(Unit):
    &#34;&#34;&#34;
    Extracts anything from the input data that looks like a zip archive file.
    &#34;&#34;&#34;

    def process(self, data: bytearray):
        end = len(data)
        mem = memoryview(data)
        rev = []
        while True:
            end = data.rfind(ZipEndOfCentralDirectory.SIGNATURE, 0, end)
            if end &lt; 0:
                break
            try:
                end_marker = ZipEndOfCentralDirectory(mem[end:])
            except ValueError as e:
                self.log_info(F&#39;error parsing end of central directory at 0x{end:X}: {e!s}&#39;)
                continue
            else:
                self.log_info(F&#39;successfully parsed end of central directory at 0x{end:X}&#39;)
            start = end - end_marker.directory_size
            shift = start - end_marker.directory_offset
            if start &lt; 0:
                self.log_debug(&#39;end of central directory size is invalid&#39;)
                continue
            try:
                central_directory = ZipCentralDirectory(mem[start:])
            except ValueError:
                self.log_debug(&#39;computed location of central directory is invalid&#39;)
                end = end - len(ZipEndOfCentralDirectory.SIGNATURE)
                continue
            start = central_directory.header_offset + shift
            if mem[start:start + 4] not in (B&#39;PK\x03\x04&#39;, B&#39;\0\0\0\0&#39;):
                # SFX payloads seem to have a nulled header, so we permit this.
                self.log_debug(&#39;computed start of ZIP archive does not have the correct signature bytes&#39;)
                continue
            rev.append((start, end + len(end_marker)))
            end = start
        for start, end in reversed(rev):
            zip = mem[start:end]
            yield self.labelled(zip, offset=start)</code></pre>
</details>
</dd>
<dt id="refinery.shell.cast"><code class="flex name class">
<span>class <span class="ident">cast</span></span>
<span>(</span><span>key, *, iv=b'', padding=None, mode=None, raw=False, little_endian=False, segment_size=0, tag=(), aad=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>CAST encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/cast.py#L9-L12" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class cast(StandardBlockCipherUnit, cipher=PyCryptoFactoryWrapper(CAST)):
    &#34;&#34;&#34;
    CAST encryption and decryption.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="refinery.shell.cca"><code class="flex name class">
<span>class <span class="ident">cca</span></span>
<span>(</span><span>data)</span>
</code></dt>
<dd>
<section class="desc"><p>Short for ConCatAppend: This unit concatenates the input data with its argument by
appending the latter to the former. See also <code><a title="refinery.ccp" href="index.html#refinery.ccp">ccp</a></code> for the unit that prepends
instead.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/strings/cca.py#L7-L19" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class cca(Unit):
    &#34;&#34;&#34;
    Short for ConCatAppend: This unit concatenates the input data with its argument by
    appending the latter to the former. See also `refinery.ccp` for the unit that prepends
    instead.
    &#34;&#34;&#34;

    def __init__(self, data: Param[buf, Arg(help=&#39;Binary string to be appended to the input.&#39;)]):
        super().__init__(data=data)

    def process(self, data: bytearray):
        data.extend(self.args.data)
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.ccp"><code class="flex name class">
<span>class <span class="ident">ccp</span></span>
<span>(</span><span>data)</span>
</code></dt>
<dd>
<section class="desc"><p>Short for ConCatPrepend: This unit concatenates the input data with its argument by
prepending the latter to the former. See also <code><a title="refinery.cca" href="index.html#refinery.cca">cca</a></code> for the unit that appends
instead.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/strings/ccp.py#L7-L19" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ccp(Unit):
    &#34;&#34;&#34;
    Short for ConCatPrepend: This unit concatenates the input data with its argument by
    prepending the latter to the former. See also `refinery.cca` for the unit that appends
    instead.
    &#34;&#34;&#34;

    def __init__(self, data: Param[buf, Arg(help=&#39;Binary string to be prepended to the input.&#39;)]):
        super().__init__(data=data)

    def process(self, data: bytearray):
        data[:0] = self.args.data
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.chacha"><code class="flex name class">
<span>class <span class="ident">chacha</span></span>
<span>(</span><span>key, stateful=False, discard=0, nonce=b'REFINERY', magic=b'', offset=0, rounds=20)</span>
</code></dt>
<dd>
<section class="desc"><p>ChaCha encryption and decryption. The nonce must be 8 bytes long as currently, only the
original Bernstein algorithm is implemented. When 64 bytes are provided as the key, this
data is interpreted as the initial state box and all other parameters are ignored.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/chacha.py#L61-L79" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class chacha(LatinCipherUnit):
    &#34;&#34;&#34;
    ChaCha encryption and decryption. The nonce must be 8 bytes long as currently, only the
    original Bernstein algorithm is implemented. When 64 bytes are provided as the key, this
    data is interpreted as the initial state box and all other parameters are ignored.
    &#34;&#34;&#34;
    def keystream(self) -&gt; Iterable[int]:
        key = self.args.key
        if len(key) == 64:
            it = ChaChaCipher.FromState(key)
        else:
            it = ChaChaCipher(
                key,
                self.args.nonce,
                self.args.magic,
                self.args.rounds,
                self.args.offset,
            )
        yield from it</code></pre>
</details>
</dd>
<dt id="refinery.shell.chacha20"><code class="flex name class">
<span>class <span class="ident">chacha20</span></span>
<span>(</span><span>key, nonce=b'REFINERY')</span>
</code></dt>
<dd>
<section class="desc"><p>ChaCha20 and XChaCha20 encryption and decryption. For ChaCha20, the IV (nonce) must
be 8 or 12 bytes long; for XChaCha20, choose an IV which is 24 bytes long. Invoking
this unit for ChaCha20 is functionally equivalent to <code><a title="refinery.chacha" href="index.html#refinery.chacha">chacha</a></code> with 20 rounds,
but this unit uses the PyCryptodome library C implementation rather than the pure
Python implementation used by <code><a title="refinery.chacha" href="index.html#refinery.chacha">chacha</a></code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/chacha.py#L39-L46" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class chacha20(LatinCipherStandardUnit, cipher=PyCryptoFactoryWrapper(ChaCha20)):
    &#34;&#34;&#34;
    ChaCha20 and XChaCha20 encryption and decryption. For ChaCha20, the IV (nonce) must
    be 8 or 12 bytes long; for XChaCha20, choose an IV which is 24 bytes long. Invoking
    this unit for ChaCha20 is functionally equivalent to `refinery.chacha` with 20 rounds,
    but this unit uses the PyCryptodome library C implementation rather than the pure
    Python implementation used by `refinery.chacha`.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="refinery.shell.chacha20poly1305"><code class="flex name class">
<span>class <span class="ident">chacha20poly1305</span></span>
<span>(</span><span>key, nonce=b'REFINERY')</span>
</code></dt>
<dd>
<section class="desc"><p>ChaCha20-Poly1305 and XChaCha20-Poly1305 encryption and decryption. For the ChaCha20
variant, the nonce must be 8 or 12 bytes long; for XChaCha20, provide a 24 bytes nonce
instead.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/chacha.py#L49-L58" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class chacha20poly1305(LatinCipherStandardUnit, cipher=PyCryptoFactoryWrapper(ChaCha20_Poly1305)):
    &#34;&#34;&#34;
    ChaCha20-Poly1305 and XChaCha20-Poly1305 encryption and decryption. For the ChaCha20
    variant, the nonce must be 8 or 12 bytes long; for XChaCha20, provide a 24 bytes nonce
    instead.
    &#34;&#34;&#34;
    def _get_cipher(self, reset_cache=False):
        cipher = super()._get_cipher(reset_cache)
        cipher.block_size = 1
        return cipher</code></pre>
</details>
</dd>
<dt id="refinery.shell.chaskey"><code class="flex name class">
<span>class <span class="ident">chaskey</span></span>
<span>(</span><span>key, iv=b'', padding=None, mode=None, raw=False, rounds=12, swap=False, *, aad=b'', tag=(), segment_size=0, little_endian=False)</span>
</code></dt>
<dd>
<section class="desc"><p>This implements a block cipher based on the Chaskey algorithm. No subkeys are computed and the
default Chaskey operation is performed on all blocks. Notably, the Donut framework uses Chaskey
with 16 rounds and in CTR mode.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/chaskey.py#L101-L120" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class chaskey(StandardBlockCipherUnit, cipher=BlockCipherFactory(Chaskey)):
    &#34;&#34;&#34;
    This implements a block cipher based on the Chaskey algorithm. No subkeys are computed and the
    default Chaskey operation is performed on all blocks. Notably, the Donut framework uses Chaskey
    with 16 rounds and in CTR mode.
    &#34;&#34;&#34;
    def __init__(
        self, key, iv=b&#39;&#39;, padding=None, mode=None, raw=False,
        rounds: Param[int, Arg.Number(&#39;-k&#39;, help=&#39;Number of rounds to use, the default is {default}&#39;)] = _R,
        swap: Param[bool, Arg.Switch(&#39;-s&#39;, help=&#39;Use big endian byte order for all blocks.&#39;)] = False,
        **more
    ):
        super().__init__(key, iv=iv, padding=padding, mode=mode, raw=raw, rounds=rounds, swap=swap, **more)

    def _new_cipher(self, **optionals) -&gt; CipherInterface:
        return super()._new_cipher(
            swap=self.args.swap,
            rounds=self.args.rounds,
            **optionals
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.chop"><code class="flex name class">
<span>class <span class="ident">chop</span></span>
<span>(</span><span>size, step=None, truncate=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Reinterprets the input as a sequence of equally sized chunks and outputs this sequence.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/chop.py#L8-L30" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class chop(Unit):
    &#34;&#34;&#34;
    Reinterprets the input as a sequence of equally sized chunks and outputs this sequence.
    &#34;&#34;&#34;

    def __init__(
        self,
        size: Param[int, Arg.Number(&#39;size&#39;, help=&#39;Chop data into chunks of this size&#39;)],
        step: Param[int, Arg.Number(&#39;step&#39;, help=(
            &#39;Optionally specify a step size (which is equal to the size by default) which indicates the number of bytes by &#39;
            &#39;which the cursor will be increased after extracting a chunk.&#39;))] = None,
        truncate: Param[bool, Arg.Switch(&#39;-t&#39;, help=(
            &#39;Truncate possible excess bytes at the end of the input, by default they are appended as a single chunk.&#39;))] = False,
    ):
        return super().__init__(size=size, step=step, truncate=truncate)

    def process(self, data):
        view = memoryview(data)
        size = self.args.size
        step = self.args.step
        if size &lt; 1:
            raise ValueError(&#39;The chunk size has to be a positive integer value.&#39;)
        yield from splitchunks(view, size, step, self.args.truncate)</code></pre>
</details>
</dd>
<dt id="refinery.shell.clower"><code class="flex name class">
<span>class <span class="ident">clower</span></span>
</code></dt>
<dd>
<section class="desc"><p>Stands for "Convert to LOWER case"; The unit simply converts all latin alphabet chacters in the
input to lowercase.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/strings/clower.py#L6-L12" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class clower(Unit):
    &#34;&#34;&#34;
    Stands for &#34;Convert to LOWER case&#34;; The unit simply converts all latin alphabet chacters in the
    input to lowercase.
    &#34;&#34;&#34;
    def process(self, data):
        return data.lower()</code></pre>
</details>
</dd>
<dt id="refinery.shell.cm"><code class="flex name class">
<span>class <span class="ident">cm</span></span>
<span>(</span><span>invert=False, all=False, reset=False, size=False, ext=False, entropy=False, ic=False, magic=False, sha1=False, sha256=False, crc32=False, md5=False, hashes=False, *names)</span>
</code></dt>
<dd>
<section class="desc"><p>The Common Meta variables unit populates the set of meta variables of the current chunk with commonly
used metadata. The unit has no effect outside a frame.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/cm.py#L10-L89" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class cm(Unit):
    &#34;&#34;&#34;
    The Common Meta variables unit populates the set of meta variables of the current chunk with commonly
    used metadata. The unit has no effect outside a frame.
    &#34;&#34;&#34;
    def __init__(
        self,
        invert: Param[bool, Arg.Switch(&#39;-x&#39;, group=&#39;ALL&#39;, help=&#39;populate only options that have not been specified&#39;)] = False,
        all: Param[bool, Arg.Switch(&#39;-a&#39;, group=&#39;ALL&#39;, help=&#39;populate all options&#39;)] = False,
        reset: Param[bool, Arg.Switch(&#39;-r&#39;, help=&#39;discard all meta variables that were not explicitly specified&#39;)] = False,
        size: Param[bool, Arg.Switch(&#39;-S&#39;, help=&#39;size of the chunk&#39;)] = False,
        ext: Param[bool, Arg.Switch(&#39;-X&#39;, help=&#39;guess file extension&#39;)] = False,
        entropy: Param[bool, Arg.Switch(&#39;-E&#39;, help=&#39;compute data entropy&#39;)] = False,
        ic: Param[bool, Arg.Switch(&#39;-C&#39;, help=&#39;compute the index of coincidence&#39;)] = False,
        magic: Param[bool, Arg.Switch(&#39;-M&#39;, help=&#39;compute file magic&#39;)] = False,
        sha1: Param[bool, Arg.Switch(&#39;-1&#39;, help=&#39;compute hash: SHA-1&#39;)] = False,
        sha256: Param[bool, Arg.Switch(&#39;-2&#39;, help=&#39;compute hash: SHA-256&#39;)] = False,
        crc32: Param[bool, Arg.Switch(&#39;-3&#39;, help=&#39;compute hash: CRC32&#39;)] = False,
        md5: Param[bool, Arg.Switch(&#39;-5&#39;, help=&#39;compute hash: MD5&#39;)] = False,
        hashes: Param[bool, Arg.Switch(&#39;-H&#39;, help=&#39;compute all common hashes&#39;)] = False,
        *names: Param[str, Arg.String(metavar=&#39;name&#39;, help=(
            F&#39;A variable name that can include the common properties: {_COMMON_PROPERTIES_LIST}.&#39;
            R&#39; If none is given, the size variable is populated. For most of these, an optional &#39;
            R&#39;argument is available that can be used as a shorthand:&#39;))]
    ):
        def stringify(name):
            if isinstance(name, (bytes, bytearray)):
                return name.decode(self.codec)
            if isinstance(name, str):
                return name
            raise TypeError(F&#39;Invalid type for name: {name!r}&#39;)

        _names = {
            stringify(name) for name in names}
        if hashes:
            md5 = sha256 = sha1 = crc32 = True
        if size:
            _names.add(&#39;size&#39;)
        if ext:
            _names.add(&#39;ext&#39;)
        if entropy:
            _names.add(&#39;entropy&#39;)
        if ic:
            _names.add(&#39;ic&#39;)
        if magic:
            _names.add(&#39;magic&#39;)
        if sha1:
            _names.add(&#39;sha1&#39;)
        if sha256:
            _names.add(&#39;sha256&#39;)
        if crc32:
            _names.add(&#39;crc32&#39;)
        if md5:
            _names.add(&#39;md5&#39;)
        if not _names and not reset:
            _names.add(&#39;size&#39;)
        if all:
            if invert:
                raise ValueError(&#39;invert and all are both enabled, resulting in empty configuration.&#39;)
            _names = set(LazyMetaOracle.derivations)
        elif invert:
            _names = set(LazyMetaOracle.derivations) - _names
        super().__init__(names=list(_names), reset=reset)

    def process(self, data):
        return data

    def filter(self, chunks):
        names = self.args.names
        reset = self.args.reset
        for chunk in chunks:
            if not chunk.visible:
                yield chunk
                continue
            meta = metavars(chunk)
            if reset:
                chunk.meta.clear()
            for name in names:
                chunk[name] = meta[name]
            yield chunk</code></pre>
</details>
</dd>
<dt id="refinery.shell.codebook"><code class="flex name class">
<span>class <span class="ident">codebook</span></span>
<span>(</span><span>words)</span>
</code></dt>
<dd>
<section class="desc"><p>Given a sequence of words (as a msgpack-encoded list of binary strings) the unit converts the
occurrence of any of these words by a byte value representing the word's index in the sequence.
The first word from the sequence that matches at a given offset will be used to determine this
value. Any substrings that cannot be matched to a word in the sequence are skipped, assuming
that they are separators.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/codebook.py#L11-L47" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class codebook(Unit):
    &#34;&#34;&#34;
    Given a sequence of words (as a msgpack-encoded list of binary strings) the unit converts the
    occurrence of any of these words by a byte value representing the word&#39;s index in the sequence.
    The first word from the sequence that matches at a given offset will be used to determine this
    value. Any substrings that cannot be matched to a word in the sequence are skipped, assuming
    that they are separators.
    &#34;&#34;&#34;
    def __init__(
        self,
        words: Param[buf, Arg.Binary(help=&#39;A list of binary strings in msgpack format.&#39;)],
    ):
        super().__init__(words=words)

    def _book(self) -&gt; list[bytes]:
        try:
            book = msgpack.loads(self.args.words)
        except Exception:
            raise ValueError(R&#39;The given words are not a valid msgpack buffer.&#39;)
        if not isinstance(book, list):
            raise ValueError(F&#39;The given words are not a list, but a {type(book).__name__}.&#39;)
        if not all(isinstance(v, bytes) for v in book):
            raise ValueError(R&#39;The given words are not all byte strings.&#39;)
        if len(book) &gt; 256:
            raise NotImplementedError(
                R&#39;Only code books up to 256 entries in size are currently supported.&#39;)
        return book

    def process(self, data: bytearray):
        book = self._book()
        lookup = {word: code for code, word in enumerate(book)}
        decode = re.compile(B&#39;|&#39;.join(re.escape(word) for word in book))
        return bytearray((lookup[x] for x in decode.findall(data)))

    def reverse(self, data: bytearray):
        book = self._book()
        return B&#39;&#39;.join(book[b] for b in data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.cp1252"><code class="flex name class">
<span>class <span class="ident">cp1252</span></span>
</code></dt>
<dd>
<section class="desc"><p>Encodes and decodes Windows CP 1252 (aka Latin1) encoded string data.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/cp1252.py#L6-L15" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class cp1252(Unit):
    &#34;&#34;&#34;
    Encodes and decodes Windows CP 1252 (aka Latin1) encoded string data.
    &#34;&#34;&#34;

    def process(self, data):
        return data.decode(self.codec).encode(&#39;cp1252&#39;)

    def reverse(self, data):
        return data.decode(&#39;cp1252&#39;).encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.crc32"><code class="flex name class">
<span>class <span class="ident">crc32</span></span>
<span>(</span><span>reps=1, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the CRC32 hash of the input data.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/hash/checksums.py#L11-L16" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class crc32(HashUnit):
    &#34;&#34;&#34;
    Returns the CRC32 hash of the input data.
    &#34;&#34;&#34;
    def _algorithm(self, data) -&gt; bytes:
        return zlib.crc32(data).to_bytes(4, &#39;big&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.csb"><code class="flex name class">
<span>class <span class="ident">csb</span></span>
<span>(</span><span>format, utf16=True, ascii=True, stripspace=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Short for carve single buffer; carves the single largest buffer of a given format from the
input data and returns it.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/pattern/carve.py#L135-L148" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class csb(carve):
    &#34;&#34;&#34;
    Short for carve single buffer; carves the single largest buffer of a given format from the
    input data and returns it.
    &#34;&#34;&#34;
    def __init__(self, format, utf16=True, ascii=True, stripspace=False):
        super().__init__(
            format,
            decode=False,
            single=True,
            utf16=utf16,
            ascii=ascii,
            stripspace=stripspace,
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.csd"><code class="flex name class">
<span>class <span class="ident">csd</span></span>
<span>(</span><span>format, utf16=True, ascii=True, stripspace=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Short for carve &amp; decode; carves the single largest buffer of a given format from the input
and decodes it with the appropriate decoder.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/pattern/carve.py#L119-L132" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class csd(carve):
    &#34;&#34;&#34;
    Short for carve &amp; decode; carves the single largest buffer of a given format from the input
    and decodes it with the appropriate decoder.
    &#34;&#34;&#34;
    def __init__(self, format, utf16=True, ascii=True, stripspace=False):
        super().__init__(
            format,
            decode=True,
            single=True,
            utf16=utf16,
            ascii=ascii,
            stripspace=stripspace,
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.csv"><code class="flex name class">
<span>class <span class="ident">csv</span></span>
<span>(</span><span>quote=b'"', delim=b',')</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts the rows of a CSV document with header and converts them into JSON chunks.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/csv.py#L15-L91" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class csv(Unit):
    &#34;&#34;&#34;
    Extracts the rows of a CSV document with header and converts them into JSON chunks.
    &#34;&#34;&#34;
    def __init__(
        self,
        quote: Param[buf, Arg(&#39;-q&#39;, help=&#39;Specify the quote character, the default is a double quote.&#39;)] = B&#39;&#34;&#39;,
        delim: Param[buf, Arg(&#39;-d&#39;, help=&#39;Specify the delimiter, the default is a single comma.&#39;)] = B&#39;,&#39;
    ):
        super().__init__(quote=quote, delim=delim)

    def json_to_csv(self, table: dict):
        quote = self.args.quote.decode(self.codec)
        delim = self.args.delim.decode(self.codec)

        if not isinstance(table, list):
            raise ValueError(&#39;Input must be a JSON list.&#39;)

        out = MemoryFile()

        with io.TextIOWrapper(out, self.codec, newline=&#39;&#39;) as stream:
            writer = _csv.writer(stream, quotechar=quote, delimiter=delim, skipinitialspace=True)
            for row in table:
                if not isinstance(row, list):
                    break
                if not all(isinstance(item, str) for item in row):
                    break
                writer.writerow(row)
            else:
                return out.getvalue()

        keys = {}
        # A dictionary is used here over a set because dictionaries remember insertion order.
        # When feeding the unit a sequence of JSON objects, the user would likely expect the
        # column order in the resulting CSV to derive from the entry oder in the JSON data.

        for row in table:
            for key in row:
                if not isinstance(key, str):
                    continue
                keys[key] = None

        keys = list(keys)
        out = MemoryFile()

        with io.TextIOWrapper(out, self.codec, newline=&#39;&#39;) as stream:
            writer = _csv.writer(stream, quotechar=quote, delimiter=delim, skipinitialspace=True)
            writer.writerow(keys)
            for row in table:
                writer.writerow([str(row.get(key, &#39;&#39;)) for key in keys])
            return out.getvalue()

    def reverse(self, data: bytearray):
        try:
            table: list[dict[str, Any]] = json.loads(data)
        except Exception:
            table: list[dict[str, Any]] = [json.loads(line) for line in data.splitlines()]
        return self.json_to_csv(table)

    def process(self, data):
        quote = self.args.quote.decode(self.codec)
        delim = self.args.delim.decode(self.codec)

        def convert(field: str):
            if field.isdigit() and not field.startswith(&#39;0&#39;):
                return int(field)
            date = isodate(field)
            if date is not None:
                return date.isoformat(&#39; &#39;, &#39;seconds&#39;)
            return field

        with io.TextIOWrapper(MemoryFile(data), self.codec) as stream:
            rows = _csv.reader(stream, quotechar=quote, delimiter=delim, skipinitialspace=True)
            keys = next(rows)
            for row in rows:
                out = {key: convert(value) for key, value in zip(keys, row)}
                yield json.dumps(out, indent=4).encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.cswap"><code class="flex name class">
<span>class <span class="ident">cswap</span></span>
</code></dt>
<dd>
<section class="desc"><p>Swap the case of the input string; all lowercase letters are turned into their uppercase
variant and vice-versa.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/strings/cswap.py#L6-L20" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class cswap(Unit):
    &#34;&#34;&#34;
    Swap the case of the input string; all lowercase letters are turned into their uppercase
    variant and vice-versa.
    &#34;&#34;&#34;
    def process(self, data: bytearray):
        lcase = bytes(range(B&#39;a&#39;[0], B&#39;z&#39;[0] + 1))
        ucase = bytes(range(B&#39;A&#39;[0], B&#39;Z&#39;[0] + 1))
        delta = lcase[0] - ucase[0]
        for k, letter in enumerate(data):
            if letter in ucase:
                data[k] += delta
            elif letter in lcase:
                data[k] -= delta
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.cupper"><code class="flex name class">
<span>class <span class="ident">cupper</span></span>
</code></dt>
<dd>
<section class="desc"><p>Stands for "Convert to UPPER case"; The unit simply converts all latin alphabet chacters in the
input to uppercase.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/strings/cupper.py#L6-L12" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class cupper(Unit):
    &#34;&#34;&#34;
    Stands for &#34;Convert to UPPER case&#34;; The unit simply converts all latin alphabet chacters in the
    input to uppercase.
    &#34;&#34;&#34;
    def process(self, data):
        return data.upper()</code></pre>
</details>
</dd>
<dt id="refinery.shell.d2p"><code class="flex name class">
<span>class <span class="ident">d2p</span></span>
<span>(</span><span>tee=False, stream=False, plain=False, force=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Stands for "dump to path"; this is a shortcut for the <code><a title="refinery.dump" href="index.html#refinery.dump">dump</a></code> unit which is equivalent
to running:</p>
<pre><code>dump {path}
</code></pre>
<p>This will dump all chunk in the current frame to the path given by the <code>path</code> meta variable,
which is cmmonly set by units like <code><a title="refinery.xt" href="index.html#refinery.xt">xt</a></code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/sinks/dump.py#L249-L260" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class d2p(dump):
    &#34;&#34;&#34;
    Stands for &#34;dump to path&#34;; this is a shortcut for the `refinery.dump` unit which is equivalent
    to running:

        dump {path}

    This will dump all chunk in the current frame to the path given by the `path` meta variable,
    which is cmmonly set by units like `refinery.xt`.
    &#34;&#34;&#34;
    def __init__(self, tee=False, stream=False, plain=False, force=False):
        super().__init__(&#39;{path}&#39;, tee=tee, stream=stream, plain=plain, force=force)</code></pre>
</details>
</dd>
<dt id="refinery.shell.datefix"><code class="flex name class">
<span>class <span class="ident">datefix</span></span>
<span>(</span><span>format='%Y-%m-%d %H:%M:%S', dos=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Parses all kinds of date formats and unifies them into the same format.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/misc/datefix.py#L48-L143" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class datefix(Unit):
    &#34;&#34;&#34;
    Parses all kinds of date formats and unifies them into the same format.
    &#34;&#34;&#34;

    def __init__(
        self,
        format: Param[str, Arg(help=&#39;Specify the output format as a strftime-like string, using ISO by default.&#39;)] = &#39;%Y-%m-%d %H:%M:%S&#39;,
        dos: Param[bool, Arg(&#39;-d&#39;, help=&#39;Parse timestamps in DOS rather than Unix format.&#39;)] = False
    ):
        super().__init__(format=format, dos=dos)

    @staticmethod
    def dostime(stamp: int) -&gt; datetime:
        &#34;&#34;&#34;
        Parses a given DOS timestamp into a datetime object.
        &#34;&#34;&#34;
        d, t = stamp &gt;&gt; 16, stamp &amp; 0xFFFF
        s = (t &amp; 0x1F) &lt;&lt; 1

        return datetime(
            year   = ((d &amp; 0xFE00) &gt;&gt; 0x9) + 1980,  # noqa
            month  = ((d &amp; 0x01E0) &gt;&gt; 0x5),         # noqa
            day    = ((d &amp; 0x001F) &gt;&gt; 0x0),         # noqa
            hour   = ((t &amp; 0xF800) &gt;&gt; 0xB),         # noqa
            minute = ((t &amp; 0x07E0) &gt;&gt; 0x5),         # noqa
            second = 59 if s == 60 else s,          # noqa
        )

    def _format(self, dt: datetime) -&gt; str:
        return dt.strftime(self.args.format)

    def _extract_timezone(self, data: str):
        def extract(match: re.Match[str]):
            nonlocal zone
            if zone is not None:
                raise ValueError
            h = int(h) if (h := match[&#39;h&#39;]) else 0
            m = int(m) if (m := match[&#39;m&#39;]) else 0
            zone = timedelta(hours=h, minutes=m)
            if match[&#39;p&#39;] == &#39;-&#39;:
                zone = -zone
            return &#39;&#39;
        zone = None
        data = re.sub(_TIMEZONE_PATTERN, extract, data)
        data = re.sub(&#39;\\s{2,}&#39;, &#39; &#39;, data).strip()
        return data, zone

    @linewise
    def process(self, data: str) -&gt; str:
        data = data.strip()

        # replace colons (i.e. for exiftool dates: 2017:01:01)
        if len(data) &gt; 10 and data[4] == &#39;:&#39; and data[7] == &#39;:&#39;:
            data = F&#39;{data[0:4]}-{data[5:7]}-{data[8:]}&#39;

        # strips Z at end (i.e. 20171022055144Z)
        if data.endswith(&#39;Z&#39;):
            data = data[:-1]

        if data.startswith(&#39;0x&#39;):
            try:
                data = str(int(data, 16))
            except Exception:
                pass

        # parses timestamps and dates without much format
        if data.isdigit():
            time_stamp = int(data)
            if len(data) &gt; 14:
                raise Exception(&#39;cannot parse all-numeric string as date: %s&#39; % data)
            elif len(data) == 14:
                # i.e. 20111020193727
                return self._format(datetime.strptime(data, &#39;%Y%m%d%H%M%S&#39;))
            elif len(data) == 13:
                # i.e. 1458016535000
                time_stamp //= 1000
                data = data[:-3]
            if self.args.dos:
                return self._format(self.dostime(time_stamp))
            else:
                return self._format(date_from_timestamp(time_stamp))

        try:
            data, time_delta = self._extract_timezone(data)
        except ValueError:
            return data

        for f in _DATETIME_PATTERNS:
            try:
                dt = datetime.strptime(data, f)
            except ValueError:
                continue
            return self._format(dt if time_delta is None else dt - time_delta)

        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.decompress"><code class="flex name class">
<span>class <span class="ident">decompress</span></span>
<span>(</span><span>prepend=True, tolerance=12, max_ratio=1.0, min_ratio=0.0001, expand_limits=range(0, 257), expand_factor=1.75, strict_limits=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Attempts all available decompression units against the input and returns
the output of the first successful one. If none succeeds, the data is
returned unaltered. The process is heavily biased against LZNT1 decompression
due to a large tendency for LZNT1 false positives.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/decompress.py#L87-L361" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class decompress(Unit):
    &#34;&#34;&#34;
    Attempts all available decompression units against the input and returns
    the output of the first successful one. If none succeeds, the data is
    returned unaltered. The process is heavily biased against LZNT1 decompression
    due to a large tendency for LZNT1 false positives.
    &#34;&#34;&#34;
    def __init__(
        self,
        prepend: Param[bool, Arg.Switch(&#39;-P&#39;, &#39;--no-prepend&#39;, off=True, help=(
            &#39;By default, if decompression fails, the unit attempts to prefix &#39;
            &#39;the data with all possible values of a single byte and decompress &#39;
            &#39;the result. This behavior can be disabled with this flag.&#39;)
        )] = True,
        tolerance: Param[int, Arg.Number(&#39;-t&#39;, help=(
            &#39;Maximum number of bytes to strip from the beginning of the data; &#39;
            &#39;The default value is 12.&#39;)
        )] = 12,
        max_ratio: Param[float, Arg.Double(&#39;-m&#39;, metavar=&#39;R&#39;, help=(
            &#39;To determine whether a decompression algorithm was successful, the &#39;
            &#39;ratio of compressed size to decompressed size may at most be as large &#39;
            &#39;as this number, a floating point value R; default value is {default}.&#39;)
        )] = 1.0,
        min_ratio: Param[float, Arg.Double(&#39;-n&#39;, metavar=&#39;R&#39;, help=(
            &#39;Require that compression ratios must be at least as large as R. This &#39;
            &#39;is a &#34;too good to be true&#34; heuristic against algorithms like lznt1 &#39;
            &#39;that can produce false positives. The default is {default}.&#39;)
        )] = 0.0001,
        expand_limits: Param[slice, Arg.Bounds(&#39;-d&#39;, metavar=&#39;a:b&#39;, help=(
            &#39;Ratio limits are expanded for sizes of input data in the given range, &#39;
            &#39;the default being 0:0x100. The reason for this is that small buffers &#39;
            &#39;can increase in size when compressed under many formats. Set this to :0 &#39;
            &#39;or use strict limits to disable this setting.&#39;)
        )] = range(0, 0x101),
        expand_factor: Param[float, Arg.Double(&#39;-k&#39;, help=(
            &#39;The number by which the maximum compression ratio is multiplied for &#39;
            &#39;small buffers. The default is {default}.&#39;
        ))] = 1.75,
        strict_limits: Param[bool, Arg.Switch(&#39;-l&#39;, help=(
            &#39;For recognized formats i.e. when a magic signature is present, the &#39;
            &#39;above limits are disabled by default. Activate this flag to enforce &#39;
            &#39;them in every case.&#39;)
        )] = False

    ):
        if min_ratio &lt;= 0:
            raise ValueError(&#39;The compression factor must be nonnegative.&#39;)
        super().__init__(
            tolerance=tolerance,
            prepend=prepend,
            min_ratio=min_ratio,
            max_ratio=max_ratio,
            strict_limits=strict_limits,
            expand_limits=expand_limits,
            expand_factor=expand_factor,
        )
        self.engines: dict[str, Unit] = {}
        for mode in (
            MSCF_MODE.XPRESS,
            MSCF_MODE.XPRESS_HUFF,
        ):
            mode = normalize_to_display(mode.name).casefold()
            unit = mscf.assemble(mode)
            self.engines[F&#39;{unit.name}[{mode}]&#39;] = unit
        for engine in [
            mscf,
            pkw,
            zstd,
            szdd,
            bz2,
            zl,
            lzf,
            flz,
            lzma,
            lzw,
            jcalg,
            lzo,
            aplib,
            qlz,
            brotli,
            blz,
            lzjb,
            lz4,
            lznt1,
            nrv2e,
            nrv2d,
            nrv2b,
        ]:
            unit: Unit = engine.assemble()
            _, _, name = unit.name.rpartition(&#39;auto-decompress-&#39;)
            self.engines[name] = unit
        for unit in self.engines.values():
            unit.log_detach()

    def process(self, data):

        data = memoryview(data)
        tiny = bounds[self.args.expand_limits]

        class Decompression(NamedTuple):
            method: str
            engine: Unit
            rating: _R
            result: buf | None = None
            cutoff: int = 0
            prefix: int | None = None
            magic: str | None = None

            def __str__(self):
                status = self.rating.summary
                method = self.method
                prefix = self.prefix
                if prefix is not None:
                    prefix = F&#39;{_COLOR_WARNING}0x{prefix:02X}{_CR}&#39;
                if cutoff := self.cutoff:
                    cutoff = F&#39;{_COLOR_WARNING}0x{cutoff:02X}{_CR}&#39;
                else:
                    cutoff = R&#39;0x00&#39;
                return F&#39;prefix={prefix}, cutoff={cutoff}, [{status}] method={method}&#39;

            def __len__(self):
                if not self.result:
                    return 0
                return len(self.result)

            @property
            def ratio(self):
                if not self.result:
                    return INF
                return (len(data) + int(bool(self.prefix)) - self.cutoff) / len(self)

            @property
            def unmodified(self):
                return self.prefix is None and self.cutoff == 0

        if self.args.prepend:
            buffer = bytearray(1 + len(data))
            buffer[1:] = data

        best_by_rating: dict[_R, Decompression] = {}

        def best_current_rating():
            return max(best_by_rating, default=_R.InvalidData)

        def decompress(method: str, engine: Unit, cutoff: int = 0, prefix: int | None = None, careful: bool = False):
            ingest = data[cutoff:]
            rating = _R.ValidData
            magic = None
            if cutoff == 0 and prefix is None and not careful:
                rating |= _R.NotMangled
            if prefix is not None:
                buffer[0] = prefix
                ingest = buffer
            is_handled = engine.handles(ingest)
            if is_handled is True:
                rating |= _R.KnownFormat
            if is_handled is False:
                return Decompression(method, engine, _R.InvalidData, None, cutoff, prefix)
            try:
                result = next(engine.act(ingest))
            except RefineryPartialResult as pr:
                rating |= _R.HadOutput
                result = pr.partial
            except Exception:
                result = None
            else:
                rating |= _R.Successful
                magic = get_structured_data_type(result)
                if magic is not None:
                    magic = magic.mnemonic
                    rating |= _R.KnownFormatOut

            return Decompression(method, engine, rating, result, cutoff, prefix, magic)

        def update(new: Decompression, discard_if_too_good=False):
            if not new.result:
                return
            ratio = new.ratio
            known = new.rating &amp; _R.KnownFormat
            strict = self.args.strict_limits
            max_ratio = self.args.max_ratio
            min_ratio = self.args.min_ratio
            if not strict and len(data) in tiny:
                max_ratio *= self.args.expand_factor
                min_ratio /= self.args.expand_factor
            if (strict or not known) and not (min_ratio &lt;= ratio &lt;= max_ratio):
                return
            best = best_by_rating.get(new.rating, None)
            prefix = new.prefix
            if prefix is not None:
                prefix = F&#39;0x{prefix:02X}&#39;
            if new.unmodified and best and not best.unmodified:
                threshold = 1.00
            else:
                threshold = 0.95

            if not best:
                q = 0
            elif (q := len(best) / len(new)) &gt; 1:
                # This is unexpected, but indicates that we may have produced incorrect output
                # before: What seems to work best is to force a reset at this point, although
                # it seems like there should be a better solution than this.
                q = -1
                assert best.result
                vb = memoryview(best.result)
                vn = memoryview(new.result)
                # This looks like we have skipped part of the compressed stream; At this point
                # we can abort and not force an update.
                if new.cutoff and vb[-len(vn):] == vn:
                    return

            if q &lt; threshold:
                if best and discard_if_too_good:
                    if q &lt; 0.5:
                        return
                    if new.rating &amp; _R.Successful != _R.Successful:
                        return
                best_by_rating[new.rating] = new
                logger = self.log_info
                _color = _COLOR_SUCCESS
            else:
                logger = self.log_info
                _color = _COLOR_FAILURE
            if ratio &gt;= 9:
                rs = &#39;USELESS&#39;
                rc = _COLOR_FAILURE
            else:
                rs = F&#39;{ratio * 100:6.2f}%&#39;
                if ratio &gt;= 1.1:
                    rc = _COLOR_FAILURE
                elif ratio &gt;= 1.0:
                    rc = _COLOR_WARNING
                else:
                    rc = _COLOR_SUCCESS
            if q &lt; 0:
                qs = &#39;RESTART&#39;
            else:
                qs = F&#39;{q:07.4f}&#39;
            logger(lambda: (
                F&#39;[{new.rating.brief}] [{rc}{rs}{_CR}] [q={_color}{qs}{_CR}] {new!s}&#39;))

        for method, engine in self.engines.items():
            self.log_debug(F&#39;attempting engine: {method}&#39;)
            careful = isinstance(engine, (lznt1, flz, lzjb))
            for t in range(self.args.tolerance + 1):
                if best_current_rating() &gt;= _R.Successful and careful and t &gt; 0:
                    break
                update(decompress(method, engine, t, None, careful), careful)
            if self.args.prepend and method not in _NO_PREFIX and best_current_rating() &lt; _R.Successful:
                for p in range(0x100):
                    update(decompress(method, engine, 0, p, careful), careful)

        for r, u in best_by_rating.items():
            self.log_debug(r, u.method)

        for r in sorted(best_by_rating, reverse=True):
            if dc := best_by_rating[r]:
                if not dc.rating &amp; _R.HadOutput:
                    continue
                self.log_info(F&#39;settling on {dc.method} decompression, cutoff={dc.cutoff} and prefix={dc.prefix}.&#39;)
                if dc.rating &amp; _R.NotMangled:
                    self.log_info(&#39;supporting evidence: no modifications to the buffer were necessary&#39;)
                if dc.rating &amp; _R.KnownFormat:
                    self.log_info(&#39;supporting evidence: found a known magic signature&#39;)
                if dc.rating &amp; _R.HadNoErrors:
                    self.log_info(&#39;supporting evidence: engine produced output without errors&#39;)
                elif dc.rating &amp; _R.HadOutput:
                    self.log_info(&#39;supporting evidence: there were errors, but the engine produced output&#39;)
                if not dc.rating &amp; _R.Successful:
                    self.log_info(&#39;the only decompression with result returned only a partial result.&#39;)
                if dc.rating &amp; _R.KnownFormatOut and (magic := dc.magic):
                    self.log_info(F&#39;the decompressed result had a known format: {magic}&#39;)
                return self.labelled(dc.result, method=dc.method)

        raise ValueError(&#39;no compression engine worked&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.dedup"><code class="flex name class">
<span>class <span class="ident">dedup</span></span>
<span>(</span><span>key=None, count=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Deduplicates a sequence of multiple inputs. The deduplication is limited to the current <code><a title="refinery.lib.frame" href="lib/frame.html">refinery.lib.frame</a></code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/dedup.py#L12-L63" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dedup(Unit):
    &#34;&#34;&#34;
    Deduplicates a sequence of multiple inputs. The deduplication is limited to the current `refinery.lib.frame`.
    &#34;&#34;&#34;
    def __init__(
        self,
        key: Param[str, Arg.String(&#39;key&#39;, help=&#39;An optional meta variable expression to deduplicate.&#39;)] = None,
        count: Param[bool, Arg.Switch(&#39;-c&#39;, help=&#39;Store the count of each deduplicated chunk.&#39;)] = False
    ):
        super().__init__(key=key, count=count)

    def filter(self, chunks):
        keyvar = self.args.key

        if keyvar is not None:
            def key(chunk):
                v = PythonExpression.Evaluate(keyvar, metavars(chunk))
                if isbuffer(v):
                    v = md5(v).digest()
                return v
        else:
            def key(chunk):
                return md5(chunk).digest()

        if self.args.count:
            counts = {}
            buffer = {}
            hashes = None
        else:
            hashes = set()
            counts = None
            buffer = None

        for chunk in chunks:
            if not chunk.visible:
                yield chunk
                continue

            uid = key(chunk)

            if hashes is None:
                counts[uid] = counts.get(uid, 0) + 1
                buffer.setdefault(uid, chunk)
            elif uid in hashes:
                continue
            else:
                hashes.add(uid)
                yield chunk

        if hashes is None:
            for uid, chunk in buffer.items():
                yield self.labelled(chunk, count=counts[uid])</code></pre>
</details>
</dd>
<dt id="refinery.shell.defang"><code class="flex name class">
<span>class <span class="ident">defang</span></span>
<span>(</span><span>url_only=False, url_protocol=False, dot_only=False, quote_md=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Defangs all URL, domain and IPv4 address indicators in the input data by replacing the last dot
in the expression by <code>[.]</code>. For example, <code>127.0.0.1</code> will be replaced by <code>127.0.0[.]1</code>. For URL
indicators, the colon after the procol scheme is also wrapped in brackets.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/pattern/defang.py#L13-L132" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class defang(Unit):
    &#34;&#34;&#34;
    Defangs all URL, domain and IPv4 address indicators in the input data by replacing the last dot
    in the expression by `[.]`. For example, `127.0.0.1` will be replaced by `127.0.0[.]1`. For URL
    indicators, the colon after the procol scheme is also wrapped in brackets.
    &#34;&#34;&#34;

    _WHITELIST = [
        B&#39;wscript.shell&#39;,
    ]

    _PROTOCOL_ESCAPES = {
        B&#39;http&#39;: B&#39;hxxp&#39;,
        B&#39;https&#39;: B&#39;hxxps&#39;,
        B&#39;ftp&#39;: B&#39;fxp&#39;,
        B&#39;ftps&#39;: B&#39;fxps&#39;,
    }

    def __init__(
        self,
        url_only: Param[bool, Arg.Switch(&#39;-u&#39;, help=&#39;Only defang URLs, do not look for domains or IPs.&#39;)] = False,
        url_protocol: Param[bool, Arg.Switch(&#39;-p&#39;, help=&#39;Escape the protocol in URLs.&#39;)] = False,
        dot_only: Param[bool, Arg.Switch(&#39;-d&#39;, help=&#39;Do not escape the protocol colon in URLs.&#39;)] = False,
        quote_md: Param[bool, Arg.Switch(&#39;-q&#39;, help=&#39;Wrap all indicators in backticks for markdown code.&#39;)] = False
    ):
        self.superinit(super(), **vars())

    def _quote(self, word):
        return word if not self.args.quote_md else B&#39;`%s`&#39; % word

    def reverse(self, data: bytearray):
        def refang(hostname):
            return hostname[0].replace(B&#39;[.]&#39;, B&#39;.&#39;)
        data = defanged.hostname.sub(refang, data)
        data = data.replace(B&#39;[:]//&#39;, B&#39;://&#39;)
        data = data.replace(B&#39;[://]&#39;, B&#39;://&#39;)
        data = re.sub(B&#39;h.{3}?(s?)://&#39;, B&#39;http\\1://&#39;, data)
        data = re.sub(B&#39;fxp(s?)://&#39;, B&#39;ftp\\1://&#39;, data)
        return data

    def process(self, data):
        def replace_hostname(hostname: bytes, match=True):
            if match:
                return self._quote(replace_hostname(hostname[0], False))
            self.log_info(&#39;replace:&#39;, hostname)
            host = hostname
            user, atsgn, host = host.rpartition(B&#39;@&#39;)
            host, colon, port = host.rpartition(B&#39;:&#39;)
            host = host.lower()
            if not colon:
                host = port
                port = B&#39;&#39;
            if host in self._WHITELIST:
                return hostname
            host = re.split(R&#39;(?:\[\.\]|\.)&#39;, host.decode(&#39;latin1&#39;))
            if len(host) == 1:
                return hostname
            components = iter(reversed(host))
            defanged_parts = [next(components)]
            separator = &#39;[.]&#39;
            for part in components:
                defanged_parts.append(separator)
                defanged_parts.append(part)
                separator = &#39;[.]&#39; if part in tlds else &#39;.&#39;
            defanged_host = &#39;&#39;.join(reversed(defanged_parts)).encode(&#39;latin1&#39;)
            return user + atsgn + defanged_host + colon + port

        def replace_url(url: bytes):
            if not url:
                return url
            self.log_info(&#39;replace:&#39;, url)
            url = url.replace(B&#39;[:]//&#39;, B&#39;://&#39;, 1)
            url = url.replace(B&#39;[.]&#39;, B&#39;.&#39;)
            prefix = B&#39;tcp&#39;
            if url.startswith(B&#39;://&#39;):
                scheme = 0
            elif url.startswith(B&#39;//&#39;):
                scheme = 1
                prefix = prefix + B&#39;:&#39;
            else:
                scheme = 2
                prefix = B&#39;&#39;
            parsed = urlparse(prefix + url)
            operations = {
                name: self.process(getattr(parsed, name))
                for name in (&#39;path&#39;, &#39;params&#39;, &#39;query&#39;, &#39;fragment&#39;)
            }
            if self.args.url_protocol and parsed.scheme:
                operations.update(scheme=self._PROTOCOL_ESCAPES.get(parsed.scheme.lower(), scheme))
            if scheme &lt; 2:
                operations.update(scheme=B&#39;&#39;)
            operations.update(netloc=replace_hostname(parsed.netloc, False))
            url = urlunparse(parsed._replace(**operations))
            if scheme == 0:
                url = B&#39;:&#39; + url
            if not self.args.dot_only:
                url = url.replace(B&#39;://&#39;, B&#39;[:]//&#39;)
            return self._quote(url)

        urlsplit = defanged.url.split(data)
        step = defanged.url.value.groups + 1
        urlsplit[1::step] = [replace_url(t) for t in itertools.islice(iter(urlsplit), 1, None, step)]

        if not self.args.url_only:
            urlsplit[0::step] = [
                indicators.hostname.sub(replace_hostname, t)
                for t in itertools.islice(iter(urlsplit), 0, None, step)
            ]

        def fuse(urlsplit):
            txt = itertools.islice(iter(urlsplit), 0, None, step)
            url = itertools.islice(iter(urlsplit), 1, None, step)
            while True:
                try:
                    yield next(txt)
                    yield next(url)
                except StopIteration:
                    break

        return B&#39;&#39;.join(fuse(urlsplit))</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_js_arithmetic"><code class="flex name class">
<span>class <span class="ident">deob_js_arithmetic</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/js/arithmetic.py#L10-L71" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_js_arithmetic(Deobfuscator):
    def deobfuscate(self, data):
        strings = StringLiterals(formats.string, data)

        @strings.outside
        def evaluate(match: re.Match[str]):
            expression = match[0]
            expression = expression.strip()
            if not any(c.isdigit() for c in expression):
                return expression
            brackets = 0
            positions = []
            ok = True
            head = tail = rest = &#39;&#39;
            for end, character in enumerate(expression):
                if character == &#39;(&#39;:
                    brackets += 1
                    positions.append(end)
                    continue
                if character == &#39;)&#39;:
                    brackets -= 1
                    if brackets &lt; 0:
                        expression, tail = expression[:end], expression[end:]
                        break
                    else:
                        positions.pop()
                    if brackets == 0 and expression[0] == &#39;(&#39;:
                        expression, rest = expression[:end + 1], expression[end + 1:]
                        break
            if expression.isdigit():
                return match[0]
            if brackets &gt; 0:
                pos = positions[~0] + 1
                head = expression[:pos]
                expression = expression[pos:]
            try:
                result = str(cautious_eval(expression + rest))
            except Exception:
                ok = False
            else:
                rest = &#39;&#39;
            if not ok and rest:
                try:
                    result = str(cautious_eval(expression))
                except Exception:
                    expression += rest
                else:
                    ok = True
            if not ok:
                result = expression
                self.log_info(F&#39;error trying to parse arithmetic expression at offset {match.start()}: ({expression})&#39;)
            else:
                if expression.startswith(&#39;(&#39;) and expression.endswith(&#39;)&#39;):
                    result = F&#39;({result})&#39;
            if tail:
                tail = self.deobfuscate(tail)
            return F&#39;{head}{result}{rest}{tail}&#39;

        pattern = re.compile(R&#39;(?:{i}|{f}|[-+(])(?:[^\S\r\n]{{0,20}}(?:{i}|{f}|[-%|&amp;~&lt;&gt;()+/*^]))+&#39;.format(
            i=str(formats.integer), f=str(formats.float)))

        return pattern.sub(evaluate, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_js_arrays"><code class="flex name class">
<span>class <span class="ident">deob_js_arrays</span></span>
</code></dt>
<dd>
<section class="desc"><p>JavaScript deobfuscator to turn <code>["Z", "t", "s", "e"][0]</code> into <code>"Z"</code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/js/arrays.py#L9-L31" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_js_arrays(Deobfuscator):
    &#34;&#34;&#34;
    JavaScript deobfuscator to turn `[&#34;Z&#34;, &#34;t&#34;, &#34;s&#34;, &#34;e&#34;][0]` into `&#34;Z&#34;`.
    &#34;&#34;&#34;

    def deobfuscate(self, data):
        strlit = StringLiterals(formats.string, data)

        @strlit.outside
        def litpick(match: re.Match[str]):
            try:
                array = match[1]
                index = int(match[2])
                lpick = array.split(&#39;,&#39;)[index].strip()
                self.log_debug(lambda: F&#39;{lpick} = {match[0]}&#39;)
            except (TypeError, IndexError):
                lpick = match[0]
            return lpick

        p = R&#39;\s{{0,5}}&#39;.join([
            &#39;\\[&#39;, &#39;((?:{i}|{s})&#39;, &#39;(?:,&#39;, &#39;(?:{i}|{s})&#39;, &#39;)*)&#39;, &#39;\\]&#39;, &#39;\\[&#39;, &#39;({i})&#39;, &#39;\\]&#39;
        ]).format(i=formats.integer, s=formats.string)
        return re.sub(p, litpick, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_js_comments"><code class="flex name class">
<span>class <span class="ident">deob_js_comments</span></span>
</code></dt>
<dd>
<section class="desc"><p>JavaScript deobfuscator that removes comments from the script.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/js/comments.py#L9-L20" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_js_comments(Deobfuscator):
    &#34;&#34;&#34;
    JavaScript deobfuscator that removes comments from the script.
    &#34;&#34;&#34;
    def deobfuscate(self, data):
        strings = StringLiterals(formats.string, data)
        @strings.outside
        def remove(_): return &#39;&#39;

        data = re.sub(R&#39;/\*.*?\*/&#39;, remove, data, flags=re.DOTALL)
        data = re.sub(R&#39;(?m)//.*$&#39;, remove, data)
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_js_concat"><code class="flex name class">
<span>class <span class="ident">deob_js_concat</span></span>
<span>(</span><span>timeout=100)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/js/concat.py#L9-L35" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_js_concat(IterativeDeobfuscator):
    _SENTINEL = re.compile(R&#39;&#39;&#39;[&#39;&#34;]\s*\+\s*[&#39;&#34;]&#39;&#39;&#39;)

    def deobfuscate(self, data):
        def concat(data):
            strlit = StringLiterals(formats.string, data)
            repeat = True
            while repeat:
                for match in self._SENTINEL.finditer(data):
                    a, b = match.span()
                    a = strlit.get_container(a)
                    if a is None:
                        continue
                    b = strlit.get_container(b)
                    if b is None or b != a + 1:
                        continue
                    _, a = strlit.ranges[a]
                    b, c = strlit.ranges[b]
                    yield data[:a - 1] + data[b + 1:c]
                    data = data[c:]
                    strlit.update(data)
                    break
                else:
                    repeat = False
            yield data

        return &#39;&#39;.join(concat(data))</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_js_getattr"><code class="flex name class">
<span>class <span class="ident">deob_js_getattr</span></span>
</code></dt>
<dd>
<section class="desc"><p>JavaScript deobfuscator to turn <code>WScript["CreateObject"]</code> into <code>WScript.CreateObject</code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/js/getattr.py#L9-L24" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_js_getattr(Deobfuscator):
    &#34;&#34;&#34;
    JavaScript deobfuscator to turn `WScript[&#34;CreateObject&#34;]` into `WScript.CreateObject`.
    &#34;&#34;&#34;

    def deobfuscate(self, data):
        strlit = StringLiterals(formats.string, data)

        @strlit.outside
        def dottify(match: re.Match[str]):
            name = match[2][1:-1]
            if name.isidentifier():
                return F&#39;{match[1]}.{name}&#39;
            return match[0]

        return re.sub(FR&#39;(\w+)\[({formats.string})\]&#39;, dottify, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_js_tuples"><code class="flex name class">
<span>class <span class="ident">deob_js_tuples</span></span>
</code></dt>
<dd>
<section class="desc"><p>JavaScript deobfuscator to turn <code>("Z", "t", "s", "e")</code> into <code>"e"</code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/js/tuples.py#L9-L28" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_js_tuples(Deobfuscator):
    &#34;&#34;&#34;
    JavaScript deobfuscator to turn `(&#34;Z&#34;, &#34;t&#34;, &#34;s&#34;, &#34;e&#34;)` into `&#34;e&#34;`.
    &#34;&#34;&#34;

    def deobfuscate(self, data):

        def litpick(match):
            try:
                array = match[1]
                lpick = array.split(&#39;,&#39;)[-1].strip()
                self.log_debug(lambda: F&#39;{lpick} = {match[0]}&#39;)
            except (TypeError, IndexError):
                lpick = match[0]
            return lpick

        p = R&#39;\s{{0,5}}&#39;.join([
            &#39;\\(&#39;, &#39;((?:{i}|{s})&#39;, &#39;(?:,&#39;, &#39;(?:{i}|{s})&#39;, &#39;)*)&#39;, &#39;\\)&#39;
        ]).format(i=formats.integer, s=formats.string)
        return re.sub(p, litpick, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1"><code class="flex name class">
<span>class <span class="ident">deob_ps1</span></span>
<span>(</span><span>timeout=100)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/ps1/all.py#L25-L51" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1(IterativeDeobfuscator):

    _SUBUNITS: list[type[Deobfuscator]] = [
        deob_ps1_escape,
        deob_ps1_cases,
        deob_ps1_brackets,
        deob_ps1_format,
        deob_ps1_typecast,
        deob_ps1_stringreplace,
        deob_ps1_b64convert,
        deob_ps1_encodings,
        deob_ps1_concat,
        deob_ps1_invoke,
        deob_ps1_uncurly
    ]

    def deobfuscate(self, data):
        units = [u() for u in self._SUBUNITS]
        for u in units:
            u.log_level = self.log_level
        for unit in units:
            self.log_debug(lambda: F&#39;invoking {unit.name}&#39;)
            checkpoint = hash(data)
            data = unit.deobfuscate(data)
            if checkpoint != hash(data) and not self.log_debug(&#39;data has changed.&#39;):
                self.log_info(F&#39;used {unit.name}&#39;)
        return re.sub(R&#39;[\r\n]+&#39;, &#39;\n&#39;, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_b64convert"><code class="flex name class">
<span>class <span class="ident">deob_ps1_b64convert</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/ps1/b64convert.py#L11-L33" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_b64convert(Deobfuscator):

    _SENTINEL = re.compile(&#39;\\s*&#39;.join(
        (re.escape(&#39;[System.Convert]::FromBase64String&#39;), &#39;\\(&#39;, &#39;({s})&#39;, &#39;\\)&#39;)
    ).format(s=formats.ps1str), flags=re.IGNORECASE)

    def deobfuscate(self, data):
        strlit = Ps1StringLiterals(data)

        def replacer(match: re.Match[str]):
            if strlit.get_container(match.start()):
                return match[0]
            try:
                string, = string_unquote(match[1])
            except ValueError:
                return match[0]
            try:
                bytes = base64.b64decode(string)
            except Exception:
                return match[0]
            return &#39;@({})&#39;.format(&#39;,&#39;.join(F&#39;0x{b:02X}&#39; for b in bytes))

        return self._SENTINEL.sub(replacer, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_brackets"><code class="flex name class">
<span>class <span class="ident">deob_ps1_brackets</span></span>
</code></dt>
<dd>
<section class="desc"><p>PowerShell deobfuscation that removes superfluous brackets around constant
literals, i.e. <code>("{0}{2}{1}")</code> is transformed to <code>"{0}{2}{1}"</code>. Currently,
only integer and string constants are supported.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/ps1/brackets.py#L10-L38" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_brackets(Deobfuscator):
    &#34;&#34;&#34;
    PowerShell deobfuscation that removes superfluous brackets around constant
    literals, i.e. `(&#34;{0}{2}{1}&#34;)` is transformed to `&#34;{0}{2}{1}&#34;`. Currently,
    only integer and string constants are supported.
    &#34;&#34;&#34;
    _SENTINEL = re.compile(
        RF&#39;&#39;&#39;(?&lt;![\w&#34;&#39;]{{2}})&#39;&#39;&#39;  # this may be a function call
        RF&#39;&#39;&#39;(\-\w+)?&#39;&#39;&#39;  # not a function call but an argument
        RF&#39;&#39;&#39;\(\s*({formats.integer}|{formats.ps1str})\s*(\S)&#39;&#39;&#39;,
        flags=re.IGNORECASE
    )

    def deobfuscate(self, data):
        strlit = Ps1StringLiterals(data)
        repeat = True

        @strlit.outside
        def replacement(match):
            nonlocal repeat
            if match[3] == &#39;)&#39;:
                repeat = True
                return (match[1] or &#39;&#39;) + match[2]

        while repeat:
            repeat = False
            data = self._SENTINEL.sub(replacement, data)

        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_cases"><code class="flex name class">
<span>class <span class="ident">deob_ps1_cases</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/ps1/cases.py#L9-L45" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_cases(Deobfuscator):
    _NAMES = [
        &#39;-BXor&#39;,
        &#39;-Exec Bypass&#39;,
        &#39;-NoLogo&#39;,
        &#39;-NonInter&#39;,
        &#39;-Replace&#39;,
        &#39;-Windows Hidden&#39;,
        &#39;.Invoke&#39;,
        &#39;Assembly&#39;,
        &#39;Byte&#39;,
        &#39;Char&#39;,
        &#39;ChildItem&#39;,
        &#39;CreateThread&#39;,
        &#39;Get-Variable&#39;,
        &#39;GetType&#39;,
        &#39;IntPtr&#39;,
        &#39;Invoke-Expression&#39;,
        &#39;Invoke&#39;,
        &#39;Length&#39;,
        &#39;Net.WebClient&#39;,
        &#39;PowerShell&#39;,
        &#39;PSVersionTable&#39;,
        &#39;Set-Item&#39;,
        &#39;Set-Variable&#39;,
        &#39;Start-Sleep&#39;,
        &#39;ToString&#39;,
        &#39;Type&#39;,
        &#39;Value&#39;,
        &#39;Void&#39;,
    ]

    @outside(formats.ps1str)
    def deobfuscate(self, data):
        for name in self._NAMES:
            data = re.sub(RF&#39;\b{re.escape(name)}\b&#39;, name, data, flags=re.IGNORECASE)
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_concat"><code class="flex name class">
<span>class <span class="ident">deob_ps1_concat</span></span>
<span>(</span><span>timeout=100)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/ps1/concat.py#L9-L42" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_concat(IterativeDeobfuscator):
    _SENTINEL = re.compile(R&#39;&#39;&#39;[&#39;&#34;]\s*[+&amp;]\s*[&#39;&#34;]&#39;&#39;&#39;)

    def deobfuscate(self, data):

        def concat(data):
            strlit = Ps1StringLiterals(data)
            repeat = True
            while repeat:
                for match in self._SENTINEL.finditer(data):
                    a, b = match.span()
                    a = strlit.get_container(a)
                    if a is None:
                        continue
                    b = strlit.get_container(b)
                    if b is None or b != a + 1:
                        continue
                    a = strlit.ranges[a]
                    b = strlit.ranges[b]
                    stra = data[slice(*a)]
                    strb = data[slice(*b)]
                    parts = list(string_unquote(stra))
                    it = iter(string_unquote(strb))
                    parts[~0] += next(it)
                    parts.extend(it)
                    yield data[:a[0]] + string_quote(parts)
                    data = data[b[1]:]
                    strlit.update(data)
                    break
                else:
                    repeat = False
            yield data

        return &#39;&#39;.join(concat(data))</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_encodings"><code class="flex name class">
<span>class <span class="ident">deob_ps1_encodings</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/ps1/encodings.py#L11-L43" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_encodings(Deobfuscator):

    _SENTINEL = re.compile(&#39;\\s*&#39;.join(
        (re.escape(&#39;[System.Text.Encoding]::&#39;) + &#39;(\\w+)\\.GetString&#39;, &#39;\\(&#39;, &#39;@\\(&#39;, &#39;({a})&#39;, &#39;\\)&#39;, &#39;\\)&#39;)
    ).format(a=formats.intarray), flags=re.IGNORECASE)

    def deobfuscate(self, data):
        strlit = Ps1StringLiterals(data)

        def replacer(match: re.Match[str]):
            if strlit.get_container(match.start()):
                return match[0]
            try:
                bytes = bytearray(int(x.strip(), 0) for x in match[2].split(&#39;,&#39;))
            except Exception:
                return match[0]
            encoding = {
                &#39;ASCII&#39;: &#39;ascii&#39;,
                &#39;BigEndianUnicode&#39;: &#39;utf-16be&#39;,
                &#39;Default&#39;: &#39;latin1&#39;,
                &#39;Unicode&#39;: &#39;utf-16le&#39;,
            }.get(match[1], match[1])
            try:
                codecs.lookup(encoding)
            except LookupError:
                encoding = &#39;utf8&#39;
            try:
                string = bytes.decode(encoding)
            except Exception:
                return match[0]
            return string_quote(string)

        return self._SENTINEL.sub(replacer, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_escape"><code class="flex name class">
<span>class <span class="ident">deob_ps1_escape</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/ps1/escape.py#L9-L15" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_escape(Deobfuscator):

    def deobfuscate(self, data):
        strlit = Ps1StringLiterals(data)
        @strlit.outside
        def repl(m): return m[1]
        return re.sub(R&#39;&#39;&#39;`([^0abfnrtv`#&#39;&#34;\$])&#39;&#39;&#39;, repl, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_format"><code class="flex name class">
<span>class <span class="ident">deob_ps1_format</span></span>
</code></dt>
<dd>
<section class="desc"><p>PowerShell deobfuscation for the following "format string"-based technique:</p>
<ul>
<li><code>"{0}{2}{1}"-f 'signa','ures','t'</code></li>
<li><code>"{0}na{2}{1}"-f 'sig','ures','t'</code></li>
</ul></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/ps1/format.py#L11-L79" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_format(Deobfuscator):
    &#34;&#34;&#34;
    PowerShell deobfuscation for the following &#34;format string&#34;-based technique:

    - `&#34;{0}{2}{1}&#34;-f &#39;signa&#39;,&#39;ures&#39;,&#39;t&#39;`
    - `&#34;{0}na{2}{1}&#34;-f &#39;sig&#39;,&#39;ures&#39;,&#39;t&#39;`
    &#34;&#34;&#34;

    def deobfuscate(self, data):

        repeat = True

        while repeat:

            repeat = False

            for string in re.finditer(str(formats.ps1str), data):
                argmatch = re.search(R&#39;^\s*-[fF]\s*((?:{s},\s*)*{s})&#39;.format(s=formats.ps1str), data[string.end():])
                if not argmatch:
                    continue

                def dbgmsg():
                    sample = string[0]
                    if len(sample) &gt; 33:
                        sample = F&#34;{sample[1:30]}...{sample[0]}&#34;
                    return F&#39;found match at {string.start()}: {sample}&#39;

                self.log_debug(dbgmsg)

                args = re.split(F&#39;({formats.ps1str})&#39;, argmatch[1])
                args = [list(string_unquote(a.strip())) for a in args[1::2]]

                def formatter(string):
                    buffer = []
                    for k, part in enumerate(re.split(R&#39;(\{\d+\})&#39;, string)):
                        if k % 2 == 0:
                            if part:
                                buffer.append(part)
                            continue
                        try:
                            index = int(part[1:-1])
                            arg = args[index]
                        except IndexError as IE:
                            raise IndexError(F&#39;only found {len(args)} arguments and format sequence {index}, aborting.&#39;) from IE

                        it = iter(arg)
                        buffer.append(next(it))

                        if len(arg) &gt; 1:
                            yield &#39;&#39;.join(buffer)
                            buffer = []
                            for last, part in lookahead(it):
                                if last:
                                    buffer.append(part)
                                    break
                                yield part

                    yield &#39;&#39;.join(buffer)

                try:
                    result = string_apply(string[0], formatter)
                except IndexError:
                    continue

                data = data[:string.start()] + result + data[argmatch.end() + string.end():]
                repeat = True
                break

        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_invoke"><code class="flex name class">
<span>class <span class="ident">deob_ps1_invoke</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/ps1/invoke.py#L9-L40" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_invoke(Deobfuscator):
    def deobfuscate(self, data):
        strlit = Ps1StringLiterals(data)

        @strlit.outside
        def invrepl1(m): return m[1] + m[3]

        data = re.sub(
            R&#39;&#39;&#39;(\.|::)&#39;&#39;&#39;                    # preceeded by dot or namespace delimiter
            R&#39;&#39;&#39;([&#39;&#34;])(\w{1,200})\2&#39;&#39;&#39;        # quoted string (actually a method name)
            R&#39;&#39;&#39;(?=[\s\(\.\,\;\+\-])&#39;&#39;&#39;,      # only if followed by certain characters
            invrepl1, data                    # remove quotes around symbol
        )

        @strlit.outside
        def invrepl2(m): return m[1] + &#39;(&#39;

        data = re.sub(
            &#39;\\s{0,5}&#39;.join([
                &#39;[.&amp;]&#39;, &#39;(\\(&#39;,               # sourcing operator
                &#39;(?:gcm|get-command)&#39;, &#39;)?&#39;,  # potentially a get-command
                &#39;([\&#39;&#34;])([-a-z]{1,100})\\2&#39;   # string enclosing a command
                &#39;(?(1)\\s{0,5}\\)|)&#39;,         # closing bracket for get-command
            ]), &#39;\\3&#39;, data, flags=re.IGNORECASE
        )
        data = re.sub(
            R&#39;&#39;&#39;(\w{1,200})\.Invoke\s*\(&#39;&#39;&#39;,
            invrepl2, data,
            flags=re.IGNORECASE
        )

        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_secstr"><code class="flex name class">
<span>class <span class="ident">deob_ps1_secstr</span></span>
<span>(</span><span>*a)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/ps1/securestring.py#L11-L69" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_secstr(Deobfuscator):
    def __init__(self, *a, **kw):
        super().__init__(*a, **kw)

        self._pack = pack()
        self._secstr = secstr()

        self._pattern = re.compile(
            R&#39;\s{{0,20}}&#39;.join([
                R&#39;&#39;&#39;([&#39;&#34;])({b})\1&#39;&#39;&#39;,
                R&#39;\|&#39;, R&#39;\.?&#39;, R&#39;&amp;?&#39;,
                R&#39;&#39;&#39;([&#39;&#34;]?)ConvertTo-SecureString\3&#39;&#39;&#39;,
                R&#39;-ke?y?&#39;,
                R&#39;&#39;&#39;(\(?)({a}|{i}\s{{0,20}}\.\.\s{{0,20}}{i})&#39;&#39;&#39;,
                R&#39;((?:\)\s{{0,20}}){{0,10}})?&#39;
            ]).format(
                b=formats.b64,
                a=formats.intarray,
                i=formats.integer
            ),
            flags=re.IGNORECASE | re.DOTALL
        )

    def _decrypt_block(self, data, match):
        if &#39;..&#39; in match[5]:
            a, b = (int(x.strip(), 0) for x in match[5].split(&#39;..&#39;))
            key = range(min(a, b), max(a, b) + 1)
            if a &gt; b:
                key = reversed(key)
            self._secstr.args.key = bytes(bytearray(key))
        else:
            self._secstr.args.key = self._pack(match[5].encode(self.codec))
        decoded = self._secstr(match[2].encode(self.codec))
        decoded = decoded.decode(self.codec)
        result = F&#39;\n\n{decoded}\n\n&#39;
        brackets = match[6].count(&#39;)&#39;)
        start = match.start()
        if match[4]:
            brackets -= 1
        if brackets &lt;= 0:
            if brackets &lt; 0:
                result += &#39;)&#39;
            return start, result
        while brackets:
            start -= 1
            if data[start] == &#39;(&#39;:
                brackets -= 1
            if data[start] == &#39;)&#39;:
                brackets += 1
        return start, result

    def deobfuscate(self, data):
        while True:
            match = self._pattern.search(data)
            if not match:
                break
            start, result = self._decrypt_block(data, match)
            data = data[:start] + result + data[match.end():]
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_stringreplace"><code class="flex name class">
<span>class <span class="ident">deob_ps1_stringreplace</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/ps1/stringreplace.py#L11-L83" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_stringreplace(Deobfuscator):

    _SENTINEL = re.compile((
        R&#39;(?i)[\&#39;&#34;]\s*&#39;               # end of haystack string
        R&#39;(-c|-i|-|\.)replace&#39;        # the replace call
        R&#39;([\(\s]*)({s})([\)\s]*),&#39;   # needle (with brackets)
        R&#39;([\(\s]*)({s})([\)\s]*)&#39;    # insert (with brackets)
    ).format(s=formats.ps1str), flags=re.IGNORECASE)

    def deobfuscate(self, data):
        repeat = True
        strlit = Ps1StringLiterals(data)

        while repeat:
            repeat = False
            needle = None

            for match in self._SENTINEL.finditer(data):
                k = strlit.get_container(match.start())
                if k is None:
                    continue
                offset, end = strlit.ranges[k]
                if match.start() != end - 1:
                    continue
                string = data[offset:end]
                pf, bl1, needle, bl2, br1, insert, br2 = match.groups()
                end = match.end()
                case = &#39;&#39; if pf[0] in &#39;.c&#39; else &#39;(?i)&#39;
                bl = bl1.count(&#39;(&#39;) - bl2.count(&#39;)&#39;)
                br = br2.count(&#39;)&#39;) - br1.count(&#39;(&#39;)
                if pf[0] == &#39;.&#39;:
                    bl -= 1
                    br -= 1
                if bl != 0 or br &lt; 0:
                    continue
                needle = list(string_unquote(needle))
                if len(needle) &gt; 1:
                    continue

                needle = needle[0]
                head, *body = string_unquote(insert)

                self.log_info(&#39;replacing&#39;, needle, &#39;by&#39;, insert)

                if not body:
                    def perform_replacement(string):
                        return re.sub(F&#39;{case}{re.escape(needle)}&#39;, lambda _: head, string)
                else:
                    *body, tail = body
                    def perform_replacement(string): # noqa
                        parts = re.split(F&#39;{case}{re.escape(needle)}&#39;, string)
                        if len(parts) == 1:
                            yield string
                            return
                        it = iter(parts)
                        yield next(it) + head
                        yield from body
                        for last, part in lookahead(it):
                            if last:
                                yield tail + part
                            else:
                                yield tail + part + head
                                yield from body

                replaced = string_apply(string, perform_replacement) + (br * &#39;)&#39;)
                strlit.ranges[k] = offset, offset + len(replaced) - br
                strlit.ranges[k + 1: k + 3] = []
                strlit.shift(len(replaced) + offset - end, k + 1)
                data = data[:offset] + replaced + data[end:]
                repeat = True
                break

        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_typecast"><code class="flex name class">
<span>class <span class="ident">deob_ps1_typecast</span></span>
</code></dt>
<dd>
<section class="desc"><p>Replaces sequences like [Char]120 to their string representation, in this
case the string "x".</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/ps1/typecast.py#L11-L68" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_typecast(Deobfuscator):
    &#34;&#34;&#34;
    Replaces sequences like [Char]120 to their string representation, in this
    case the string &#34;x&#34;.
    &#34;&#34;&#34;

    def deobfuscate(self, data):
        strlit = Ps1StringLiterals(data)

        @strlit.outside
        def strip_typecast(m): return m[1]

        data = re.sub(
            FR&#39;\[(?:string|char\[\])\]\s*({formats.ps1str!s})&#39;,
            strip_typecast,
            data,
            flags=re.IGNORECASE
        )

        @strlit.outside
        def char_literal(match):
            c = chr(int(match[1].lower(), 0))
            if c == &#34;&#39;&#34;:
                return &#39;&#39;&#39;&#34;&#39;&#34;&#39;&#39;&#39;
            return F&#34;&#39;{c}&#39;&#34;

        data = re.sub(
            R&#39;\[char\]\s*0*(0x[0-9a-f]+|\d+)&#39;,
            char_literal,
            data,
            flags=re.IGNORECASE
        )

        def char_array(match):
            result = bytes(int(x, 0) for x in match[1].split(&#39;,&#39;))
            try:
                result = result.decode(&#39;ascii&#39;)
                if not all(x in string.printable or x.isspace() for x in result):
                    raise ValueError
            except ValueError:
                return match[0]
            else:
                return string_quote(result)

        data = re.sub(
            R&#39;\s*&#39;.join([
                R&#39;\[char\[\]\]&#39;,
                R&#39;\((&#39;,
                R&#39;(?:\s*(?:0x[0-9a-f]+|\d+)\s*,)+&#39;,
                R&#39;(?:0x[0-9a-f]+|\d+)&#39;,
                R&#39;)\)&#39;
            ]),
            char_array,
            data,
            flags=re.IGNORECASE
        )

        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_uncurly"><code class="flex name class">
<span>class <span class="ident">deob_ps1_uncurly</span></span>
</code></dt>
<dd>
<section class="desc"><p>PowerShell deobfuscation that removes superfluous curly braces around variable
names that do not require it, i.e. <code>${variable}</code> is transformed to just <code>$variable</code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/ps1/uncurly.py#L9-L21" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_uncurly(Deobfuscator):
    &#34;&#34;&#34;
    PowerShell deobfuscation that removes superfluous curly braces around variable
    names that do not require it, i.e. `${variable}` is transformed to just `$variable`.
    &#34;&#34;&#34;

    _SENTINEL = re.compile(R&#39;\$\{(\w+)\}&#39;)

    def deobfuscate(self, data):
        strlit = Ps1StringLiterals(data)
        @strlit.outside
        def strip(m): return F&#39;${m[1]}&#39;
        return self._SENTINEL.sub(strip, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_vba"><code class="flex name class">
<span>class <span class="ident">deob_vba</span></span>
<span>(</span><span>timeout=100)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/vba/all.py#L23-L47" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_vba(IterativeDeobfuscator):

    _SUBUNITS: list[type[Deobfuscator]] = [
        deob_vba_comments,
        deob_vba_brackets,
        deob_vba_char_function,
        deob_vba_concat,
        deob_vba_arithmetic,
        deob_vba_constants,
        deob_vba_dummy_variables,
        deob_vba_stringreplace,
        deob_vba_stringreverse,
    ]

    def deobfuscate(self, data):
        units = [u() for u in self._SUBUNITS]
        for u in units:
            u.log_level = self.log_level
        for unit in units:
            self.log_debug(lambda: F&#39;invoking {unit.name}&#39;)
            checkpoint = hash(data)
            data = unit.deobfuscate(data)
            if checkpoint != hash(data) and not self.log_debug(&#39;data has changed.&#39;):
                self.log_info(F&#39;used {unit.name}&#39;)
        return re.sub(R&#39;[\r\n]+&#39;, &#39;\n&#39;, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_vba_arithmetic"><code class="flex name class">
<span>class <span class="ident">deob_vba_arithmetic</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/vba/arithmetic.py#L14-L88" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_vba_arithmetic(Deobfuscator):
    def deobfuscate(self, data):
        strings = StringLiterals(formats.vbastr, data)

        def vba_int_eval(match: re.Match[str]) -&gt; str:
            s = match[0].lower()
            if not s.startswith(&#39;&amp;&#39;):
                return s
            t, s = s[1], s[2:].rstrip(&#39;&amp;&#39;)
            if t == &#39;h&#39;:
                return str(int(s, 16))
            if t == &#39;b&#39;:
                return str(int(s, 2))
            if t == &#39;o&#39;:
                return str(int(s, 8))

        @strings.outside
        def evaluate(match: re.Match[str]):
            expression = match[0]
            expression = expression.strip()
            if not any(c.isdigit() for c in expression):
                return expression
            expression = re.sub(str(formats.vbaint), vba_int_eval, expression)
            brackets = 0
            positions = []
            ok = True
            head = tail = rest = &#39;&#39;
            for end, character in enumerate(expression):
                if character == &#39;(&#39;:
                    brackets += 1
                    positions.append(end)
                    continue
                if character == &#39;)&#39;:
                    brackets -= 1
                    if brackets &lt; 0:
                        expression, tail = expression[:end], expression[end:]
                        break
                    else:
                        positions.pop()
                    if brackets == 0 and expression[0] == &#39;(&#39;:
                        expression, rest = expression[:end + 1], expression[end + 1:]
                        break
            if expression.isdigit():
                return match[0]
            if brackets &gt; 0:
                pos = positions[~0] + 1
                head = expression[:pos]
                expression = expression[pos:]
            try:
                result = str(_cautious_vba_eval(expression + rest))
            except Exception:
                ok = False
            else:
                rest = &#39;&#39;
            if not ok and rest:
                try:
                    result = str(_cautious_vba_eval(expression))
                except Exception:
                    expression += rest
                else:
                    ok = True
            if not ok:
                result = expression
                self.log_info(F&#39;error trying to parse arithmetic expression at offset {match.start()}: ({expression})&#39;)
            else:
                if expression.startswith(&#39;(&#39;) and expression.endswith(&#39;)&#39;):
                    result = F&#39;({result})&#39;
            if tail:
                tail = self.deobfuscate(tail)
            return F&#39;{head}{result}{rest}{tail}&#39;

        pattern = re.compile(R&#39;(?:{i}|{f}|[-+(])(?:[^\S\r\n]{{0,20}}(?:{i}|{f}|[-%|&amp;~&lt;&gt;()+/*^]))+&#39;.format(
            i=str(formats.vbaint), f=str(formats.float)))

        return pattern.sub(evaluate, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_vba_brackets"><code class="flex name class">
<span>class <span class="ident">deob_vba_brackets</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/vba/brackets.py#L9-L31" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_vba_brackets(Deobfuscator):
    _SENTINEL = re.compile(
        RF&#39;&#39;&#39;(?&lt;![\w&#34;&#39;]{{2}})&#39;&#39;&#39;  # this may be a function call
        RF&#39;&#39;&#39;\(\s*({formats.vbaint}|{formats.vbastr}|{formats.float})\s*(\S)&#39;&#39;&#39;,
        flags=re.IGNORECASE
    )

    def deobfuscate(self, data):
        strlit = StringLiterals(formats.vbastr, data)
        repeat = True

        @strlit.outside
        def replacement(match):
            nonlocal repeat
            if match[2] == &#39;)&#39;:
                repeat = True
                return match[1]

        while repeat:
            repeat = False
            data = self._SENTINEL.sub(replacement, data)

        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_vba_char_function"><code class="flex name class">
<span>class <span class="ident">deob_vba_char_function</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/vba/char.py#L9-L28" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_vba_char_function(Deobfuscator):
    def deobfuscate(self, data):
        strings = StringLiterals(formats.vbastr, data)

        @strings.outside
        def evaluate_char_function(match: re.Match[str]):
            try:
                c = chr(int(match[1]))
            except ValueError:
                return match[0]
            if c == &#39;&#34;&#39;:
                return &#39;&#34;&#34;&#34;&#34;&#39;
            if c == &#39;\\&#39;:
                return &#39;&#34;\\&#34;&#39;
            c = repr(c)[1:-1]
            if len(c) &gt; 1:
                return match[0]
            return f&#39;&#34;{c}&#34;&#39;

        return re.sub(R&#39;(?i)\bchrw?\s*\(\s*(\d+)\s*\)&#39;, evaluate_char_function, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_vba_chr_literals"><code class="flex name class">
<span>class <span class="ident">deob_vba_chr_literals</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/vba/vba.py#L8-L17" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_vba_chr_literals(Unit):
    def process(self, data):
        def _chr(m):
            code = int(m[1], 0)
            if code == 34:
                return B&#39;&#34;&#34;&#34;&#34;&#39;
            return B&#39;&#34;%s&#34;&#39; % chr(code).encode(&#39;unicode_escape&#39;)
        data = re.sub(BR&#39;Chr\((\d+x?\d+)\)&#39;, _chr, data, flags=re.IGNORECASE)
        data = re.sub(BR&#39;&#34;\s*\&amp;\s*&#34;&#39;, B&#39;&#39;, data)
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_vba_comments"><code class="flex name class">
<span>class <span class="ident">deob_vba_comments</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/vba/comments.py#L8-L10" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_vba_comments(Deobfuscator):
    def deobfuscate(self, data):
        return re.sub(R&#34;(?im)^\s{0,20}(?:&#39;|rem\b|dim\b).*(?:\Z|$\n\r?)&#34;, &#39;&#39;, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_vba_concat"><code class="flex name class">
<span>class <span class="ident">deob_vba_concat</span></span>
<span>(</span><span>timeout=100)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/vba/concat.py#L9-L36" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_vba_concat(IterativeDeobfuscator):
    _SENTINEL = re.compile(R&#39;&#39;&#39;&#34;\s*(\++|&amp;)\s*&#34;&#39;&#39;&#39;)

    def deobfuscate(self, data):

        def concat(data):
            strlit = StringLiterals(formats.vbastr, data)
            repeat = True
            while repeat:
                for match in self._SENTINEL.finditer(data):
                    a, b = match.span()
                    a = strlit.get_container(a)
                    if a is None:
                        continue
                    b = strlit.get_container(b)
                    if b is None or b != a + 1:
                        continue
                    _, a = strlit.ranges[a]
                    b, c = strlit.ranges[b]
                    yield data[:a - 1] + data[b + 1:c]
                    data = data[c:]
                    strlit.update(data)
                    break
                else:
                    repeat = False
            yield data

        return &#39;&#39;.join(concat(data))</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_vba_constants"><code class="flex name class">
<span>class <span class="ident">deob_vba_constants</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/vba/constants.py#L9-L41" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_vba_constants(Deobfuscator):
    def deobfuscate(self, data):
        codelines = data.splitlines(keepends=True)
        constants = {}
        constline = {}
        variables = set()
        for k, line in enumerate(codelines):
            match = re.match(R&#39;(?im)^\s*(?:sub|function)\s*(\w+)&#39;, line)
            if match:
                variables.add(match[1])
                continue
            match = re.match(
                R&#39;(?im)^(?:\s*const)?\s*(\w+)\s*=\s*({i}|{s})\s*(?:\&#39;|rem|$)&#39;.format(
                    s=formats.ps1str,
                    i=formats.integer
                ), line)
            if match is None or match[1] in variables:
                pass
            elif match[2] != constants.get(match[1], match[2]):
                self.log_debug(F&#39;del {match[1]}&#39;)
                del constants[match[1]]
                del constline[match[1]]
                variables.add(match[1])
            else:
                self.log_debug(F&#39;add {match[1]} = {match[2]}&#39;)
                constants[match[1]] = match[2]
                constline[match[1]] = k
        codelines = [line for k, line in enumerate(codelines) if k not in constline.values()]
        data = &#39;&#39;.join(codelines)
        for name, value in constants.items():
            data = re.sub(RF&#39;\b{re.escape(name)!s}\b&#39;, lambda _: value, data)

        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_vba_dummy_variables"><code class="flex name class">
<span>class <span class="ident">deob_vba_dummy_variables</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/vba/dummies.py#L10-L57" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_vba_dummy_variables(Deobfuscator):
    def deobfuscate(self, data):
        lines = data.splitlines(keepends=False)
        names = collections.defaultdict(list)

        def might_be_used_in(name, line):
            # avoid finding the name within a string literal
            line = &#39;&#34;&#34;&#39;.join(re.split(str(formats.ps1str), line))
            line = re.split(RF&#39;\b{name}\b&#39;, line)
            try:
                L, R = line
            except ValueError:
                return False
            L = L.strip().lower()
            if L.startswith(&#34;&#39;&#34;) or L.startswith(&#39;rem&#39;):
                return False
            R = R.strip().lower()
            if R.startswith(&#39;=&#39;) and &#39;if&#39; not in L:
                return False
            if L.startswith(&#39;dim&#39;):
                return False
            return True

        pattern = re.compile(
            R&#39;(?i)^\s{0,8}(?:const\s{1,8})?(\w+)\s{1,8}=\s{1,8}.*$&#39;
        )

        for k, line in enumerate(lines):
            try:
                name = pattern.match(line)[1]
            except (AttributeError, TypeError):
                continue
            if re.search(r&#39;\w+\(&#39;, line):
                # might be a function call
                continue
            names[name].append(k)

        for line in lines:
            while True:
                for name in names:
                    if might_be_used_in(name, line):
                        del names[name]
                        break
                else:
                    break

        return &#39;\n&#39;.join(line for k, line in enumerate(lines) if not any(
            k in rows for rows in names.values()))</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_vba_stringreplace"><code class="flex name class">
<span>class <span class="ident">deob_vba_stringreplace</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/vba/stringreplace.py#L10-L31" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_vba_stringreplace(Deobfuscator):

    _SENTINEL = re.compile((
        R&#39;(?i)\bReplace\s*\(&#39;  # the replace call
        R&#39;\s*({s}),&#39;           # haystack (with brackets)
        R&#39;\s*({s}),&#39;           # needle (with brackets)
        R&#39;\s*({s})\s*\)&#39;       # insert (with brackets)
    ).format(s=formats.vbastr), flags=re.IGNORECASE)

    def deobfuscate(self, data):
        strlit = StringLiterals(formats.vbastr, data)

        @strlit.outside
        def replacement(match: re.Match[str]):
            return string_quote(
                string_unquote(match[1]).replace(
                    string_unquote(match[2]),
                    string_unquote(match[3])
                )
            )

        return self._SENTINEL.sub(replacement, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_vba_stringreverse"><code class="flex name class">
<span>class <span class="ident">deob_vba_stringreverse</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/obfuscation/vba/stringreverse.py#L10-L24" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_vba_stringreverse(Deobfuscator):

    _SENTINEL = re.compile((
        R&#39;(?i)\bStrReverse\s*\(&#39;  # the reverse call
        R&#39;\s*({s})\s*\)&#39;          # string
    ).format(s=formats.vbastr), flags=re.IGNORECASE)

    def deobfuscate(self, data):
        strlit = StringLiterals(formats.vbastr, data)

        @strlit.outside
        def replacement(match: re.Match[str]):
            return string_quote(&#39;&#39;.join(reversed(string_unquote(match[1]))))

        return self._SENTINEL.sub(replacement, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.des"><code class="flex name class">
<span>class <span class="ident">des</span></span>
<span>(</span><span>key, *, iv=b'', padding=None, mode=None, raw=False, little_endian=False, segment_size=0, tag=(), aad=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>DES encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/des.py#L9-L12" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class des(StandardBlockCipherUnit, cipher=PyCryptoFactoryWrapper(DES)):
    &#34;&#34;&#34;
    DES encryption and decryption.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="refinery.shell.des3"><code class="flex name class">
<span>class <span class="ident">des3</span></span>
<span>(</span><span>key, *, iv=b'', padding=None, mode=None, raw=False, little_endian=False, segment_size=0, tag=(), aad=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>3-DES encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/des3.py#L9-L12" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class des3(StandardBlockCipherUnit, cipher=PyCryptoFactoryWrapper(DES3)):
    &#34;&#34;&#34;
    3-DES encryption and decryption.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="refinery.shell.deskd"><code class="flex name class">
<span>class <span class="ident">deskd</span></span>
<span>(</span><span>size=8)</span>
</code></dt>
<dd>
<section class="desc"><p>Stands for "DES Key Derivation". It implements the same functionality as <code>DES_string_to_key</code> in OpenSSL. It
converts a string to an 8 byte DES key with odd byte parity, per FIPS specification. This is not a modern
key derivation function.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/keyderive/deskd.py#L47-L85" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deskd(KeyDerivation):
    &#34;&#34;&#34;
    Stands for &#34;DES Key Derivation&#34;. It implements the same functionality as `DES_string_to_key` in OpenSSL. It
    converts a string to an 8 byte DES key with odd byte parity, per FIPS specification. This is not a modern
    key derivation function.
    &#34;&#34;&#34;
    def __init__(self, size: Param[int, Arg(help=&#39;The number of bytes to generate, default is the maximum of 8.&#39;)] = 8):
        super().__init__(size=size, salt=None)

    def process(self, password):
        from Cryptodome.Cipher import DES
        from Cryptodome.Util.strxor import strxor

        password = bytes(password)
        key = bytearray(8)

        for i, p in enumerate(password):
            if ((i % 16) &lt; 8):
                key[i % 8] ^= (p &lt;&lt; 1) &amp; 0xFF
            else:
                p = (((p &lt;&lt; 4) &amp; 0xf0) | ((p &gt;&gt; 4) &amp; 0x0f))
                p = (((p &lt;&lt; 2) &amp; 0xcc) | ((p &gt;&gt; 2) &amp; 0x33))
                p = (((p &lt;&lt; 1) &amp; 0xaa) | ((p &gt;&gt; 1) &amp; 0x55))
                key[7 - (i % 8)] ^= p

        des_set_odd_parity(key)

        if password:
            n = len(password)
            password = password.ljust(n + 7 - ((n - 1) % 8), b&#39;\0&#39;)
            des = DES.new(key, DES.MODE_ECB)
            for k in range(0, n, 8):
                key[:] = des.encrypt(strxor(password[k:k + 8], key))
            des_set_odd_parity(key)

        if self.args.size &gt; 8:
            raise RefineryPartialResult(&#39;can provide at most 8 bytes.&#39;, partial=key)

        return key[:self.args.size]</code></pre>
</details>
</dd>
<dt id="refinery.shell.dexstr"><code class="flex name class">
<span>class <span class="ident">dexstr</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extract strings from DEX (Dalvik Executable) files.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/dexstr.py#L7-L14" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dexstr(Unit):
    &#34;&#34;&#34;
    Extract strings from DEX (Dalvik Executable) files.
    &#34;&#34;&#34;
    def process(self, data):
        dex = DexFile(data)
        for string in dex.read_strings():
            yield string.encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.djb2"><code class="flex name class">
<span>class <span class="ident">djb2</span></span>
<span>(</span><span>reps=1, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Computes the DJB2 hash of the input data.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/hash/checksums.py#L27-L35" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class djb2(HashUnit):
    &#34;&#34;&#34;
    Computes the DJB2 hash of the input data.
    &#34;&#34;&#34;
    def _algorithm(self, data) -&gt; bytes:
        h = 5381
        for b in data:
            h = ((h &lt;&lt; 5) + h + b) &amp; 0xFFFFFFFF
        return h.to_bytes(4, &#39;big&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.dnarrays"><code class="flex name class">
<span>class <span class="ident">dnarrays</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts arrays of strings or integers that are encoded in the .NET binary as IL opcodes.
The data is exported as JSON.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pe/dotnet/dnarrays.py#L15-L133" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dnarrays(Unit):
    &#34;&#34;&#34;
    Extracts arrays of strings or integers that are encoded in the .NET binary as IL opcodes.
    The data is exported as JSON.
    &#34;&#34;&#34;
    @staticmethod
    def _read_int(reader: StructReader):
        value = reader.read_byte() - 0x16
        if value &lt; 0:
            raise ValueError
        elif value &lt;= 8:
            return value
        elif value == 9:
            return reader.read_byte()
        elif value == 10:
            return reader.u32()
        else:
            raise ValueError

    @staticmethod
    def _read_str(reader: StructReader, header: DotNetHeader):
        if reader.read_byte() != 0x72:
            raise ValueError
        token: int = reader.read_integer(24)
        value: str = header.meta.Streams.US[token]
        if reader.read_byte() != 0x70:
            raise ValueError
        return value

    _STACK_ARRAY_PATTERN_STR = re.compile(
        BR&#39;&#39;&#39;(?x)
        (?: [\x16-\x1E]|\x1F.|\x20.{4} ) # load array length
        (?:  \x8D...\x01               ) # newarr System.String
        (?:
        (?:  \x25                      ) # dup
        (?: [\x16-\x1E]|\x1F.|\x20.{4} ) # load integer index
        (?:  \x72...\x70               ) # load the string
        (?:  \xA2                      ) # stelem.ref
        ){4,}
        &#39;&#39;&#39;, flags=re.DOTALL)

    def _str_arrays(self, data: buf, header: DotNetHeader, tables: NetMetaDataTables):
        for match in self._STACK_ARRAY_PATTERN_STR.finditer(data):
            reader = StructReader(match[0])
            result: list[str] = []
            size = self._read_int(reader)
            if reader.read_byte() != 0x8D:
                raise RuntimeError
            stt = reader.read_integer(24)
            if reader.read_byte() != 0x01:
                raise RuntimeError
            if stt &lt; 1 or tables.TypeRef[stt - 1].TypeName != &#39;String&#39;:
                continue
            self.log_info(F&#39;str array pattern at 0x{match.start():X}, size {size}&#39;)
            for k in range(size):
                if reader.read_byte() != 0x25:
                    raise RuntimeError
                if self._read_int(reader) != k:
                    break
                result.append(self._read_str(reader, header))
                if reader.read_byte() != 0xA2:
                    raise RuntimeError
            else:
                yield match.start(), result

    _STACK_ARRAY_PATTERN_INT = re.compile(
        BR&#39;&#39;&#39;(?x)
        (    \x12.|\xFE\x0D..          ) # load array variable
        (?: [\x16-\x1E]|\x1F.|\x20.{4} ) # push integer value
        (?:  \x52                      ) # store value into array
        (?:
        (?:  \1                        ) # load same array variable
        (?: [\x16-\x1E]|\x1F.|\x20.{4} ) # load integer index
        (?:  \x58                      ) # add; compute offset
        (?: [\x16-\x1E]|\x1F.|\x20.{4} ) # push integer value
        (?:  \x52                      ) # store value into array
        ){4,}
        &#39;&#39;&#39;, flags=re.DOTALL)

    def _int_arrays(self, data: buf, header: DotNetHeader, tables: NetMetaDataTables):
        for match in self._STACK_ARRAY_PATTERN_INT.finditer(data):
            self.log_info(F&#39;int array pattern at 0x{match.start():X}&#39;)
            reader = StructReader(match[0])
            result: list[int] = []
            opc, = reader.peek(1)
            skip = {0x12: 2, 0xFE: 4}[opc]
            reader.seekrel(skip)
            for index in itertools.count(1):
                result.append(self._read_int(reader))
                assert reader.read_byte() == 0x52
                if reader.eof:
                    yield match.start(), result
                    break
                reader.seekrel(skip)
                if self._read_int(reader) != index:
                    self.log_info(&#39;index inconsistency; aborting&#39;)
                    break
                assert reader.read_byte() == 0x58

    def process(self, data):
        header = DotNetHeader(data)
        tables = header.meta.Streams.Tables
        cp = CodePath(header)

        arrays = dict(itertools.chain(
            self._int_arrays(data, header, tables),
            self._str_arrays(data, header, tables),
        ))
        result = collections.defaultdict(list)
        for offset in sorted(arrays):
            result[cp.method_spec(offset)].append(arrays[offset])

        result = {m: {F&#39;v{k}&#39;: v for k, v in enumerate(t, 1)} for m, t in result.items()}
        return json.dumps(result, indent=4).encode(self.codec)

    @classmethod
    def handles(cls, data):
        from refinery.lib.id import is_likely_pe_dotnet
        return is_likely_pe_dotnet(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.dnasm"><code class="flex name class">
<span>class <span class="ident">dnasm</span></span>
<span>(</span><span>*, count=None, until=None, no_il_refs=False, no_address=False, no_hexdump=False, no_args=False, description=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Disassembles the input data as MSIL (.NET/C# bytecode) and produces a human-readable disassembly listing. If you are
looking for a more programmatic disassembly, take a look at <code><a title="refinery.dnopc" href="index.html#refinery.dnopc">dnopc</a></code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/sinks/dnasm.py#L13-L65" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dnasm(DotnetDisassemblerUnit):
    &#34;&#34;&#34;
    Disassembles the input data as MSIL (.NET/C# bytecode) and produces a human-readable disassembly listing. If you are
    looking for a more programmatic disassembly, take a look at `refinery.dnopc`.
    &#34;&#34;&#34;

    def __init__(
        self, *,
        count=None, until=None,
        no_il_refs: Param[bool, Arg.Switch(&#39;-I&#39;, help=&#39;Disable reference resolution to IL_*.&#39;)] = False,
        no_address: Param[bool, Arg.Switch(&#39;-A&#39;, help=&#39;Disable address display.&#39;)] = False,
        no_hexdump: Param[bool, Arg.Switch(&#39;-H&#39;, help=&#39;Disable opcodes hexdump.&#39;)] = False,
        no_args: Param[bool, Arg.Switch(&#39;-O&#39;, help=&#39;Disable output of instruction arguments.&#39;)] = False,
        description: Param[bool, Arg.Switch(&#39;-d&#39;, help=&#39;Enable opcodes descriptions in output.&#39;)] = False,
    ):
        self._output_factory = OutputFactory(
            il_refs=not no_il_refs,
            address=not no_address,
            hexdump=not no_hexdump,
            arguments=not no_args,
        )
        self._disassembler = Disassembler()
        super().__init__(
            count=count,
            until=until,
            description=description,
        )

    def process(self, data):
        meta = metavars(data)
        r = re.compile(r&#39;t[0-9a-f]+&#39;, re.IGNORECASE)
        self._output_factory.extend_token_labels({int(k[1:], 16): v for k, v in meta.items() if r.match(k)})
        until = str(self.args.until or &#39;&#39;).lower()

        max_line_length = 0
        if self.args.description:
            disasm = []
            for ins in self._disassembler.disasm(data, self.args.count):
                disasm.append(ins)
                line = self._output_factory.instruction(ins)
                max_line_length = max(max_line_length, len(line))
        else:
            disasm = self._disassembler.disasm(data, self.args.count)

        for ins in disasm:
            line = self._output_factory.instruction(ins)
            if self.args.description:
                line += &#39; &#39; * (max_line_length - len(line) + 2)
                line += f&#39;-- {ins.op.description}&#39;
            yield line.encode(&#34;utf-8&#34;)

            if until and until in line.lower():
                break</code></pre>
</details>
</dd>
<dt id="refinery.shell.dnblob"><code class="flex name class">
<span>class <span class="ident">dnblob</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts all blobs defined in the <code>#Blob</code> stream of .NET executables.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pe/dotnet/dnblob.py#L7-L18" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dnblob(Unit):
    &#34;&#34;&#34;
    Extracts all blobs defined in the `#Blob` stream of .NET executables.
    &#34;&#34;&#34;
    def process(self, data):
        header = DotNetHeader(data, parse_resources=False)
        yield from header.meta.Streams.Blob.values()

    @classmethod
    def handles(cls, data):
        from refinery.lib.id import is_likely_pe_dotnet
        return is_likely_pe_dotnet(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.dnds"><code class="flex name class">
<span>class <span class="ident">dnds</span></span>
<span>(</span><span>dereference=True, encode=None, digest=None, arrays=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Stands for "DotNet DeSerialize": Expects data that has been serialized using the .NET class
"BinaryFormatter". The output is a representation of the deserialized data in JSON format.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pe/dotnet/dnds.py#L9-L37" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dnds(DotNetJSONEncoderUnit):
    &#34;&#34;&#34;
    Stands for &#34;DotNet DeSerialize&#34;: Expects data that has been serialized using the .NET class
    &#34;BinaryFormatter&#34;. The output is a representation of the deserialized data in JSON format.
    &#34;&#34;&#34;

    def __init__(
        self,
        dereference: Param[bool, Arg.Switch(&#39;-r&#39;, &#39;--keep-references&#39;, off=True,
            help=&#39;Do not resolve Object references in serialized data.&#39;)] = True,
        encode=None, digest=None, arrays=False
    ):
        super().__init__(encode=encode, digest=digest, arrays=arrays, dereference=dereference)

    def process(self, data):
        self.log_debug(&#39;initializing parser, will fail on malformed stream&#39;)
        bf = BinaryFormatterParser(
            data,
            keep_meta=True,
            dereference=self.args.dereference,
            ignore_errors=not self.log_debug(),
        )

        return self.to_json([
            {
                &#39;Type&#39;: repr(record),
                &#39;Data&#39;: record
            } for record in bf
        ])</code></pre>
</details>
</dd>
<dt id="refinery.shell.dnfields"><code class="flex name class">
<span>class <span class="ident">dnfields</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>This unit can extract data from constant field variables in classes of .NET
executables. Since the .NET header stores only the offset and not the size of
constant fields, heuristics are used to search for opcode sequences that load
the data and additional heuristics are used to guess the size of the data
type.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pe/dotnet/dnfields.py#L22-L187" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dnfields(PathExtractorUnit):
    &#34;&#34;&#34;
    This unit can extract data from constant field variables in classes of .NET
    executables. Since the .NET header stores only the offset and not the size of
    constant fields, heuristics are used to search for opcode sequences that load
    the data and additional heuristics are used to guess the size of the data
    type.
    &#34;&#34;&#34;
    @classmethod
    def handles(cls, data):
        from refinery.lib.id import is_likely_pe_dotnet
        return is_likely_pe_dotnet(data)

    def unpack(self, data):
        header = DotNetHeader(data, parse_resources=False)
        tables = header.meta.Streams.Tables
        fields = tables.FieldRVA
        cpaths = CodePath(header)

        if not fields:
            return

        icache: dict[bytes, FieldInfo] = {}
        memory = memoryview(data)

        def _guess_field_info(t: int, signature: bytes, field_name: str | None = None, sizemap: dict = {
            &#39;^s?byte$&#39;       : 1,
            &#39;^s?char$&#39;       : 2,
            &#39;^[us]?int.?16$&#39; : 2,
            &#39;^[us]?int.?32$&#39; : 4,
            &#39;^[us]?int.?64$&#39; : 8,
        }) -&gt; tuple[str | None, FieldInfo]:
            try:
                info = icache[signature]
            except KeyError:
                info = None
            else:
                if field_name is not None:
                    return field_name, info
            pattern = (
                BR&#39;(\x20....|\x1F.|[\x17-\x1E])&#39;    # ldc.i4  count
                BR&#39;\x8D(...)([\x01\x02])&#39;           # newarr  col|row
                BR&#39;\x25&#39;                            # dup
                BR&#39;\xD0\x%02x\x%02x\x%02x\x04&#39;      # ldtoken t
                BR&#39;(?:.{0,12}?&#39;                     # ...
                BR&#39;\x80(...)\x04)?&#39; % (             # stsfld variable
                    (t &gt;&gt; 0x00) &amp; 0xFF,
                    (t &gt;&gt; 0x08) &amp; 0xFF,
                    (t &gt;&gt; 0x10) &amp; 0xFF
                )
            )
            for match in re.finditer(pattern, memory, flags=re.DOTALL):
                if info is None:
                    count, j, r, name = match.groups()
                    count = integer_from_ldc(count)
                    j, r = struct.unpack(&#39;&lt;LB&#39;, B&#39;%s\0%s&#39; % (j, r))
                    typename = tables[r][j - 1].TypeName
                else:
                    name = match.group(4)
                    typename = info.type
                for pattern, size in sizemap.items():
                    if not re.match(pattern, typename, flags=re.IGNORECASE):
                        continue
                    if name:
                        try:
                            name = struct.unpack(&#39;&lt;L&#39;, B&#39;%s\0&#39; % name)
                            name = name[0]
                            name = tables[4][name - 1].Name
                        except Exception as E:
                            self.log_info(F&#39;attempt to parse field name failed: {E!s}&#39;)
                            name = None
                    if name is None:
                        name = field_name
                    if info is None:
                        info = FieldInfo(typename, count, size, match.start())
                    icache[signature] = info
                    return name, info
            else:
                return None, None

        iwidth = len(str(len(fields)))
        rwidth = max(len(F&#39;{field.RVA:X}&#39;) for field in fields)
        rwidth = max(rwidth, 4)
        remaining_field_indices = set(range(len(tables.Field)))

        unpack = []
        name_count = Counter(tables.Field[rv.Field.Index - 1].Name for rv in fields)
        name_width = len(str(len(fields)))

        for k, rv in enumerate(fields):
            _index = rv.Field.Index
            field = tables.Field[_index - 1]
            remaining_field_indices.discard(_index - 1)
            if not field.Flags.HasFieldRVA:
                continue
            fname = field.Name
            type = None
            signature: bytes = field.Signature
            offset = header.pe.rva_to_offset(rv.RVA)

            if len(signature) == 2:
                # Crude signature parser for non-array case. Reference:
                # https://www.codeproject.com/Articles/42649/NET-File-Format-Signatures-Under-the-Hood-Part-1
                # https://www.codeproject.com/Articles/42655/NET-file-format-Signatures-under-the-hood-Part-2
                guess = {
                    0x03: FieldInfo(&#39;Char&#39;,   1, 1, 0),  # noqa
                    0x04: FieldInfo(&#39;SByte&#39;,  1, 1, 0),  # noqa
                    0x05: FieldInfo(&#39;Byte&#39;,   1, 1, 0),  # noqa
                    0x06: FieldInfo(&#39;Int16&#39;,  1, 2, 0),  # noqa
                    0x07: FieldInfo(&#39;UInt16&#39;, 1, 2, 0),  # noqa
                    0x08: FieldInfo(&#39;Int32&#39;,  1, 4, 0),  # noqa
                    0x09: FieldInfo(&#39;UInt32&#39;, 1, 4, 0),  # noqa
                    0x0A: FieldInfo(&#39;Int64&#39;,  1, 8, 0),  # noqa
                    0x0B: FieldInfo(&#39;UInt64&#39;, 1, 8, 0),  # noqa
                    0x0C: FieldInfo(&#39;Single&#39;, 1, 4, 0),  # noqa
                    0x0D: FieldInfo(&#39;Double&#39;, 1, 8, 0),  # noqa
                }.get(signature[1], None)
            else:
                fname, guess = _guess_field_info(_index, signature, fname)

            if guess is None:
                self.log_warn(lambda: F&#39;field {k:0{iwidth}d} with signature {field.Signature.hex()}: unable to guess type information&#39;)
                continue
            if not fname.isprintable() or name_count[fname] &gt; 1:
                fname = F&#39;Field{k + 1:0{name_width}d}&#39;
            type = guess.type.lower()
            if guess.count &gt; 1:
                type += F&#39;[{guess.count}]&#39;
            self.log_debug(
                F&#39;field {k:0{iwidth}d}; token 0x{_index:06X}; RVA 0x{rv.RVA:04X}; count {guess.count}; type {guess.type}; name {fname}&#39;)
            end = offset + guess.count * guess.size
            path = cpaths.method_path(guess.offset) if guess.offset else &#39;&#39;
            unpack.append(UnpackResult(F&#39;{path}/{fname}&#39;, memory[offset:end], name=fname, type=type))

        for _index in remaining_field_indices:
            field = tables.Field[_index]
            index = _index + 1
            name = field.Name
            if field.Flags.HasFieldRVA:
                self.log_warn(F&#39;field {name} has RVA flag set, but no RVA was found&#39;)
            token = index.to_bytes(3, &#39;little&#39;)
            values = {}
            for match in re.finditer((
                BR&#39;\x72(?P&lt;token&gt;...)\x70&#39;          # ldstr
                BR&#39;(?:\x6F(?P&lt;function&gt;...)\x0A)?&#39;  # call GetBytes
                BR&#39;\x80%s\x04&#39;                      # stsfld
            ) % re.escape(token), data, re.DOTALL):
                md = match.groupdict()
                fn_token = md.get(&#39;function&#39;)
                fn_index = fn_token and int.from_bytes(fn_token, &#39;little&#39;) or None
                if fn_index is not None:
                    fn_name = tables.MemberRef[fn_index].Name
                    if fn_name != &#39;GetBytes&#39;:
                        self.log_info(F&#39;skipping string assignment passing through call to {fn_name}&#39;)
                        continue
                k = int.from_bytes(md[&#39;token&#39;], &#39;little&#39;)
                values[match.start()] = header.meta.Streams.US[k].encode(self.codec)
            if not values:
                continue
            if len(values) == 1:
                offset, value = values.popitem()
                path = cpaths.method_path(offset)
                unpack.append(UnpackResult(F&#39;{path}/{name}&#39;, value, name=name, type=&#39;string&#39;))

        unpack.sort(key=lambda u: u.path)
        yield from unpack</code></pre>
</details>
</dd>
<dt id="refinery.shell.dnhdr"><code class="flex name class">
<span>class <span class="ident">dnhdr</span></span>
<span>(</span><span>resources=False, encode=None, digest=None, arrays=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Expects data that has been formatted with the <code>BinaryFormatter</code> class. The
output is a representation of the deserialized data in JSON format.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pe/dotnet/dnhdr.py#L9-L36" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dnhdr(DotNetJSONEncoderUnit):
    &#34;&#34;&#34;
    Expects data that has been formatted with the `BinaryFormatter` class. The
    output is a representation of the deserialized data in JSON format.
    &#34;&#34;&#34;
    def __init__(
        self,
        resources: Param[bool, Arg.Switch(&#39;-r&#39;, &#39;--resources&#39;, help=&#39;Also parse .NET resources.&#39;)] = False,
        encode=None, digest=None, arrays=False,
    ):
        super().__init__(encode=encode, digest=digest, arrays=arrays, resources=resources)

    def process(self, data):
        dn = DotNetHeader(data, parse_resources=self.args.resources)
        dn = {
            &#39;Head&#39;: dn.head,
            &#39;Meta&#39;: dn.meta
        }

        if self.args.resources:
            dn[&#39;RSRC&#39;] = dn.resources

        return self.to_json(dn)

    @classmethod
    def handles(cls, data):
        from refinery.lib.id import is_likely_pe_dotnet
        return is_likely_pe_dotnet(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.dnmr"><code class="flex name class">
<span>class <span class="ident">dnmr</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, exact=False, fuzzy=0, regex=False, path=b'name', raw=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts subfiles from .NET managed resources.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pe/dotnet/dnmr.py#L10-L50" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dnmr(PathExtractorUnit):
    &#34;&#34;&#34;
    Extracts subfiles from .NET managed resources.
    &#34;&#34;&#34;
    def __init__(
        self, *paths, list=False, join_path=False, drop_path=False, exact=False, fuzzy=0, regex=False, path=b&#39;name&#39;,
        raw: Param[bool, Arg.Switch(&#39;-w&#39;, help=&#39;Do not deserialize the managed resource entry data.&#39;)] = False
    ):
        super().__init__(
            *paths,
            list=list,
            join_path=join_path,
            drop_path=drop_path,
            path=path,
            raw=raw,
            fuzzy=fuzzy,
            exact=exact,
            regex=regex,
        )

    def unpack(self, data):
        try:
            managed = NetStructuredResources(data)
        except NoManagedResource:
            managed = None
        if not managed:
            raise RefineryPartialResult(&#39;no managed resources found&#39;, partial=data)
        for entry in managed:
            if entry.Error:
                self.log_warn(F&#39;entry {entry.Name} carried error message: {entry.Error}&#39;)
            data = entry.Data
            if not self.args.raw:
                if isinstance(entry.Value, str):
                    data = entry.Value.encode(&#39;utf-16le&#39;)
                elif isbuffer(entry.Value):
                    data = entry.Value
            yield UnpackResult(entry.Name, data)

    @classmethod
    def handles(cls, data):
        return data[:4] == b&#39;\xCE\xCA\xEF\xBE&#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.dnopc"><code class="flex name class">
<span>class <span class="ident">dnopc</span></span>
<span>(</span><span>*, count=None, until=None, nvar='name', avar='addr', ovar='arg')</span>
</code></dt>
<dd>
<section class="desc"><p>Disassembles the input data as MSIL (.NET/C# bytecode) and generates opcodes with metadata as output. This
is useful for programmatic disassembly, while the <code><a title="refinery.dnasm" href="index.html#refinery.dnasm">dnasm</a></code> unit outputs a human-readable
representation.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pe/dotnet/dnopc.py#L30-L80" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dnopc(DotnetDisassemblerUnit):
    &#34;&#34;&#34;
    Disassembles the input data as MSIL (.NET/C# bytecode) and generates opcodes with metadata as output. This
    is useful for programmatic disassembly, while the `refinery.dnasm` unit outputs a human-readable
    representation.
    &#34;&#34;&#34;

    def __init__(
        self,
        *,
        count=None,
        until=None,
        nvar: Param[str, Arg.String(
            &#39;-n&#39;,
            help=&#39;Variable to receive the disassembled mnemonic. Default is &#34;{default}&#34;.&#39;,
        )] = &#39;name&#39;,
        avar: Param[str, Arg.String(
            &#39;-a&#39;,
            help=&#39;Variable to receive the address of the instruction. Default is &#34;{default}&#34;.&#39;,
        )] = &#39;addr&#39;,
        ovar: Param[str, Arg.String(
            &#39;-o&#39;,
            help=(&#39;Variable prefix for instruction operands. Default is &#34;{default}&#34;. The complete operand &#39;
                  &#39;string will be in {default}s, the first argument in {default}1, the second in {default}2, &#39;
                  &#39;and so on.&#39;),
        )] = &#39;arg&#39;,
        **more
    ):
        super().__init__(
            count=count,
            until=until,
            nvar=nvar,
            avar=avar,
            ovar=ovar,
            **more
        )

    def process(self, data):
        until = str(self.args.until or &#39;&#39;).lower()
        factory = OutputFactory()
        for ins in Disassembler().disasm(data, self.args.count):
            kwargs = {
                self.args.avar: ins.offset,
                self.args.nvar: ins.op.mnemonic,
            }
            for k, arg in enumerate(ins.arguments, 1):
                kwargs[F&#39;{self.args.ovar}{k}&#39;] = arg.value
            yield self.labelled(ins.data, **kwargs)

            if until and until in factory.instruction(ins).lower():
                break</code></pre>
</details>
</dd>
<dt id="refinery.shell.dnrc"><code class="flex name class">
<span>class <span class="ident">dnrc</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts all .NET resources whose name matches any of the given patterns
and outputs them. Use the <code><a title="refinery.units.formats.pe.dotnet.dnmr" href="units/formats/pe/dotnet/dnmr.html">refinery.units.formats.pe.dotnet.dnmr</a></code> unit to
extract subfiles from managed .NET resources.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pe/dotnet/dnrc.py#L7-L27" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dnrc(PathExtractorUnit):
    &#34;&#34;&#34;
    Extracts all .NET resources whose name matches any of the given patterns
    and outputs them. Use the `refinery.units.formats.pe.dotnet.dnmr` unit to
    extract subfiles from managed .NET resources.
    &#34;&#34;&#34;
    def unpack(self, data):
        header = DotNetHeader(data)

        if not header.resources:
            if self.args.list:
                return
            raise ValueError(&#39;This file contains no resources.&#39;)

        for resource in header.resources:
            yield UnpackResult(resource.Name, resource.Data)

    @classmethod
    def handles(cls, data):
        from refinery.lib.id import is_likely_pe_dotnet
        return is_likely_pe_dotnet(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.dnsdomain"><code class="flex name class">
<span>class <span class="ident">dnsdomain</span></span>
<span>(</span><span>min=1, max=0, len=0, stripspace=False, duplicates=False, longest=False, take=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts domain names in the format as they appear in DNS requests. This
can be used as a quick and dirty way to extract domains from PCAP files,
for example.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/pattern/dnsdomain.py#L13-L43" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dnsdomain(PatternExtractorBase):
    &#34;&#34;&#34;
    Extracts domain names in the format as they appear in DNS requests. This
    can be used as a quick and dirty way to extract domains from PCAP files,
    for example.
    &#34;&#34;&#34;

    _DOMAIN_CHARACTERS = (
        B&#39;ABCDEFGHIJKLMNOPQRSTUVWXYZ&#39;
        B&#39;abcdefghijklmnopqrstuvwxyz&#39;
        B&#39;0123456789-_&#39;
    )

    _DOMAIN_PATTERN = BR&#39;(?:%s){1,20}(?:%s)\b&#39; % (_lps(0xFF), _lps(25))

    def process(self, data):

        def transform(match):
            match = bytearray(match[0])
            pos = 0
            while pos &lt; len(match):
                length = match[pos]
                match[pos] = 0x2E
                if len(match) &lt; length + pos:
                    return None
                if any(x not in self._DOMAIN_CHARACTERS for x in match[pos + 1 : pos + length]):
                    return None
                pos += 1 + length
            return match[1:]

        yield from self.matches_filtered(memoryview(data), self._DOMAIN_PATTERN, transform)</code></pre>
</details>
</dd>
<dt id="refinery.shell.dnsfx"><code class="flex name class">
<span>class <span class="ident">dnsfx</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts files from .NET single file applications.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pe/dotnet/dnsfx.py#L10-L79" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dnsfx(PathExtractorUnit):
    &#34;&#34;&#34;
    Extracts files from .NET single file applications.
    &#34;&#34;&#34;
    _SIGNATURE = bytes([
        # 32 bytes represent the bundle signature: SHA-256 for &#39;.net core bundle&#39;
        0x8b, 0x12, 0x02, 0xb9, 0x6a, 0x61, 0x20, 0x38,
        0x72, 0x7b, 0x93, 0x02, 0x14, 0xd7, 0xa0, 0x32,
        0x13, 0xf5, 0xb9, 0xe6, 0xef, 0xae, 0x33, 0x18,
        0xee, 0x3b, 0x2d, 0xce, 0x24, 0xb3, 0x6a, 0xae
    ])

    def unpack(self, data):
        reader = StreamReader(data)
        reader.seek(self._find_bundle_manifest_offset(data))

        major_version = reader.expect(UInt32)
        minor_version = reader.expect(UInt32)
        self.log_info(F&#39;version {major_version}.{minor_version}&#39;)

        count = reader.expect(UInt32)
        bhash = reader.expect(StringPrimitive)
        self.log_info(F&#39;bundle {bhash} contains {count} files&#39;)

        if major_version &gt;= 2:
            reader.expect(UInt64) # depsOffset
            reader.expect(UInt64) # depsSize
            reader.expect(UInt64) # runtimeConfigOffset
            reader.expect(UInt64) # runtimeConfigSize
            reader.expect(UInt64) # flags

        for _ in range(count):
            try:
                offset = reader.expect(UInt64)
                size = reader.expect(UInt64)
                compressed_size = 0
                if major_version &gt;= 6:
                    compressed_size = reader.expect(UInt64)
                type = reader.expect(Byte)
                path = reader.expect(StringPrimitive)

                def _logmsg():
                    _log = F&#39;read item at offset 0x{offset:08X}, type 0x{type:02X}, size {SizeInt(size)!r}&#39;
                    if compressed_size:
                        return F&#39;{_log}, compressed to size {SizeInt(compressed_size)!r}&#39;
                    return F&#39;{_log}, uncompressed&#39;

                self.log_debug(_logmsg)

                with reader.checkpoint():
                    reader.seek(offset)
                    if compressed_size:
                        item_data = reader.read(compressed_size) | zl | bytearray
                    else:
                        item_data = reader.read(size)

                yield UnpackResult(path, item_data)
            except ParserEOF:
                self.log_warn(&#39;unexpected EOF while parsing bundle, terminating&#39;)
                break

    def _find_bundle_manifest_offset(self, data: bytearray) -&gt; int:
        bundle_sig_offset = data.find(self._SIGNATURE, 0)
        if bundle_sig_offset &lt; 0:
            raise ValueError(&#39;Cannot find valid Bundle Manifest offset. Is this a .NET Bundle?&#39;)
        return int.from_bytes(data[bundle_sig_offset - 8:bundle_sig_offset], &#39;little&#39;)

    @classmethod
    def handles(cls, data):
        return buffer_contains(data, cls._SIGNATURE)</code></pre>
</details>
</dd>
<dt id="refinery.shell.dnstr"><code class="flex name class">
<span>class <span class="ident">dnstr</span></span>
<span>(</span><span>user=True, meta=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts all strings defined in the <code>#Strings</code> and <code>#US</code> streams of .NET executables.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pe/dotnet/dnstr.py#L8-L34" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dnstr(Unit):
    &#34;&#34;&#34;
    Extracts all strings defined in the `#Strings` and `#US` streams of .NET executables.
    &#34;&#34;&#34;

    def __init__(
        self,
        user: Param[bool, Arg.Switch(&#39;-m&#39;, &#39;--meta&#39;, off=True, group=&#39;HEAP&#39;, help=&#39;Only extract from #Strings.&#39;)] = True,
        meta: Param[bool, Arg.Switch(&#39;-u&#39;, &#39;--user&#39;, off=True, group=&#39;HEAP&#39;, help=&#39;Only extract from #US.&#39;)] = True,
    ):
        if not meta and not user:
            raise ValueError(&#39;Either ascii or utf16 strings must be enabled.&#39;)
        super().__init__(meta=meta, user=user)

    def process(self, data):
        header = DotNetHeader(data, parse_resources=False)
        if self.args.meta:
            for string in header.meta.Streams.Strings.values():
                yield string.encode(self.codec)
        if self.args.user:
            for string in header.meta.Streams.US.values():
                yield string.encode(self.codec)

    @classmethod
    def handles(cls, data):
        from refinery.lib.id import is_likely_pe_dotnet
        return is_likely_pe_dotnet(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.docmeta"><code class="flex name class">
<span>class <span class="ident">docmeta</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract metadata from Word Documents such as custom document properties.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/office/docmeta.py#L8-L29" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class docmeta(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract metadata from Word Documents such as custom document properties.
    &#34;&#34;&#34;
    @PathExtractorUnit.Requires(&#39;olefile&#39;, [&#39;formats&#39;, &#39;office&#39;])
    def _olefile():
        import olefile
        return olefile

    def unpack(self, data: bytearray):
        properties = data | xtdoc(&#39;docProps/custom.xml&#39;) | str
        if not properties:
            return
        properties = xml.parse(properties)
        while properties.tag.lower() != &#39;properties&#39;:
            properties = properties.children[0]
        for node in properties:
            assert node.tag.lower() == &#39;property&#39;
            assert len(node.children) == 1
            content = node.children[0].content
            assert content is not None
            yield UnpackResult(node.attributes[&#39;name&#39;], content.encode(self.codec))</code></pre>
</details>
</dd>
<dt id="refinery.shell.doctxt"><code class="flex name class">
<span>class <span class="ident">doctxt</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts the text body from Word documents.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/office/doctxt.py#L18-L153" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class doctxt(Unit):
    &#34;&#34;&#34;
    Extracts the text body from Word documents.
    &#34;&#34;&#34;

    @Unit.Requires(&#39;olefile&#39;, [&#39;formats&#39;, &#39;office&#39;, &#39;extended&#39;])
    def _olefile():
        import olefile
        return olefile

    def process(self, data: bytearray):
        extractors: dict[str, Callable[[bytearray], str]] = OrderedDict(
            doc=self._extract_ole,
            docx=self._extract_docx,
            odt=self._extract_odt,
        )
        if data.startswith(B&#39;PK&#39;):
            self.log_debug(&#39;document contains zip file signature, likely a odt or docx file&#39;)
            extractors.move_to_end(&#39;doc&#39;)
            if &#39;opendocument&#39; in str(data | xtzip(&#39;mimetype&#39;)):
                self.log_debug(&#39;odt signature detected&#39;)
                extractors.move_to_end(&#39;odt&#39;, last=False)
        for filetype, extractor in extractors.items():
            self.log_debug(F&#39;trying to extract as {filetype}&#39;)
            try:
                result = extractor(data)
            except ImportError:
                raise
            except Exception as error:
                self.log_info(F&#39;failed extractring as {filetype}: {error!s}&#39;)
            else:
                return result.encode(self.codec)
        raise ValueError(&#39;All extractors failed, the input data is not recognized as any known document format.&#39;)

    def _extract_docx(self, data: Chunk) -&gt; str:
        NAMESPACE = &#39;{http://schemas.openxmlformats.org/wordprocessingml/2006/main}&#39;
        PARAGRAPH = F&#39;{NAMESPACE}p&#39;
        TEXT = F&#39;{NAMESPACE}t&#39;
        chunk = data | xtzip(&#39;word/document.xml&#39;) | bytearray
        if not chunk:
            raise ValueError(&#39;No document.xml file found.&#39;)
        root: Element = XML(chunk)
        with StringIO() as output:
            for index, paragraph in enumerate(root.iter(PARAGRAPH)):
                if index &gt; 0:
                    output.write(&#39;\n&#39;)
                for node in paragraph.iter(TEXT):
                    if node.text:
                        output.write(node.text)
            return output.getvalue()

    def _extract_odt(self, data: bytes):
        def _extract_text(node: Element):
            NAMESPACE = &#39;{urn:oasis:names:tc:opendocument:xmlns:text:1.0}&#39;
            PARAGRAPH = F&#39;{NAMESPACE}p&#39;
            SPAN = F&#39;{NAMESPACE}span&#39;
            SPACE = F&#39;{NAMESPACE}s&#39;
            with StringIO() as res:
                for element in node:
                    tag = element.tag
                    text = element.text or &#39;&#39;
                    tail = element.tail or &#39;&#39;
                    if tag in [PARAGRAPH, SPAN]:
                        res.write(text)
                    elif tag == SPACE:
                        res.write(&#39; &#39;)
                    else:
                        self.log_debug(F&#39;unknown tag: {tag}&#39;)
                    res.write(_extract_text(element))
                    res.write(tail)
                    if tag == PARAGRAPH:
                        res.write(&#39;\n&#39;)
                return res.getvalue()

        NAMESPACE = &#39;{urn:oasis:names:tc:opendocument:xmlns:office:1.0}&#39;
        BODY = F&#39;{NAMESPACE}body&#39;
        TEXT = F&#39;{NAMESPACE}text&#39;
        for part in xtzip().unpack(data):
            if part.path != &#39;content.xml&#39;:
                continue
            xml_content: bytes = part.get_data()
            root: Element = XML(xml_content)
            body: Element = root.find(BODY)
            text: Element = body.find(TEXT)
            return _extract_text(text)
        else:
            raise ValueError(&#39;found no text&#39;)

    def _extract_ole(self, data: bytearray) -&gt; str:
        stream = MemoryFile(data)
        with self._olefile.OleFileIO(stream) as ole:
            doc = ole.openstream(&#39;WordDocument&#39;).read()
            with StructReader(doc) as reader:
                table_name = F&#39;{(doc[11] &gt;&gt; 1) &amp; 1}Table&#39;
                reader.seek(0x1A2)
                offset = reader.u32()
                length = reader.u32()
            with StructReader(ole.openstream(table_name).read()) as reader:
                reader.seek(offset)
                table = reader.read(length)
            piece_table = self._load_piece_table(table)
            return self._get_text(doc, piece_table)

    def _load_piece_table(self, table: bytes) -&gt; bytes:
        with StructReader(table) as reader:
            while not reader.eof:
                entry_type = reader.read_byte()
                if entry_type == 1:
                    reader.seekrel(reader.read_byte())
                    continue
                if entry_type == 2:
                    length = reader.u32()
                    return reader.read(length)
                raise NotImplementedError(F&#39;Unsupported table entry type value 0x{entry_type:X}.&#39;)

    def _get_text(self, doc: bytes, piece_table: bytes) -&gt; str:
        piece_count: int = 1 + (len(piece_table) - 4) // 12
        with StringIO() as text:
            with StructReader(piece_table) as reader:
                character_positions = [reader.u32() for _ in range(piece_count)]
                for i in range(piece_count - 1):
                    cp_start = character_positions[i]
                    cp_end = character_positions[i + 1]
                    fc_value = reader.read_one_struct(&#39;xxLxx&#39;)
                    is_ansi = bool((fc_value &gt;&gt; 30) &amp; 1)
                    fc = fc_value &amp; 0xBFFFFFFF
                    cb = cp_end - cp_start
                    if is_ansi:
                        encoding = &#39;cp1252&#39;
                        fc = fc // 2
                    else:
                        encoding = &#39;utf16&#39;
                        cb *= 2
                    raw = doc[fc : fc + cb]
                    text.write(raw.decode(encoding).replace(&#39;\r&#39;, &#39;\n&#39;))
            return text.getvalue()</code></pre>
</details>
</dd>
<dt id="refinery.shell.drp"><code class="flex name class">
<span>class <span class="ident">drp</span></span>
<span>(</span><span>consecutive=False, align=False, min=1, max=0, len=0, all=False, threshold=20, weight=0, buffer=1024, chug=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Detect Repeating Patterns - detects the most prevalent repeating byte pattern
in a chunk of data. The unit computes a suffix tree which may require a lot of
memory for large buffers.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/misc/drp.py#L25-L197" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class drp(Unit):
    &#34;&#34;&#34;
    Detect Repeating Patterns - detects the most prevalent repeating byte pattern
    in a chunk of data. The unit computes a suffix tree which may require a lot of
    memory for large buffers.
    &#34;&#34;&#34;
    def __init__(
        self,
        consecutive: Param[bool, Arg.Switch(&#39;-c&#39;,
            help=&#39;Assume that the repeating pattern is consecutive when observable.&#39;)] = False,
        align: Param[bool, Arg.Switch(&#39;-d&#39;,
            help=&#39;Assume that the pattern occurs at offsets that are multiples of its length.&#39;)] = False,
        min: Param[int, Arg.Number(&#39;-n&#39;,
            help=&#39;Minimum size of the pattern to search for. Default is {default}.&#39;)] = 1,
        max: Param[int, Arg.Number(&#39;-N&#39;,
            help=&#39;Maximum size of the pattern to search for. Default is infinite.&#39;)] = 0,
        len: Param[int, Arg.Number(&#39;-l&#39;,
            help=&#39;Set the exact size of the pattern. This is equivalent to --min=N --max=N.&#39;)] = 0,
        all: Param[bool, Arg.Switch(&#39;-a&#39;,
            help=&#39;Produce one output for each repeating pattern that was detected.&#39;)] = False,
        threshold: Param[int, Arg.Number(&#39;-t&#39;,
            help=&#39;Patterns must match this performance threshold in percent, lest they be discarded.&#39;)] = 20,
        weight: Param[int, Arg.Number(&#39;-w&#39;,
            help=&#39;Specifies how much longer patterns are favored over small ones. Default is {default}.&#39;)] = 0,
        buffer: Param[int, Arg.Number(&#39;-b&#39;, group=&#39;BFR&#39;,
            help=&#39;Maximum number of bytes to inspect at once. The default is {default}.&#39;)] = 1024,
        chug: Param[bool, Arg.Switch(&#39;-g&#39;, group=&#39;BFR&#39;,
            help=&#39;Compute the prefix tree for the entire buffer instead of chunking it.&#39;)] = False
    ):
        if len &gt;= 1:
            min = max = len
        super().__init__(
            min=min,
            max=max or INF,
            all=all,
            consecutive=consecutive,
            align=align,
            weight=weight,
            buffer=buffer,
            chug=chug,
            threshold=threshold
        )

    def _get_patterns(self, data):
        with stackdepth(len(data)):
            tree = SuffixTree(data)
        min_size = self.args.min
        max_size = self.args.max
        patterns = set()
        cursor = 0
        while cursor &lt; len(data):
            node = tree.root
            rest = data[cursor:]
            remaining = len(rest)
            length = 0
            offset = None
            while node.children and length &lt; remaining:
                for child in node.children.values():
                    if tree.data[child.start] == rest[length]:
                        node = child
                        break
                if node.start &gt;= cursor:
                    break
                offset = node.start - length
                length = node.end + 1 - offset
            if offset is None:
                cursor += 1
                continue
            length = min(remaining, length)
            if max_size &gt;= length &gt;= min_size:
                pattern = rest[:length].tobytes()
                patterns.add(pattern)
            cursor += length
        del tree
        return patterns

    @staticmethod
    def _consecutive_count(data, pattern):
        length = len(pattern)
        if length == 1:
            return data.count(pattern)
        view = memoryview(data)
        return max(sum(1 for i in range(k, len(view), length) if view[i:i + length] == pattern)
            for k in range(len(pattern)))

    @staticmethod
    def _truncate_pattern(pattern):
        offset = 0
        for byte in pattern[1:]:
            if byte == pattern[offset]:
                offset += 1
            else:
                offset = 0
        if offset &gt; 0:
            pattern = pattern[:-offset]
        return pattern

    def process(self, data: bytearray):
        if len(data) &lt;= 1:
            yield data
            return

        memview = memoryview(data)
        weight = 1 + (self.args.weight / 10)

        if self.args.chug:
            patterns = self._get_patterns(memview)
        else:
            patterns = set()
            chunksize = self.args.buffer
            for k in range(0, len(memview), chunksize):
                patterns |= self._get_patterns(memview[k:k + chunksize])
        if not patterns:
            raise RefineryPartialResult(&#39;no repeating sequences found&#39;, data)

        self.log_debug(&#39;removing duplicate pattern detections&#39;)
        duplicates = set()
        maxlen = max(len(p) for p in patterns)
        for pattern in sorted(patterns, key=len):
            for k in range(2, maxlen // len(pattern) + 1):
                repeated = pattern * k
                if repeated in patterns:
                    duplicates.add(repeated)
        patterns -= duplicates

        self.log_debug(F&#39;counting coverage of {len(patterns)} patterns&#39;)
        pattern_count = {p: data.count(p) for p in patterns}
        pattern_performance = dict(pattern_count)

        for consecutive in (False, True):
            if consecutive:
                self.log_debug(F&#39;re-counting coverage of {len(patterns)} patterns&#39;)
                patterns = {self._truncate_pattern(p) for p in patterns}
                pattern_performance = {p: self._consecutive_count(data, p) for p in patterns}

            self.log_debug(&#39;evaluating pattern performance&#39;)
            for pattern, count in pattern_performance.items():
                pattern_performance[pattern] = count * (len(pattern) ** weight)
            best_performance = max(pattern_performance.values())
            for pattern, performance in pattern_performance.items():
                pattern_performance[pattern] = performance / best_performance

            self.log_debug(&#39;removing patterns below performance threshold&#39;)
            threshold = self.args.threshold
            patterns = {p for p in patterns if pattern_performance[p] * 100 &gt;= threshold}
            pattern_count = {p: data.count(p) for p in patterns}

            if not self.args.consecutive:
                break

        if self.args.all:
            for pattern in sorted(patterns, key=pattern_performance.get, reverse=True):
                yield self.labelled(pattern, count=pattern_count[pattern])
            return

        best_patterns = [p for p in patterns if pattern_performance[p] == 1.0]

        if len(best_patterns) &gt; 1:
            self.log_warn(&#39;could not determine unique best repeating pattern, returning the first of these:&#39;)
            for k, pattern in enumerate(best_patterns):
                self.log_warn(F&#39;{k:02d}.: {pattern.hex()}&#39;)

        result = best_patterns[0]

        if self.args.align:
            def rotated(pattern):
                for k in range(len(pattern)):
                    yield pattern[k:] + pattern[:k]
            rotations = {k % len(result): r for k, r in (
                (data.find(r), r) for r in rotated(result)) if k &gt;= 0}
            result = rotations[min(rotations)]

        yield result</code></pre>
</details>
</dd>
<dt id="refinery.shell.dsjava"><code class="flex name class">
<span>class <span class="ident">dsjava</span></span>
</code></dt>
<dd>
<section class="desc"><p>Deserialize Java serialized data and re-serialize as JSON.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/java/deserialize.py#L65-L76" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dsjava(Unit):
    &#34;&#34;&#34;
    Deserialize Java serialized data and re-serialize as JSON.
    &#34;&#34;&#34;
    @Unit.Requires(&#39;javaobj-py3&gt;=0.4.0.1&#39;, [&#39;formats&#39;])
    def _javaobj():
        import javaobj.v2
        return javaobj.v2

    def process(self, data):
        with JavaEncoder as encoder:
            return encoder.dumps(self._javaobj.loads(data)).encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.dsphp"><code class="flex name class">
<span>class <span class="ident">dsphp</span></span>
</code></dt>
<dd>
<section class="desc"><p>Deserialize PHP serialized data and re-serialize as JSON.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/deserialize_php.py#L8-L42" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dsphp(Unit):
    &#34;&#34;&#34;
    Deserialize PHP serialized data and re-serialize as JSON.
    &#34;&#34;&#34;
    @Unit.Requires(&#39;phpserialize&#39;, [&#39;formats&#39;])
    def _php():
        import phpserialize
        return phpserialize

    def reverse(self, data):
        return self._php.dumps(json.loads(data))

    def process(self, data):
        phpobject = self._php.phpobject

        class encoder(json.JSONEncoder):
            def default(self, obj):
                try:
                    return super().default(obj)
                except TypeError:
                    pass
                if isinstance(obj, bytes) or isinstance(obj, bytearray):
                    return obj.decode(&#39;utf8&#39;)
                if isinstance(obj, phpobject):
                    return obj._asdict()

        return json.dumps(
            self._php.loads(
                data,
                object_hook=phpobject,
                decode_strings=True
            ),
            indent=4,
            cls=encoder
        ).encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.dump"><code class="flex name class">
<span>class <span class="ident">dump</span></span>
<span>(</span><span>*files, tee=False, stream=False, plain=False, force=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Dump incoming data to files on disk. It is possible to specify filenames with format fields.
Any metadata field on an incoming chunk is available. Additionally, any field that can be
populated by the <code><a title="refinery.cm" href="index.html#refinery.cm">cm</a></code> unit is also available. These include the following:</p>
<pre><code>{ext}    : Automatically guessed file extension
{crc32}  : CRC32 checksum of the data
{index}  : Index of the data in the input stream, starting at 0
{size}   : Size of the data in bytes
{md5}    : MD5 hash of the data
{sha1}   : SHA1 hash of the data
{sha256} : SHA-256 hash of the data
{path}   : Associated path; defaults to {sha256} if none is given.
</code></pre>
<p>When not using formatted file names, the unit ingests as many incoming inputs as filenames were
specified on the command line. Unless connected to a terminal, the remaining inputs will be
forwarded on STDOUT. The <code>-t</code> or <code>--tee</code> switch can be used to forward all inputs, under all
circumstances, regardless of whether or not they have been processed.</p>
<p>If the data cannot be written to the specified path because a file already exists in place of a
directory that would have to be created, the unit renames the directory until dumping is possible.</p>
<p>If no file is specified, all ingested inputs are concatenated and written to the clipboard. This
will only succeed when the data can successfully be encoded.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/sinks/dump.py#L46-L246" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dump(Unit):
    &#34;&#34;&#34;
    Dump incoming data to files on disk. It is possible to specify filenames with format fields.
    Any metadata field on an incoming chunk is available. Additionally, any field that can be
    populated by the `refinery.cm` unit is also available. These include the following:

        {ext}    : Automatically guessed file extension
        {crc32}  : CRC32 checksum of the data
        {index}  : Index of the data in the input stream, starting at 0
        {size}   : Size of the data in bytes
        {md5}    : MD5 hash of the data
        {sha1}   : SHA1 hash of the data
        {sha256} : SHA-256 hash of the data
        {path}   : Associated path; defaults to {sha256} if none is given.

    When not using formatted file names, the unit ingests as many incoming inputs as filenames were
    specified on the command line. Unless connected to a terminal, the remaining inputs will be
    forwarded on STDOUT. The `-t` or `--tee` switch can be used to forward all inputs, under all
    circumstances, regardless of whether or not they have been processed.

    If the data cannot be written to the specified path because a file already exists in place of a
    directory that would have to be created, the unit renames the directory until dumping is possible.

    If no file is specified, all ingested inputs are concatenated and written to the clipboard. This
    will only succeed when the data can successfully be encoded.
    &#34;&#34;&#34;

    def __init__(
        self, *files: Param[str, Arg.String(metavar=&#39;file&#39;, help=&#39;Optionally formatted filename.&#39;)],
        tee: Param[bool, Arg.Switch(&#39;-t&#39;, help=&#39;Forward all inputs to STDOUT.&#39;)] = False,
        stream: Param[bool, Arg.Switch(&#39;-s&#39;, help=&#39;Dump all incoming data to the same file.&#39;)] = False,
        plain: Param[bool, Arg.Switch(&#39;-p&#39;, help=&#39;Never apply any formatting to file names.&#39;)] = False,
        force: Param[bool, Arg.Switch(&#39;-f&#39;, help=&#39;Remove files if necessary to create dump path.&#39;)] = False,
    ):
        if stream and len(files) != 1:
            raise ValueError(&#39;Can only use exactly one file in stream mode.&#39;)
        super().__init__(files=files, tee=tee, stream=stream, force=force)
        self.stream = None
        self._formatted = not plain and any(_has_format(f) for f in files)
        self._reset()

    def _reset(self):
        self.exhausted = False
        self.paths = cycle(self.args.files) if self._formatted else iter(self.args.files)
        self._close()

    @property
    def _clipcopy(self):
        return not self.args.files

    @lru_cache(maxsize=None)
    def _fix_path_part(self, base: Path) -&gt; Path:
        if not _is_path_obstruction(base):
            return base
        if self.args.force:
            try:
                os.unlink(base)
            except Exception:
                raise RefineryCriticalException(F&#39;Unable to remove path obstruction: {base}.&#39;)
            else:
                self.log_info(F&#39;removed path obstruction: {base}&#39;)
                return base
        else:
            stem = base = base.with_suffix(&#39;&#39;)
            counter = 0
            while _is_path_obstruction(base):
                base = stem.with_suffix(F&#39;.{counter}&#39;)
                counter += 1
            return base

    def _fix_path(self, path: Path) -&gt; Path:
        fixed = Path()
        for p in path.parent.parts:
            fixed = self._fix_path_part(fixed / p)
        return fixed / path.name

    def _open(self, path, unc=False):
        if hasattr(path, &#39;close&#39;):
            return path
        path = self._fix_path(Path(path).absolute())
        base = path.parent
        try:
            os.makedirs(base, exist_ok=True)
        except FileNotFoundError:
            if unc or os.name != &#39;nt&#39;:
                raise
            return self._open(F&#39;\\\\?\\{path}&#39;, unc=True)
        except FileExistsError:
            raise RefineryCriticalException(
                F&#39;Unknown error while attempting to create parent directory: {base}&#39;)
        except OSError as e:
            if not self.log_info():
                self.log_warn(&#39;opening:&#39;, path)
            self.log_warn(&#39;errored:&#39;, e.args[1])
            return open(os.devnull, &#39;wb&#39;)
        else:
            info = str(path)
            self.log_info(&#39;opening:&#39;, info[4:] if unc else info)
            mode = &#39;ab&#39; if self.args.stream else &#39;wb&#39;
            return path.open(mode)

    def _close(self, final=False):
        if not self.stream:
            return
        self.stream.flush()
        if self.args.stream and not final:
            return
        if self._clipcopy:
            if os.name == &#39;nt&#39;:
                from refinery.lib.winclip import CF, ClipBoard
                try:
                    img = self._image.open(self.stream)
                    with io.BytesIO() as out:
                        img.save(out, &#39;BMP&#39;)
                except Exception:
                    with ClipBoard(CF.TEXT) as cpb:
                        cpb.copy(self.stream.getvalue())
                else:
                    with ClipBoard(CF.DIB) as cpb:
                        out.seek(14, io.SEEK_SET)
                        cpb.copy(out.read())
            else:
                data = self.stream.getvalue()
                data = data.decode(self.codec, errors=&#39;backslashreplace&#39;)
                self._pyperclip.copy(data)
        self.stream.close()
        self.stream = None

    @Unit.Requires(&#39;pyperclip&#39;)
    def _pyperclip():
        import pyperclip
        return pyperclip

    @Unit.Requires(&#39;Pillow&#39;, [&#39;formats&#39;])
    def _image():
        from PIL import Image
        return Image

    def process(self, data: bytearray):
        forward_input_data = self.args.tee
        if self._clipcopy:
            if stream := self.stream:
                stream.write(data)
        elif not self.exhausted:
            if not self.stream:
                # This should happen only when the unit is called from Python code
                # rather than via the command line.
                try:
                    path = next(self.paths)
                except StopIteration:
                    raise RefineryCriticalException(&#39;the list of filenames was exhausted.&#39;)
                else:
                    with self._open(path) as stream:
                        stream.write(data)
            else:
                self.stream.write(data)
                self.log_debug(F&#39;wrote 0x{len(data):08X} bytes&#39;)
                self._close()
        else:
            forward_input_data = forward_input_data or not self.isatty()
            if not forward_input_data:
                size = metavars(data).size
                self.log_warn(F&#39;discarding unprocessed chunk of size {size!s}.&#39;)
        if forward_input_data:
            yield data

    def filter(self, chunks):
        if self.exhausted:
            self._reset()

        nostream = not self.args.stream
        clipcopy = self._clipcopy

        if clipcopy:
            self.stream = io.BytesIO()

        for index, chunk in enumerate(chunks, 0):
            if not chunk.visible:
                continue
            if not clipcopy and not self.exhausted and (nostream or not self.stream):
                try:
                    path = next(self.paths)
                except StopIteration:
                    self.exhausted = True
                else:
                    if _has_format(path):
                        meta = metavars(chunk)
                        meta.ghost = True
                        meta.index = index
                        new_path = meta.format_str(path, self.codec, [chunk])
                        if new_path != path:
                            path = new_path
                        elif self.leniency &lt; 1:
                            raise ValueError(
                                F&#39;Could not resolve formatting in path &#34;{path}&#34;; &#39;
                                R&#39;increase leniency to ignore this.&#39;)
                    self.stream = self._open(path)
            yield chunk

        self._close(final=True)
        self.exhausted = True</code></pre>
</details>
</dd>
<dt id="refinery.shell.eat"><code class="flex name class">
<span>class <span class="ident">eat</span></span>
<span>(</span><span>name)</span>
</code></dt>
<dd>
<section class="desc"><p>Consume a meta variable and replace the contents of the current chunk with it. If the variable
contains a string, it is encoded with the default codec. If the variable cannot be converted to
a byte string, the data is lost and an empty chunk is returned.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/eat.py#L9-L42" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class eat(Unit):
    &#34;&#34;&#34;
    Consume a meta variable and replace the contents of the current chunk with it. If the variable
    contains a string, it is encoded with the default codec. If the variable cannot be converted to
    a byte string, the data is lost and an empty chunk is returned.
    &#34;&#34;&#34;
    def __init__(
        self,
        name: Param[str, Arg.String(help=&#39;The name of the variable to be used.&#39;)],
    ):
        super().__init__(name=check_variable_name(name))

    def process(self, data: Chunk):
        def invalid_type():
            return F&#39;variable {name} is of type &#34;{type}&#34;, unable to convert to byte string - data is lost&#39;
        name = self.args.name
        meta = metavars(data)
        data = meta.pop(name)
        type = data.__class__.__name__
        if isinstance(data, int):
            self.log_info(F&#39;variable {name} is an integer, converting to string.&#39;)
            data = str(data).encode(self.codec)
        if isinstance(data, str):
            self.log_info(F&#39;variable {name} is a string, encoding as {self.codec}&#39;)
            data = data.encode(self.codec)
        elif not isbuffer(data):
            try:
                wrapped = bytearray(data)
            except Exception:
                self.log_warn(invalid_type())
                data = None
            else:
                data = wrapped
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.ef"><code class="flex name class">
<span>class <span class="ident">ef</span></span>
<span>(</span><span>*filenames, list=False, meta=False, size=None, read=0, wild=False, tame=False, symlinks=False, linewise=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Short for "emit file". The unit reads files from disk and outputs them individually. Has the ability to
read large files in chunks.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/ef.py#L23-L263" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ef(Unit):
    &#34;&#34;&#34;
    Short for &#34;emit file&#34;. The unit reads files from disk and outputs them individually. Has the ability to
    read large files in chunks.
    &#34;&#34;&#34;

    def __init__(self,
        *filenames: Param[str, Arg.String(metavar=&#39;FILEMASK&#39;, nargs=&#39;+&#39;, help=(
            &#39;A list of file masks. Each matching file will be read from disk and &#39;
            &#39;emitted. The file masks can include format string expressions which &#39;
            &#39;will be substituted from the current meta variables. The masks can &#39;
            &#39;use wild-card expressions, but this feature is disabled by default on &#39;
            &#39;Posix platforms, where it has to be enabled explicitly using the -w &#39;
            &#39;switch. On Windows, the feature is enabled by default and can be &#39;
            &#39;disabled using the -t switch.&#39;
        ))],
        list: Param[bool, Arg.Switch(&#39;-l&#39;, help=&#39;Only lists files with metadata.&#39;)] = False,
        meta: Param[bool, Arg.Switch(&#39;-m&#39;, help=(
            &#39;Adds the atime, mtime, ctime, and size metadata variables.&#39;
        ))] = False,
        size: Param[slice, Arg.Bounds(&#39;-s&#39;, help=(
            &#39;If specified, only files are read whose size is in the given range.&#39;))] = None,
        read: Param[int, Arg.Number(&#39;-r&#39;, help=(
            &#39;If specified, files will be read in chunks of size N and each &#39;
            &#39;chunk is emitted as one element in the output list.&#39;
        ))] = 0,
        wild: Param[bool, Arg.Switch(&#39;-w&#39;, group=&#39;W&#39;, help=&#39;Force use of wildcard patterns in file masks.&#39;)] = False,
        tame: Param[bool, Arg.Switch(&#39;-t&#39;, group=&#39;W&#39;, help=&#39;Disable wildcard patterns in file masks.&#39;)] = False,
        symlinks: Param[bool, Arg.Switch(&#39;-y&#39;, help=&#39;Follow symbolic links and junctions, these are ignored by default.&#39;)] = False,
        linewise: Param[bool, Arg.Switch(&#39;-i&#39;, help=(
            &#39;Read the file linewise. By default, one line is read at a time. &#39;
            &#39;In line mode, the --read argument can be used to read the given &#39;
            &#39;number of lines in each chunk.&#39;
        ))] = False
    ):
        if wild and tame:
            raise ValueError(&#39;Cannot be both wild and tame!&#39;)
        super().__init__(
            size=size,
            read=read,
            list=list,
            meta=meta,
            wild=wild,
            tame=tame,
            symlinks=symlinks,
            linewise=linewise,
            filenames=filenames
        )

    def _read_chunks(self, fd):
        while True:
            buffer = fd.read(self.args.read)
            if not buffer:
                break
            yield buffer

    def _read_lines(self, fd):
        count = self.args.read or 1
        if count == 1:
            while True:
                buffer = fd.readline()
                if not buffer:
                    break
                yield buffer
            return
        with MemoryFile() as out:
            while True:
                for _ in range(count):
                    buffer = fd.readline()
                    if not buffer:
                        break
                    out.write(buffer)
                if not out.tell():
                    break
                yield out.getvalue()
                out.seek(0)
                out.truncate()

    def _absolute_path(self, path_string: str):
        path = Path(path_string).resolve().absolute()
        if os.name == &#39;nt&#39; and not path.parts[0].startswith(&#39;\\\\?\\&#39;):
            # The pathlib glob method will simply fail mid-traversal if it attempts to descend into
            # a folder or to a file whose path exceeds MAX_PATH on Windows. As a workaround, we use
            # UNC paths throughout and truncate to relative paths after enumeration.
            path = Path(F&#39;\\\\?\\{path!s}&#39;)
        return path

    def _glob(self, pattern: str) -&gt; Iterable[Path]:
        if pattern.endswith(&#39;**&#39;):
            pattern += &#39;/*&#39;
        wildcard = re.search(R&#39;[\[\?\*]&#39;, pattern)
        if wildcard is None:
            yield self._absolute_path(pattern)
            return
        k = wildcard.start()
        base, pattern = pattern[:k], pattern[k:]
        path = self._absolute_path(base or &#39;.&#39;)
        last = path.parts[-1]
        if base.endswith(last):
            # /base/something.*
            pattern = F&#39;{last}{pattern}&#39;
            path = path.parent

        scandir = os.scandir

        class EmptyIterator:
            def __enter__(self): return self
            def __exit__(self, *_, **__): pass
            def __next__(self): raise StopIteration
            def __iter__(self): return self

        if sys.version_info &gt;= (3, 12):
            def islink(path):
                return os.path.islink(path) or os.path.isjunction(path)
        else:
            def islink(path):
                try:
                    return bool(os.readlink(path))
                except OSError:
                    return False

        paths_scanned = set()

        def _patched_scandir(path):
            if islink(path):
                if not self.args.symlinks:
                    return EmptyIterator()
                try:
                    rp = os.path.realpath(path, strict=True)
                except OSError:
                    return EmptyIterator()
                if rp in paths_scanned:
                    self.log_warn(F&#39;file system loop at: {path!s}&#39;)
                    return EmptyIterator()
                paths_scanned.add(rp)
                path = rp
            try:
                return scandir(path)
            except Exception as e:
                ignore = _ERROR_IGNORES.get(os.name, set())
                if not any(p.lower() in ignore for p in Path(path).parts):
                    self.log_warn(F&#39;error calling scandir, {exception_to_string(e)}: {path}&#39;)
                return EmptyIterator()

        try:
            os.scandir = _patched_scandir
            yield from path.glob(pattern)
        finally:
            os.scandir = scandir

    def process(self, data):
        meta = metavars(data)
        size = self.args.size
        size = size and bounds[size]
        meta.ghost = True
        wild = (os.name == &#39;nt&#39; or self.args.wild) and not self.args.tame
        root = self._absolute_path(&#39;.&#39;)
        paths = self._glob if wild else lambda mask: [self._absolute_path(mask)]
        do_meta = self.args.meta
        do_stat = size or do_meta

        class SkipErrors:
            unit = self

            def __init__(self):
                self._history: set[type] = set()
                self._message: dict[type, str | None] = {
                    ValueError: (
                        None
                    ), PermissionError: (
                        &#39;access error while scanning: {}&#39;
                    ), OSError: (
                        &#39;system error while scanning: {}&#39;
                    ), FileNotFoundError: (
                        &#39;file unexpectedly not found: {}&#39;
                    ), Exception: (
                        &#39;unknown error while reading: {}&#39;
                    ),
                }
                self.path = None

            def reset(self, path):
                self._history.clear()
                self.path = path
                return self

            def __enter__(self):
                return self

            def __exit__(self, et, ev, trace):
                if et is None:
                    return False
                for t, msg in self._message.items():
                    if issubclass(et, t):
                        if t not in self._history:
                            self._history.add(t)
                            if msg is not None:
                                self.unit.log_info(msg.format(self.path))
                        return True
                else:
                    return False

        for mask in self.args.filenames:
            mask = meta.format_str(mask, self.codec, [data])
            self.log_debug(&#39;scanning for mask:&#39;, mask)
            kwargs = dict()
            skip_errors = SkipErrors()
            for path in paths(mask):
                skip_errors.reset(path)
                filesize = None
                with skip_errors:
                    path = path.relative_to(root)
                with skip_errors:
                    if wild and not path.is_file():
                        continue
                with skip_errors:
                    if do_stat:
                        stat = path.stat()
                        filesize = stat.st_size
                    if do_meta:
                        kwargs.update(
                            fsize=filesize,
                            atime=datetime.fromtimestamp(stat.st_atime).isoformat(&#39; &#39;, &#39;seconds&#39;),
                            ctime=datetime.fromtimestamp(stat.st_ctime).isoformat(&#39; &#39;, &#39;seconds&#39;),
                            mtime=datetime.fromtimestamp(stat.st_mtime).isoformat(&#39; &#39;, &#39;seconds&#39;)
                        )
                if size is not None and filesize not in size:
                    continue
                with skip_errors:
                    if self.args.list:
                        yield self.labelled(str(path).encode(self.codec), **kwargs)
                        continue
                    with path.open(&#39;rb&#39;) as stream:
                        if self.args.linewise:
                            yield from self._read_lines(stream)
                        elif self.args.read:
                            yield from self._read_chunks(stream)
                        else:
                            contents = stream.read()
                            self.log_info(lambda: F&#39;reading: {path!s} ({len(contents)} bytes)&#39;)
                            yield self.labelled(contents, path=path.as_posix(), **kwargs)</code></pre>
</details>
</dd>
<dt id="refinery.shell.emit"><code class="flex name class">
<span>class <span class="ident">emit</span></span>
<span>(</span><span>*data)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/emit.py#L13-L40" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class emit(Unit):

    def __init__(self, *data: Param[buf, Arg(help=(
        &#39;Data to be emitted. If no argument is specified, data is retrieved from &#39;
        &#39;the clipboard. Multiple arguments are output in framed format.&#39;
    ))]):
        super().__init__(data=data)

    @Unit.Requires(&#39;pyperclip&#39;)
    def _pyperclip():
        import pyperclip
        return pyperclip

    def process(self, data):
        if self.args.data:
            yield from self.args.data
            return
        if os.name == &#39;nt&#39;:
            from refinery.lib.winclip import get_any_data
            mode, data = get_any_data()
            if mode is not None:
                self.log_info(F&#39;retrieved clipboard data in {mode.name} format&#39;)
            yield data
        else:
            data = self._pyperclip.paste()
            if not data:
                return
            yield data.encode(self.codec, &#39;replace&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.esc"><code class="flex name class">
<span>class <span class="ident">esc</span></span>
<span>(</span><span>hex=False, unicode=False, greedy=False, unquoted=False, quoted=False, bare=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Encodes and decodes common ASCII escape sequences.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/esc.py#L9-L114" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class esc(Unit):
    &#34;&#34;&#34;
    Encodes and decodes common ASCII escape sequences.
    &#34;&#34;&#34;
    _ESCAPE = {
        0x00: BR&#39;\0&#39;,
        0x07: BR&#39;\a&#39;,
        0x08: BR&#39;\b&#39;,
        0x0C: BR&#39;\f&#39;,
        0x0A: BR&#39;\n&#39;,
        0x0D: BR&#39;\r&#39;,
        0x09: BR&#39;\t&#39;,
        0x0B: BR&#39;\v&#39;,
        0x5C: BR&#39;\\&#39;,
        0x27: BR&#39;\&#39;&#39;,
        0x22: BR&#39;\&#34;&#39;
    }
    _UNESCAPE = {
        BR&#39;0&#39;: B&#39;\x00&#39;,
        BR&#39;a&#39;: B&#39;\x07&#39;,
        BR&#39;b&#39;: B&#39;\x08&#39;,
        BR&#39;f&#39;: B&#39;\x0C&#39;,
        BR&#39;n&#39;: B&#39;\x0A&#39;,
        BR&#39;r&#39;: B&#39;\x0D&#39;,
        BR&#39;t&#39;: B&#39;\x09&#39;,
        BR&#39;v&#39;: B&#39;\x0B&#39;,
        B&#39;\\&#39;: B&#39;\x5C&#39;,
        BR&#34;&#39;&#34;: B&#39;\x27&#39;,
        BR&#39;&#34;&#39;: B&#39;\x22&#39;
    }

    def __init__(self,
        hex: Param[bool, Arg.Switch(&#39;-x&#39;,
            help=&#39;Hex encode everything, do not use C escape sequences.&#39;)] = False,
        unicode: Param[bool, Arg.Switch(&#39;-u&#39;,
            help=&#39;Use unicode escape sequences and UTF-8 encoding.&#39;)] = False,
        greedy: Param[bool, Arg.Switch(&#39;-g&#39;,
            help=&#39;Replace \\x by x and \\u by u when not followed by two or four hex digits, respectively.&#39;)] = False,
        unquoted: Param[bool, Arg.Switch(&#39;-p&#39;, group=&#39;Q&#39;,
            help=&#39;Never remove enclosing quotes.&#39;)] = False,
        quoted: Param[bool, Arg.Switch(&#39;-q&#39;, group=&#39;Q&#39;,
            help=&#39;Remove enclosing quotes while decoding and add them for encoding.&#39;)] = False,
        bare: Param[bool, Arg.Switch(&#39;-b&#39;,
            help=&#39;Do not escape quote characters.&#39;)] = False,
    ): pass # noqa

    def process(self, data):
        data = memoryview(data)

        if self.args.quoted:
            quote = data[0]
            if data[-1] != quote:
                self.log_info(&#39;string is not correctly quoted&#39;)
            else:
                data = data[1:-1]
        elif not self.args.unquoted:
            quote = data[:1]
            strip = data[1:-1]
            if data[-1:] == quote and not re.search(br&#39;(?&lt;!\\)&#39; + re.escape(quote), strip):
                self.log_info(&#39;removing automatically detected quotes&#39;)
                data = strip

        def unescape(match: re.Match[bytes]):
            c = match[1]
            if len(c) &gt; 1:
                if c[0] == 0x75:
                    # unicode
                    upper = int(c[1:3], 16)
                    lower = int(c[3:5], 16)
                    if self.args.unicode:
                        return bytes((lower, upper)).decode(&#39;utf-16le&#39;).encode(self.codec)
                    return bytes((lower,))
                elif c[0] == 0x78:
                    # hexadecimal
                    return bytes((int(c[1:3], 16),))
                else:
                    # octal escape sequence
                    return bytes((int(c, 8) &amp; 0xFF,))
            elif c in B&#39;ux&#39;:
                return c if self.args.greedy else match[0]
            return self._UNESCAPE.get(c, c)
        data = re.sub(
            RB&#39;\\(u[a-fA-F0-9]{4}|x[a-fA-F0-9]{1,2}|[0-7]{3}|.)&#39;, unescape, data)
        return data

    def reverse(self, data):
        if self.args.unicode:
            string = data.decode(self.codec).encode(&#39;UNICODE_ESCAPE&#39;)
        else:
            if not self.args.hex:
                def escape(match: re.Match[bytes]):
                    c = match[0][0]
                    return self._ESCAPE.get(c, RB&#39;\x%02x&#39; % c)
                pattern = RB&#39;[\x00-\x1F\x22\x27\x5C\x7F-\xFF]&#39;
                if self.args.bare:
                    pattern = RB&#39;[\x00-\x1F\x5C\x7F-\xFF]&#39;
                string = re.sub(pattern, escape, data)
            else:
                string = bytearray(4 * len(data))
                for k in range(len(data)):
                    a = k * 4
                    b = k * 4 + 4
                    string[a:b] = RB&#39;\x%02x&#39; % data[k]
        if self.args.quoted:
            string = B&#39;&#34;%s&#34;&#39; % string
        return string</code></pre>
</details>
</dd>
<dt id="refinery.shell.escps"><code class="flex name class">
<span>class <span class="ident">escps</span></span>
</code></dt>
<dd>
<section class="desc"><p>Escapes and unescapes PowerShell strings.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/escps.py#L9-L72" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class escps(Unit):
    &#34;&#34;&#34;
    Escapes and unescapes PowerShell strings.
    &#34;&#34;&#34;
    UNESCAPE = {
        &#39;`0&#39;: &#39;\0&#39;,
        &#39;`a&#39;: &#39;\a&#39;,
        &#39;`b&#39;: &#39;\b&#39;,
        &#39;`f&#39;: &#39;\f&#39;,
        &#39;`n&#39;: &#39;\n&#39;,
        &#39;`r&#39;: &#39;\r&#39;,
        &#39;`t&#39;: &#39;\t&#39;,
        &#39;`v&#39;: &#39;\v&#39;,
        &#39;``&#39;: &#39;`&#39;,
        &#34;`&#39;&#34;: &#39;\&#39;&#39;,
        &#39;`&#34;&#39;: &#39;\&#34;&#39;,
    }
    ESCAPE = {
        &#39;`&#39; : &#39;``&#39;,
        &#39;$&#39; : &#39;`$&#39;,
        &#39;\0&#39;: &#39;`0&#39;,
        &#39;\a&#39;: &#39;`a&#39;,
        &#39;\b&#39;: &#39;`b&#39;,
        &#39;\f&#39;: &#39;`f&#39;,
        &#39;\n&#39;: &#39;`n&#39;,
        &#39;\r&#39;: &#39;`r&#39;,
        &#39;\t&#39;: &#39;`t&#39;,
        &#39;\v&#39;: &#39;`v&#39;,
        &#39;\&#39;&#39;: &#34;`&#39;&#34;,
        &#39;\&#34;&#39;: &#39;&#34;&#34;&#39;,
    }

    def __init__(self): pass

    @unicoded
    def process(self, data):
        match = re.fullmatch(R&#39;&#39;&#39;@([&#39;&#34;])\s*?[\r\n](.*?)[\r\n]\1@&#39;&#39;&#39;, data, flags=re.DOTALL)
        if match:
            return match.group(2)
        if data[0] not in &#39;\&#39;\&#34;&#39; or data[-1] != data[0]:
            raise ValueError(
                &#39;No quotes found at beginning of input. To escape a PowerShell string, the &#39;
                &#39;quotes must be included because quote escaping depends on whether a single &#39;
                &#39;or double quote was used.&#39;)

        quote, data = data[0], data[1:-1]

        def unescape(match):
            string = match[0]
            return self.UNESCAPE.get(string, string[1:])

        if quote == &#39;&#34;&#39;:
            if re.search(R&#39;(?&lt;!`)\$(?=[\w\(\{\$\?\^:])&#39;, data):
                self.log_warn(&#39;Loss of information: double quoted string contains variable substitutions.&#39;)
            data = re.sub(&#39;`.&#39;, unescape, data)

        return data.replace(quote + quote, quote)

    @unicoded
    def reverse(self, data):
        def escaper(match):
            char = match[0]
            return escps.ESCAPE.get(char, char)
        return &#39;&#34;{}&#34;&#39;.format(re.sub(R&#39;&#39;&#39;[\x00\x07-\x0D`$&#39;&#34;]&#39;&#39;&#39;, escaper, data))</code></pre>
</details>
</dd>
<dt id="refinery.shell.escvb"><code class="flex name class">
<span>class <span class="ident">escvb</span></span>
</code></dt>
<dd>
<section class="desc"><p>Escapes and unescapes Visual Basic strings.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/escvb.py#L6-L16" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class escvb(Unit):
    &#34;&#34;&#34;
    Escapes and unescapes Visual Basic strings.
    &#34;&#34;&#34;
    def process(self, data):
        if data[:1] == B&#39;&#34;&#39; and data[-1:] == B&#39;&#34;&#39;:
            data = data[1:-1]
        return data.replace(B&#39;&#34;&#34;&#39;, B&#39;&#34;&#39;)

    def reverse(self, data):
        return B&#39;&#34;%s&#34;&#39; % data.replace(B&#39;&#34;&#39;, B&#39;&#34;&#34;&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.evtx"><code class="flex name class">
<span>class <span class="ident">evtx</span></span>
<span>(</span><span>raw=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts data from Windows Event Log files (EVTX). Each extracted log entry is returned as a single
output chunk in XML format.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/evtx.py#L8-L27" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class evtx(Unit):
    &#34;&#34;&#34;
    Extracts data from Windows Event Log files (EVTX). Each extracted log entry is returned as a single
    output chunk in XML format.
    &#34;&#34;&#34;

    def __init__(self, raw: Param[bool, Arg.Switch(&#39;-r&#39;, help=&#39;Extract raw event data rather than XML.&#39;)] = False):
        super().__init__(raw=raw)

    @Unit.Requires(&#39;python-evtx&#39;, [&#39;formats&#39;])
    def _evtx():
        from Evtx.Evtx import Evtx
        return Evtx

    def process(self, data):
        with VirtualFileSystem() as vfs:
            raw = self.args.raw
            with self._evtx(vfs.new(data)) as log:
                for record in log.records():
                    yield record.data() if raw else record.xml().encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.fernet"><code class="flex name class">
<span>class <span class="ident">fernet</span></span>
<span>(</span><span>key)</span>
</code></dt>
<dd>
<section class="desc"><p>Decrypt Fernet messages.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/fernet.py#L14-L52" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class fernet(Unit):
    &#34;&#34;&#34;
    Decrypt Fernet messages.
    &#34;&#34;&#34;
    def __init__(self, key: Param[buf, Arg(help=&#39;A fernet key, either in base64 or raw binary.&#39;)]):
        super().__init__(key=key)

    def _b64(self, data):
        try:
            return data | b64(urlsafe=True) | bytearray
        except Exception:
            return data

    def process(self, data):
        fk = self._b64(self.args.key)
        if len(fk) != 32:
            raise ValueError(F&#39;The given Fernet key has length {len(fk)}, expected 32 bytes.&#39;)
        signing_key = fk[:16]
        encryption_key = fk[16:]
        decoded = self._b64(data)
        reader = StructReader(memoryview(decoded), bigendian=True)
        signed_data = reader.peek(reader.remaining_bytes - 32)
        version = reader.u8()
        timestamp = datetime.fromtimestamp(reader.u64())
        iv = reader.read(16)
        if version != 0x80:
            self.log_warn(F&#39;The Fernet version is 0x{version:02X}, the only documented one is 0x80.&#39;)
        ciphertext = reader.read(reader.remaining_bytes - 32)
        if len(ciphertext) % 16 != 0:
            raise ValueError(&#39;The encoded ciphertext is not 16-byte block aligned.&#39;)
        signature = reader.read(32)
        hmac = HMAC.new(signing_key, digestmod=SHA256)
        hmac.update(signed_data)
        if hmac.digest() != signature:
            self.log_warn(&#39;HMAC verification failed; the message has been tampered with.&#39;)
            self.log_info(F&#39;computed signature: {hmac.hexdigest().upper()}&#39;)
            self.log_info(F&#39;provided signature: {signature.hex().upper()}&#39;)
        plaintext = ciphertext | aes(mode=&#39;cbc&#39;, iv=iv, key=encryption_key) | bytearray
        return self.labelled(plaintext, timestamp=timestamp.isoformat(&#39; &#39;, &#39;seconds&#39;))</code></pre>
</details>
</dd>
<dt id="refinery.shell.flz"><code class="flex name class">
<span>class <span class="ident">flz</span></span>
<span>(</span><span>level=0)</span>
</code></dt>
<dd>
<section class="desc"><p>FastLZ (or FLZ for short) compression and decompression. This implementation was ported to
pure Python from the C reference and is therefore much slower.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/flz.py#L210-L247" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class flz(Unit):
    &#34;&#34;&#34;
    FastLZ (or FLZ for short) compression and decompression. This implementation was ported to
    pure Python from the C reference and is therefore much slower.
    &#34;&#34;&#34;
    def __init__(
        self,
        level: Param[int, Arg.Number(&#39;-l&#39;, bound=(1, 2), help=(
            &#39;Specify a FastLZ level (either 0 or 1). By default, compression will select a level &#39;
            &#39;based on buffer length like the reference implementation. Decompression reads level &#39;
            &#39;information from the header by default.&#39;))] = 0
    ):
        super().__init__(level=level)

    def reverse(self, data):
        if not data:
            return data
        if (level := self.args.level) &lt;= 0:
            level = 1 + int(len(data) &gt;= 0x10000)
        output = bytearray()
        _flz_compress(memoryview(data), output, level - 1)
        return output

    def process(self, data):
        try:
            hl = 1 + (data[0] &gt;&gt; 5)
        except IndexError:
            return None
        if (level := self.args.level) == 0:
            level = hl
        if level != hl:
            self.log_info(F&#39;Using level {level} despite header-defined level {hl}.&#39;)
        return _flz_decompress(memoryview(data), level - 1)

    @classmethod
    def handles(cls, data):
        if data and (data[0] &gt;&gt; 5) &gt; 1:
            return False</code></pre>
</details>
</dd>
<dt id="refinery.shell.gost"><code class="flex name class">
<span>class <span class="ident">gost</span></span>
<span>(</span><span>key, iv=b'', padding=None, mode=None, raw=False, swap=False, sbox=SBOX.R34, *, aad=b'', tag=(), segment_size=0, little_endian=False)</span>
</code></dt>
<dd>
<section class="desc"><p>GOST encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/gost.py#L106-L126" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class gost(StandardBlockCipherUnit, cipher=BlockCipherFactory(GOST)):
    &#34;&#34;&#34;
    GOST encryption and decryption.
    &#34;&#34;&#34;
    def __init__(
        self, key, iv=B&#39;&#39;, padding=None, mode=None, raw=False,
        swap: Param[bool, Arg.Switch(&#39;-s&#39;, help=&#39;Decode blocks as big endian rather than little endian.&#39;)] = False,
        sbox: Param[str, Arg.Option(&#39;-x&#39;, choices=SBOX, help=(
            &#39;Choose an SBOX. The default is {default}, which corresponds to the R-34.12.2015 standard. &#39;
            &#39;The other option is CBR, which is the SBOX used by the Central Bank of Russia.&#39;
        ))] = SBOX.R34, **more
    ):
        sbox = Arg.AsOption(sbox, SBOX)
        super().__init__(key, iv=iv, padding=padding, mode=mode, raw=raw, swap=swap, sbox=sbox, **more)

    def _new_cipher(self, **optionals) -&gt; CipherInterface:
        return super()._new_cipher(
            swap=self.args.swap,
            sbox=self.args.sbox,
            **optionals
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.group"><code class="flex name class">
<span>class <span class="ident">group</span></span>
<span>(</span><span>size)</span>
</code></dt>
<dd>
<section class="desc"><p>Group incoming chunks into frames of the given size.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/group.py#L9-L30" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class group(Unit):
    &#34;&#34;&#34;
    Group incoming chunks into frames of the given size.
    &#34;&#34;&#34;
    def __init__(self, size: Param[int, Arg.Number(help=&#39;Size of each group; must be at least 2.&#39;, bound=(2, None))]):
        super().__init__(size=size)

    def process(self, data: Chunk):
        if not data.temp:
            return
        yield data
        yield from islice(data.temp, 0, self.args.size - 1)

    def filter(self, chunks):
        it = iter(chunks)
        while True:
            try:
                head: Chunk = next(it)
            except StopIteration:
                return
            head.temp = it
            yield head</code></pre>
</details>
</dd>
<dt id="refinery.shell.groupby"><code class="flex name class">
<span>class <span class="ident">groupby</span></span>
<span>(</span><span>name)</span>
</code></dt>
<dd>
<section class="desc"><p>Group incoming chunks by the contents of a meta variable. Note that the unit
blocks and cannot stream any output until the input frame is consumed: It has
to read every input chunk to make sure that all groupings are complete.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/groupby.py#L11-L35" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class groupby(Unit):
    &#34;&#34;&#34;
    Group incoming chunks by the contents of a meta variable. Note that the unit
    blocks and cannot stream any output until the input frame is consumed: It has
    to read every input chunk to make sure that all groupings are complete.
    &#34;&#34;&#34;
    def __init__(self, name: Param[str, Arg.String(help=&#39;name of the meta variable&#39;)]):
        super().__init__(name=check_variable_name(name))

    def process(self, data):
        yield from data.temp

    def filter(self, chunks: Iterable[Chunk]) -&gt; Generator[Chunk]:
        name = self.args.name
        members = defaultdict(list)
        for chunk in chunks:
            try:
                value = chunk.meta[name]
            except KeyError:
                value = None
            members[value].append(chunk)
        for chunklist in members.values():
            dummy = chunklist[0]
            dummy.temp = chunklist
            yield dummy</code></pre>
</details>
</dd>
<dt id="refinery.shell.hc128"><code class="flex name class">
<span>class <span class="ident">hc128</span></span>
<span>(</span><span>key, discard=0, stateful=False)</span>
</code></dt>
<dd>
<section class="desc"><p>HC-128 encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/hc128.py#L75-L82" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class hc128(StreamCipherUnit):
    &#34;&#34;&#34;
    HC-128 encryption and decryption.
    &#34;&#34;&#34;
    key_size = {32}

    def keystream(self) -&gt; Iterable[int]:
        return HC128(self.args.key)</code></pre>
</details>
</dd>
<dt id="refinery.shell.hc256"><code class="flex name class">
<span>class <span class="ident">hc256</span></span>
<span>(</span><span>key, iv=b'\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00', discard=0, stateful=False)</span>
</code></dt>
<dd>
<section class="desc"><p>HC-256 encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/hc256.py#L75-L91" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class hc256(StreamCipherUnit):
    &#34;&#34;&#34;
    HC-256 encryption and decryption.
    &#34;&#34;&#34;
    key_size = {32}

    def __init__(
        self, key,
        iv: Param[buf, Arg(help=&#39;An initialization vector; the default is a sequence of 32 zero bytes.&#39;)] = bytes(32),
        discard=0, stateful=False,
    ):
        super().__init__(key=key, iv=iv, stateful=stateful, discard=discard)
        self._keystream = None

    def keystream(self) -&gt; Iterable[int]:
        for num in HC256(self.args.key, self.args.iv):
            yield from num.to_bytes(4, &#39;little&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.hex"><code class="flex name class">
<span>class <span class="ident">hex</span></span>
</code></dt>
<dd>
<section class="desc"><p>Hex-decodes and encodes binary data. Non-hex characters are removed from
the input. For decoding, any odd trailing hex digits are stripped as two
hex digits are required to represent a byte.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/hex.py#L6-L29" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class hex(Unit):
    &#34;&#34;&#34;
    Hex-decodes and encodes binary data. Non-hex characters are removed from
    the input. For decoding, any odd trailing hex digits are stripped as two
    hex digits are required to represent a byte.
    &#34;&#34;&#34;

    def reverse(self, data):
        import base64
        return base64.b16encode(data)

    def process(self, data):
        import base64
        import re
        data = re.sub(B&#39;[^A-Fa-f0-9]+&#39;, B&#39;&#39;, data)
        if len(data) % 2:
            data = data[:-1]
        return base64.b16decode(data, casefold=True)

    @classmethod
    def handles(cls, data):
        from refinery.lib.patterns import formats
        if formats.b16s.value.bin.fullmatch(data):
            return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.hexload"><code class="flex name class">
<span>class <span class="ident">hexload</span></span>
<span>(</span><span>blocks=1, dense=False, expand=False, narrow=False, width=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Convert hex dumps back to the original data and vice versa. All options of this unit apply
to its reverse operation where binary data is converted to a readable hexdump format.
The default mode of the unit expects the input data to contain a readable hexdump and
converts it back to binary.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/hexload.py#L25-L137" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class hexload(HexViewer):
    &#34;&#34;&#34;
    Convert hex dumps back to the original data and vice versa. All options of this unit apply
    to its reverse operation where binary data is converted to a readable hexdump format.
    The default mode of the unit expects the input data to contain a readable hexdump and
    converts it back to binary.
    &#34;&#34;&#34;
    @regex
    class _ENCODED_BYTES:
        R&#34;&#34;&#34;
        (?ix)(?:^|(?&lt;=\s))                      # encoded byte patches must be prefixed by white space
        (?:
            (?:                                 # separated chunks of hex data
                [a-f0-9]{2}                     # hexadecimal chunk; single byte (two hexadecimal letters)
                \s{1,2}                         # encoded byte followed by whitespace
                (?:                             # at least one more encoded byte
                    [a-f0-9]{2}                 # followed by more encoded bytes
                    (?:\s{1,2}[a-f0-9]{2})*     # unless it was just a single byte
                )?
            )
            | (?:[a-f0-9]{4}\s{1,2}             # 2-byte chunks
              (?:[a-f0-9]{4}
              (?:\s{1,2}[a-f0-9]{4})*)?)
            | (?:[a-f0-9]{8}\s{1,2}             # 4-byte chunks
              (?:[a-f0-9]{8}
              (?:\s{1,2}[a-f0-9]{8})*)?)
            | (?:(?:[a-f0-9]{2})+)              # continuous line of hexadecimal characters
        )(?=\s|$)                               # terminated by a whitespace or line end
        &#34;&#34;&#34;

    def __init__(self, blocks=1, dense=False, expand=False, narrow=False, width=0):
        super().__init__(blocks=blocks, dense=dense, expand=expand, narrow=narrow, width=width)
        self._hexline_pattern = re.compile(F&#39;{make_hexline_pattern(1)}(?:[\r\n]|$)&#39;, flags=re.MULTILINE)

    def process(self, data: bytearray):
        if not (lines := [
            line for line in data.decode(self.codec).splitlines(keepends=False)
            if line.strip()
        ]):
            return None

        result = bytearray()
        encoded_byte_matches: list[dict[int, int]] = []

        for check in lines:
            matches: dict[int, int] = {}
            encoded_byte_matches.append(matches)
            for match in self._ENCODED_BYTES.finditer(check):
                a, b = match.span()
                matches[a] = b - a

        it = iter(encoded_byte_matches)
        offsets = set(next(it).keys())
        for matches in it:
            offsets.intersection_update(matches.keys())
        if not offsets:
            raise ValueError(&#39;unable to determine the position of the hex bytes in this dump&#39;)
        lengths: dict[int, list[int]] = {offset: [] for offset in offsets}
        del offsets
        for matches in encoded_byte_matches:
            for offset in lengths:
                lengths[offset].append(matches[offset])
        for offset in lengths:
            lengths[offset].sort()
        midpoint = len(encoded_byte_matches) // 2
        offset, length = max(((offset, lengths[offset][midpoint]) for offset in lengths),
            key=operator.itemgetter(1))
        end = offset + length
        del lengths

        line_checks: list[HexLineCheck] = []

        for k, check in enumerate(lines, 1):
            encoded = check[offset:end]
            onlyhex = re.search(r&#39;^[\sA-Fa-f0-9]+&#39;, encoded)
            if not onlyhex:
                self.log_warn(F&#39;ignoring line without hexadecimal data: {check}&#39;)
                continue
            if onlyhex.group(0) != encoded:
                if k != len(lines):
                    self.log_warn(F&#39;ignoring line with mismatching hex data length: {check}&#39;)
                    continue
                encoded = onlyhex.group(0)
            self.log_debug(F&#39;decoding: {encoded.strip()}&#39;)
            decoded = bytes.fromhex(encoded)
            result.extend(decoded)
            matched = True
            if preview := check[end:]:
                pattern = re.compile(
                    &#39;.&#39;.join(re.escape(x.decode(&#39;ascii&#39;)) for x in re.split(b&#39;[^!-~]&#39;, decoded)))
                matched = pattern.search(preview) is not None
            line_checks.append(HexLineCheck(len(decoded), len(preview), matched))

        decoded_sizes: set[int] = set()
        for last, hl in lookahead(line_checks):
            if not last:
                decoded_sizes.add(hl.decoded_length)
                if len(decoded_sizes) &gt; 1:
                    raise RefineryPartialResult(&#39;inconsistent text preview sizes&#39;, result)

        for k, check in enumerate(line_checks, 1):
            if check.preview_length and not check.matched_binary:
                self.log_warn(F&#39;preview mismatch in line {k}: {lines[k - 1]}&#39;, result)

        if result:
            yield result

    def reverse(self, data):
        metrics = self._get_metrics(len(data))
        if not self.args.width:
            metrics.fit_to_width(allow_increase=True)
        for line in self.hexdump(data, metrics):
            yield line.encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.hkdf"><code class="flex name class">
<span>class <span class="ident">hkdf</span></span>
<span>(</span><span>size, salt, hash='SHA512')</span>
</code></dt>
<dd>
<section class="desc"><p>HKDF Key derivation</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/keyderive/hkdf.py#L6-L14" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class hkdf(KeyDerivation):
    &#34;&#34;&#34;HKDF Key derivation&#34;&#34;&#34;

    def __init__(self, size, salt, hash=&#39;SHA512&#39;):
        super().__init__(size=size, salt=salt, hash=hash)

    def process(self, data):
        from Cryptodome.Protocol.KDF import HKDF
        return HKDF(data, self.args.size, self.args.salt, self.hash)</code></pre>
</details>
</dd>
<dt id="refinery.shell.hmac"><code class="flex name class">
<span>class <span class="ident">hmac</span></span>
<span>(</span><span>salt, hash='SHA1', size=None)</span>
</code></dt>
<dd>
<section class="desc"><p>HMAC based key derivation</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/keyderive/hmac.py#L6-L16" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class hmac(KeyDerivation):
    &#34;&#34;&#34;
    HMAC based key derivation
    &#34;&#34;&#34;

    def __init__(self, salt, hash=&#39;SHA1&#39;, size=None):
        super().__init__(salt=salt, size=size, hash=hash)

    def process(self, data):
        from Cryptodome.Hash import HMAC
        return HMAC.new(data, self.args.salt, digestmod=self.hash).digest()</code></pre>
</details>
</dd>
<dt id="refinery.shell.htmlesc"><code class="flex name class">
<span>class <span class="ident">htmlesc</span></span>
</code></dt>
<dd>
<section class="desc"><p>Encodes and decodes HTML entities.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/htmlesc.py#L9-L20" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class htmlesc(Unit):
    &#34;&#34;&#34;
    Encodes and decodes HTML entities.
    &#34;&#34;&#34;

    @unicoded
    def process(self, data: str) -&gt; str:
        return html_entities.unescape(data)

    @unicoded
    def reverse(self, data: str) -&gt; str:
        return html_entities.escape(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.httprequest"><code class="flex name class">
<span>class <span class="ident">httprequest</span></span>
</code></dt>
<dd>
<section class="desc"><p>Parses HTTP request data, as you would obtain from a packet dump. The unit extracts
POST data in any format; each uploaded file is emitted as a separate chunk.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/httprequest.py#L47-L104" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class httprequest(Unit):
    &#34;&#34;&#34;
    Parses HTTP request data, as you would obtain from a packet dump. The unit extracts
    POST data in any format; each uploaded file is emitted as a separate chunk.
    &#34;&#34;&#34;
    def process(self, data: Chunk):
        def header(line: bytes):
            name, colon, data = line.decode(&#39;utf8&#39;).partition(&#39;:&#39;)
            if colon:
                yield (name.strip().lower(), data.strip())

        head, _, body = data.partition(b&#39;\r\n\r\n&#39;)
        request, *headers = head.splitlines(False)
        headers = dict(t for line in headers for t in header(line))
        method, path, _, *rest = request.split()

        mode = _Fmt.RawBody

        if rest:
            self.log_warn(&#39;unexpected rest data while parsing HTTP request:&#39;, rest)

        if method == b&#39;GET&#39; and not body:
            mode = _Fmt.UrlEncode
            body = path.partition(B&#39;?&#39;)[1]
        if method == b&#39;POST&#39; and (ct := headers.get(&#39;content-type&#39;, None)):
            ct, _ = _parse_header(ct)
            try:
                mode = _Fmt(ct)
            except ValueError:
                mode = _Fmt.RawBody

        def chunks(upload: dict[bytes, list[bytes]]):
            for key, values in upload.items():
                for value in values:
                    yield self.labelled(value, name=key.decode(&#39;utf8&#39;))

        if mode is _Fmt.RawBody:
            yield body
            return
        if mode is _Fmt.Multipart:
            _, _, message_data = data.partition(b&#39;\n&#39;)
            msg = BytesParser().parsebytes(message_data)
            for part in msg.walk():
                payloads = part.get_payload(decode=True)
                if not isinstance(payloads, list):
                    payloads = [payloads]
                for payload in payloads:
                    if buffer := asbuffer(payload):
                        if name := part.get_filename():
                            buffer = self.labelled(buffer, name=name)
                        yield buffer

        if mode is _Fmt.UrlEncode:
            yield from chunks(parse_qs(body, keep_blank_values=True))

    @classmethod
    def handles(cls, data) -&gt; bool | None:
        return data[:5] == B&#39;POST &#39; or data[:4] == B&#39;GET &#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.httpresponse"><code class="flex name class">
<span>class <span class="ident">httpresponse</span></span>
</code></dt>
<dd>
<section class="desc"><p>Parses HTTP response text, as you would obtain from a packet dump. This can be
useful if chunked or compressed transfer encoding was used.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/httpresponse.py#L18-L44" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class httpresponse(Unit):
    &#34;&#34;&#34;
    Parses HTTP response text, as you would obtain from a packet dump. This can be
    useful if chunked or compressed transfer encoding was used.
    &#34;&#34;&#34;
    def process(self, data):
        with SockWrapper(data) as mock:
            mock.seek(0)
            parser = HTTPResponse(mock) # type:ignore
            parser.begin()
            try:
                payload = parser.read()
            except IncompleteRead as incomplete:
                msg = F&#39;incomplete read: {len(incomplete.partial)} bytes processed, {incomplete.expected} more expected&#39;
                raise RefineryPartialResult(msg, incomplete.partial) from incomplete
            try:
                date = parser.headers[&#39;date&#39;] | datefix | str
            except Exception:
                pass
            else:
                if len(date) == 19:
                    payload = self.labelled(payload, date=date)
            return payload

    @classmethod
    def handles(cls, data) -&gt; bool | None:
        return data[:6] == B&#39;HTTP/1&#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.iemap"><code class="flex name class">
<span>class <span class="ident">iemap</span></span>
<span>(</span><span>legend=False, bgfill=False, fgchar='#', *label)</span>
</code></dt>
<dd>
<section class="desc"><p>The information entropy map displays a colored bar on the terminal visualizing the file's local
entropy from beginning to end.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/sinks/iemap.py#L14-L156" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class iemap(Unit):
    &#34;&#34;&#34;
    The information entropy map displays a colored bar on the terminal visualizing the file&#39;s local
    entropy from beginning to end.
    &#34;&#34;&#34;
    def __init__(
        self,
        legend: Param[bool, Arg.Switch(&#39;-l&#39;, help=&#39;Show entropy color legend.&#39;)] = False,
        bgfill: Param[bool, Arg.Switch(&#39;-b&#39;, help=&#39;Generate the bar by coloring the bgfill.&#39;)] = False,
        fgchar: Param[str, Arg.String(&#39;-c&#39;, &#39;--block-char&#39;, metavar=&#39;C&#39;,
            help=&#39;Character used for filling the bar, default is {default}&#39;)] = &#39;#&#39;,
        *label: Param[str, Arg.String(metavar=&#39;label-part&#39;, help=(
            &#39;The remaining command line specifies a format string expression that will be printed &#39;
            &#39;over the heat map display of each processed chunk.&#39;
        ))]
    ):
        super().__init__(label=&#39; &#39;.join(label), bgfill=bgfill, legend=legend, fgchar=fgchar)

    @Unit.Requires(&#39;colorama&#39;, [&#39;display&#39;, &#39;default&#39;, &#39;extended&#39;])
    def _colorama():
        import colorama
        return colorama

    def process(self, data):
        from os import name as os_name
        from sys import stderr
        colorama = self._colorama
        colorama.init(autoreset=False, convert=(os_name == &#39;nt&#39;))

        nobg = not self.args.bgfill
        meta = metavars(data)

        label = meta.format_str(self.args.label, self.codec, [data])
        if label:
            if not label.endswith(&#39; &#39;):
                label = F&#39;{label} &#39;
            if not label.startswith(&#39; &#39;):
                label = F&#39; {label}&#39;

        bgmap = [
            colorama.Back.BLACK,
            colorama.Back.WHITE,
            colorama.Back.YELLOW,
            colorama.Back.CYAN,
            colorama.Back.BLUE,
            colorama.Back.GREEN,
            colorama.Back.LIGHTRED_EX,
            colorama.Back.MAGENTA,
        ]
        fgmap = [
            colorama.Fore.LIGHTBLACK_EX,
            colorama.Fore.LIGHTWHITE_EX,
            colorama.Fore.LIGHTYELLOW_EX,
            colorama.Fore.LIGHTCYAN_EX,
            colorama.Fore.LIGHTBLUE_EX,
            colorama.Fore.LIGHTGREEN_EX,
            colorama.Fore.LIGHTRED_EX,
            colorama.Fore.LIGHTMAGENTA_EX,
        ]

        _reset = colorama.Back.BLACK + colorama.Fore.WHITE + colorama.Style.RESET_ALL

        clrmap = fgmap if nobg else bgmap
        header = &#39;[&#39;
        header_length = 1
        footer_length = 4 + 7

        if self.args.legend:
            header = &#39;[{1}{0}] {2}&#39;.format(_reset, &#39;&#39;.join(F&#39;{bg}{k}&#39; for k, bg in enumerate(clrmap, 1)), header)
            header_length += 3 + len(clrmap)

        _tw = get_terminal_size()
        width = _tw - header_length - footer_length
        if width &lt; 16:
            raise RuntimeError(F&#39;computed terminal width {_tw} is too small for heatmap&#39;)

        def entropy_select(value, map):
            index = min(len(map) - 1, math.floor(value * len(map)))
            return map[index]

        view = memoryview(data)
        size = len(data)
        chunk_size = 0

        for block_size in range(1, width + 1):
            block_count = width // block_size
            chunk_size = size // block_count
            if chunk_size &gt; 1024:
                break

        q, remainder = divmod(width, block_size)
        assert q == block_count
        indices = list(range(q))
        random.seed(sum(view[:1024]))
        random.shuffle(indices)

        block_sizes = [block_size] * q
        q, r = divmod(remainder, block_count)
        for i in indices:
            block_sizes[i] += q
        for i in indices[:r]:
            block_sizes[i] += 1
        assert sum(block_sizes) == width

        q, remainder = divmod(size, block_count)
        assert q == chunk_size
        chunk_sizes = [chunk_size] * block_count
        for i in indices[:remainder]:
            chunk_sizes[i] += 1
        assert sum(chunk_sizes) == size

        stream = MemoryFile(view)
        filler = self.args.fgchar if nobg else &#39; &#39;

        try:
            stderr.write(header)
            if label is not None:
                stderr.write(colorama.Fore.WHITE)
                stderr.flush()
            it = itertools.chain(itertools.repeat(filler, 3), label, itertools.cycle(filler))
            cp = None
            for chunk_size, block_size in zip(chunk_sizes, block_sizes):
                chunk = stream.read(chunk_size)
                chunk_entropy = entropy(chunk)
                pp = entropy_select(chunk_entropy, clrmap)
                string = &#39;&#39;.join(itertools.islice(it, block_size))
                if pp != cp:
                    string = F&#39;{pp}{string}&#39;
                cp = pp
                stderr.write(string)
                stderr.flush()
        except BaseException:
            eraser = &#39; &#39; * width
            stderr.write(F&#39;\r{_reset}{eraser}\r&#39;)
            raise
        else:
            stderr.write(F&#39;{_reset}] [---.--%]&#39;)
            te = meta[&#39;entropy&#39;]
            stderr.write(&#39;\b&#39; * footer_length)
            stderr.write(F&#39;] [{te!r:&gt;7}]\n&#39;)
            stderr.flush()
        if not self.isatty():
            yield data</code></pre>
</details>
</dd>
<dt id="refinery.shell.iff"><code class="flex name class">
<span>class <span class="ident">iff</span></span>
<span>(</span><span>*expression, ge=None, gt=None, le=None, lt=None, ct=None, ne=None, iN=None, eq=None, retain=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Filter incoming chunks depending on whether a given Python expression evaluates to true. If no
expression is given, the unit filters out empty chunks.</p>
<p>Note: The reverse operation of a conditional unit uses the logical negation of its condition.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/iff.py#L13-L113" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class iff(ConditionalUnit, docs=&#39;{0}{p}{1}&#39;):
    &#34;&#34;&#34;
    Filter incoming chunks depending on whether a given Python expression evaluates to true. If no
    expression is given, the unit filters out empty chunks.
    &#34;&#34;&#34;
    def __init__(
        self,
        *expression: Param[str, Arg.String(metavar=&#39;token&#39;, help=(
            &#39;All &#34;token&#34; arguments to this unit are joined with spaces to produce the expression &#39;
            &#39;to be evaluated. This is done so that unnecessary shell quoting is avoided.&#39;))],
        ge: Param[str, Arg.String(&#39;-ge&#39;, metavar=&#39;RHS&#39;, group=&#39;OP&#39;,
            help=&#39;check that the expression is greater or equal to {varname}&#39;)] = None,
        gt: Param[str, Arg.String(&#39;-gt&#39;, metavar=&#39;RHS&#39;, group=&#39;OP&#39;,
            help=&#39;check that the expression is greater than {varname}&#39;)] = None,
        le: Param[str, Arg.String(&#39;-le&#39;, metavar=&#39;RHS&#39;, group=&#39;OP&#39;,
            help=&#39;check that the expression is less or equal to {varname}&#39;)] = None,
        lt: Param[str, Arg.String(&#39;-lt&#39;, metavar=&#39;RHS&#39;, group=&#39;OP&#39;,
            help=&#39;check that the expression is less than {varname}&#39;)] = None,
        ct: Param[str, Arg.String(&#39;-ct&#39;, metavar=&#39;RHS&#39;, group=&#39;OP&#39;,
            help=&#39;check that the expression contains {varname}&#39;)] = None,
        ne: Param[str, Arg.String(&#39;-ne&#39;, metavar=&#39;RHS&#39;, group=&#39;OP&#39;,
            help=&#39;check that the expression is equal to {varname}&#39;)] = None,
        iN: Param[str, Arg.String(&#39;-in&#39;, metavar=&#39;RHS&#39;, group=&#39;OP&#39;,
            help=&#39;check that the expression is contained in {varname}&#39;)] = None,
        eq: Param[str, Arg.String(&#39;-eq&#39;, metavar=&#39;RHS&#39;, group=&#39;OP&#39;,
            help=&#39;check that the expression is equal to {varname}&#39;)] = None,
        retain=False,
    ):
        def encodings(v: str):
            if not isinstance(v, str):
                return
            for codec in [self.codec, &#39;latin1&#39;, &#39;utf-16le&#39;]:
                yield v.encode(codec)

        def __br_contains__(container, value):
            if value in container:
                return True
            if isinstance(value, str):
                return any(b in container for b in encodings(value))
            else:
                return any(value == b for v in container for b in encodings(v))

        operators = [
            (ge, operator.__ge__),
            (gt, operator.__gt__),
            (le, operator.__le__),
            (lt, operator.__lt__),
            (eq, operator.__eq__),
            (ne, operator.__ne__),
            (ct, __br_contains__),
            (iN, lambda a, b: __br_contains__(b, a)),
        ]

        operators = [
            (rhs, cmp)
            for (rhs, cmp) in operators
            if rhs is not None
        ]

        rhs, cmp, lhs = None, None, &#39;\x20&#39;.join(expression) or None

        if len(operators) &gt; 0:
            if not lhs:
                raise ValueError(&#39;Comparison operator with empty left hand side.&#39;)
            if len(operators) &gt; 1:
                raise ValueError(&#39;Only one comparison operation can be specified.&#39;)
            rhs, cmp = operators[0]

        super().__init__(
            lhs=lhs,
            rhs=rhs,
            cmp=cmp,
            retain=retain,
        )

    def match(self, chunk):
        meta = metavars(chunk)
        lhs: str | None = self.args.lhs
        rhs: Any | None = self.args.rhs
        cmp: Callable[[Any, Any], bool] | None = self.args.cmp

        if cmp is None and rhs is not None:
            raise ValueError(&#39;right hand side defined but no operator&#39;)

        if lhs is not None:
            if rhs is not None:
                lhs = DelayedNumSeqArgument(lhs, additional_types=(float, str))(chunk)
            else:
                lhs = PythonExpression.Evaluate(lhs, meta)

        rhs = rhs and DelayedNumSeqArgument(rhs, additional_types=(float, str))(chunk)

        self.log_info(F&#39;lhs: type={lhs.__class__.__name__}; value={lhs!r}&#39;)
        self.log_info(F&#39;rhs: type={rhs.__class__.__name__}; value={rhs!r}&#39;)

        if lhs is None:
            return bool(chunk)
        if rhs is None:
            return bool(lhs)

        return cmp(lhs, rhs)</code></pre>
</details>
</dd>
<dt id="refinery.shell.iffc"><code class="flex name class">
<span>class <span class="ident">iffc</span></span>
<span>(</span><span>*bounds, retain=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Filter incoming chunks depending on whether their size is within any of the given bounds.</p>
<p>Note: The reverse operation of a conditional unit uses the logical negation of its condition.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/iffc.py#L7-L35" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class iffc(ConditionalUnit, docs=&#39;{0}{p}{1}&#39;):
    &#34;&#34;&#34;
    Filter incoming chunks depending on whether their size is within any of the given bounds.
    &#34;&#34;&#34;
    def __init__(
        self,
        *bounds: Param[slice, Arg.Bounds(help=&#39;Specifies an (inclusive) range to check for.&#39;, intok=True)],
        retain=False,
    ):
        if not bounds:
            raise ValueError(&#39;cannot filter for size without specifying any bounds&#39;)
        super().__init__(
            bounds=bounds,
            retain=retain,
        )

    def match(self, chunk):
        length: int = len(chunk)
        for bounds in self.args.bounds:
            if isinstance(bounds, int):
                if length == bounds:
                    return True
            if isinstance(bounds, slice):
                a = bounds.start or 0
                b = bounds.stop or INF
                t = bounds.step or 1
                if a &lt;= length &lt;= b and not (length - a) % t:
                    return True
        return False</code></pre>
</details>
</dd>
<dt id="refinery.shell.iffp"><code class="flex name class">
<span>class <span class="ident">iffp</span></span>
<span>(</span><span>*patterns, partial=False, retain=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Filter incoming chunks depending on whether it matches any of a given set of patterns. The
available patterns are the following: integer, float, number, string, multiline_string, cmdstr, ps1str, vbastr, vbaint, printable, urlquote, urlquote_coarse, urlquote_narrow, intarray, strarray, numarray, word, letters, wshenc, alphanumeric, b32, b58, b62, b64, b85, a85, z85, b92, b64url, hex, b16, b16s, b64s, b85s, a85s, z85s, utf8, hexdump, hexarray, uuencode, domain, email, guid, date, ipv4, ipv6, md5, sha1, sha256, hostname, socket, subdomain, url, btc, pem, xmr, path_terse, path, winpath, nixpath, environment_variable.</p>
<p>Note: The reverse operation of a conditional unit uses the logical negation of its condition.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/iffp.py#L12-L36" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class iffp(ConditionalUnit, docs=&#39;{0}{p}{1}&#39;):
    &#34;&#34;&#34;
    Filter incoming chunks depending on whether it matches any of a given set of patterns. The
    available patterns are the following: {}.
    &#34;&#34;&#34;

    def __init__(
        self,
        *patterns: Param[str, Arg.Choice(metavar=&#39;pattern&#39;, choices=list(_PATTERNS))],
        partial: Param[bool, Arg.Switch(&#39;-p&#39;, help=&#39;Allow partial matches on the data.&#39;)] = False,
        retain=False
    ):
        super().__init__(
            retain=retain,
            patterns=patterns,
            partial=partial
        )

    def match(self, chunk):
        for name in self.args.patterns:
            p: pattern = _PATTERNS[name]
            matcher = p.match if self.args.partial else p.fullmatch
            if matcher(chunk):
                return True
        return False</code></pre>
</details>
</dd>
<dt id="refinery.shell.iffs"><code class="flex name class">
<span>class <span class="ident">iffs</span></span>
<span>(</span><span>needle, retain=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Filter incoming chunks depending on whether they contain a given binary substring.</p>
<p>Note: The reverse operation of a conditional unit uses the logical negation of its condition.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/iffs.py#L7-L22" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class iffs(ConditionalUnit, docs=&#39;{0}{p}{1}&#39;):
    &#34;&#34;&#34;
    Filter incoming chunks depending on whether they contain a given binary substring.
    &#34;&#34;&#34;
    def __init__(
        self,
        needle: Param[buf, Arg(help=&#39;the string to search for&#39;)],
        retain=False,
    ):
        super().__init__(
            needle=needle,
            retain=retain,
        )

    def match(self, chunk):
        return self.args.needle in chunk</code></pre>
</details>
</dd>
<dt id="refinery.shell.iffx"><code class="flex name class">
<span>class <span class="ident">iffx</span></span>
<span>(</span><span>regex, count=0, fullmatch=False, multiline=False, ignorecase=False, retain=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Filter incoming chunks by discarding those that do not match the given
regular expression.</p>
<p>Note: The reverse operation of a conditional unit uses the logical negation of its condition.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/iffx.py#L7-L19" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class iffx(SingleRegexUnit, ConditionalUnit, docs=&#39;{0}{p}{1}&#39;):
    &#34;&#34;&#34;
    Filter incoming chunks by discarding those that do not match the given
    regular expression.
    &#34;&#34;&#34;
    def __init__(self, regex, count=0, fullmatch=False, multiline=False, ignorecase=False, retain=False):
        pass

    def match(self, chunk):
        if matcher := self._make_matcher(self.args.regex):
            return bool(matcher(chunk))
        else:
            return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.ifps"><code class="flex name class">
<span>class <span class="ident">ifps</span></span>
<span>(</span><span>bytes, codec='cp1252')</span>
</code></dt>
<dd>
<section class="desc"><p>Disassembles compiled Pascal script files that start with the magic sequence "IFPS". These
scripts can be found, for example, when unpacking InnoSetup installers using innounp.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/ifps.py#L22-L39" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ifps(IFPSBase):
    &#34;&#34;&#34;
    Disassembles compiled Pascal script files that start with the magic sequence &#34;IFPS&#34;. These
    scripts can be found, for example, when unpacking InnoSetup installers using innounp.
    &#34;&#34;&#34;
    def __init__(
        self,
        bytes: Param[bool, Arg.Switch(&#39;-b&#39;, help=&#39;Print opcode bytes in the disassembly.&#39;)],
        codec=&#39;cp1252&#39;
    ):
        super().__init__(codec=codec, bytes=bytes)

    def process(self, data):
        return IFPSFile(data, self.args.codec).disassembly(self.args.bytes).encode(self.codec)

    @classmethod
    def handles(cls, data) -&gt; bool:
        return data[:len(IFPSFile.Magic)] == IFPSFile.Magic</code></pre>
</details>
</dd>
<dt id="refinery.shell.ifpsstr"><code class="flex name class">
<span>class <span class="ident">ifpsstr</span></span>
<span>(</span><span>codec='cp1252')</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts strings from compiled Pascal script files that start with the magic sequence "IFPS".
These scripts can be found, for example, when unpacking InnoSetup installers using innounp.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/ifpsstr.py#L7-L19" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ifpsstr(IFPSBase):
    &#34;&#34;&#34;
    Extracts strings from compiled Pascal script files that start with the magic sequence &#34;IFPS&#34;.
    These scripts can be found, for example, when unpacking InnoSetup installers using innounp.
    &#34;&#34;&#34;
    def process(self, data):
        ifps = IFPSFile(data, self.args.codec)
        for string in ifps.strings:
            yield string.encode(self.codec)

    @classmethod
    def handles(cls, data) -&gt; bool:
        return data[:len(IFPSFile.Magic)] == IFPSFile.Magic</code></pre>
</details>
</dd>
<dt id="refinery.shell.imgdb"><code class="flex name class">
<span>class <span class="ident">imgdb</span></span>
</code></dt>
<dd>
<section class="desc"><p>Provides access to the direct bytes of an image file. Each row of pixels is emitted as an
individual chunk.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/imgdb.py#L14-L53" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class imgdb(Unit):
    &#34;&#34;&#34;
    Provides access to the direct bytes of an image file. Each row of pixels is emitted as an
    individual chunk.
    &#34;&#34;&#34;
    @Unit.Requires(&#39;Pillow&#39;, [&#39;formats&#39;])
    def _image():
        from PIL import Image
        return Image

    def _get_rows(self, image: Image):
        width = image.width
        pixels = iter(image.getdata())
        while row := list(islice(pixels, 0, width)):
            yield row

    def process(self, data):
        try:
            image = self._image.open(MemoryFile(data, output=bytes))
        except Exception:
            raise ValueError(&#39;input could not be parsed as an image&#39;)
        test = image.getpixel((0, 0))
        if isinstance(test, int):
            self.log_info(&#39;reading each pixel as an integer&#39;)
            for row in self._get_rows(image):
                yield bytearray(row)
        else:
            self.log_info(&#39;reading each pixel as a color value tuple&#39;)
            count = len(test)
            total = count * image.width
            out = bytearray(total)
            for row in self._get_rows():
                for pixel, offset in zip(row, range(0, total, count)):
                    out[offset:offset + count] = pixel
            yield out

    @classmethod
    def handles(cls, data) -&gt; bool | None:
        if get_image_format(data) is not None:
            return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.imgto"><code class="flex name class">
<span>class <span class="ident">imgto</span></span>
<span>(</span><span>format='png')</span>
</code></dt>
<dd>
<section class="desc"><p>Convert an image to a given format.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/imgto.py#L11-L41" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class imgto(Unit):
    &#34;&#34;&#34;
    Convert an image to a given format.
    &#34;&#34;&#34;
    def __init__(
        self,
        format: Param[str, Arg.String(
            help=&#39;An image file format like png, jpg, or bmp. The default is {default}.&#39;)] = &#39;png&#39;
    ):
        super().__init__(format=format)

    @Unit.Requires(&#39;Pillow&#39;, [&#39;formats&#39;])
    def _image():
        from PIL import Image
        return Image

    def process(self, data):
        try:
            image = self._image.open(MemoryFile(data, output=bytes))
        except ImportError:
            raise
        except Exception:
            raise ValueError(&#39;input could not be parsed as an image&#39;)
        with io.BytesIO() as out:
            image.save(out, self.args.format)
            return out.getvalue()

    @classmethod
    def handles(cls, data) -&gt; bool | None:
        if get_image_format(data) is not None:
            return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.imgtp"><code class="flex name class">
<span>class <span class="ident">imgtp</span></span>
<span>(</span><span>transformation='R')</span>
</code></dt>
<dd>
<section class="desc"><p>Perform a number of transpositions on an input image. The transformation string must be a
sequence composed of the letters H, V, and R. Each letter represents an operation:</p>
<ul>
<li>R rotates the image to the left by 90 degrees.</li>
<li>V flips the image top to bottom (vertically).</li>
<li>H flips the image left to right (horizontally).</li>
</ul>
<p>These transpositions are performed in the order in which they are specified.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/imgtp.py#L18-L64" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class imgtp(Unit):
    &#34;&#34;&#34;
    Perform a number of transpositions on an input image. The transformation string must be a
    sequence composed of the letters H, V, and R. Each letter represents an operation:

    - R rotates the image to the left by 90 degrees.
    - V flips the image top to bottom (vertically).
    - H flips the image left to right (horizontally).

    These transpositions are performed in the order in which they are specified.
    &#34;&#34;&#34;
    def __init__(
        self,
        transformation: Param[str, Arg.String(help=&#39;The transformation sequence; default is {default}.&#39;)] = &#39;R&#39;
    ):
        transformation = [Arg.AsOption(t, T) for t in transformation]
        super().__init__(transformation=transformation)

    @Unit.Requires(&#39;Pillow&#39;, [&#39;formats&#39;])
    def _image():
        from PIL import Image
        return Image

    def process(self, data):
        imglib = self._image

        try:
            image = imglib.open(MemoryFile(data, output=bytes))
        except Exception:
            raise ValueError(&#39;input could not be parsed as an image&#39;)
        else:
            format = image.format
        conversion = {
            T.V: imglib.Transpose.FLIP_TOP_BOTTOM,
            T.H: imglib.Transpose.FLIP_LEFT_RIGHT,
            T.R: imglib.Transpose.ROTATE_90,
        }
        for tf in self.args.transformation:
            image = image.transpose(conversion[tf])
        with io.BytesIO() as out:
            image.save(out, format)
            return out.getvalue()

    @classmethod
    def handles(cls, data) -&gt; bool | None:
        if get_image_format(data) is not None:
            return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.imphash"><code class="flex name class">
<span>class <span class="ident">imphash</span></span>
<span>(</span><span>reps=1, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Implements the import hash for PE files.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/hash/imphash.py#L7-L15" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class imphash(HashUnit):
    &#34;&#34;&#34;
    Implements the import hash for PE files.
    &#34;&#34;&#34;

    def _algorithm(self, data):
        pe = lief.load_pe(data)
        th = lief.PE.get_imphash(pe, lief.PE.IMPHASH_MODE.PEFILE)
        return bytes.fromhex(th)</code></pre>
</details>
</dd>
<dt id="refinery.shell.innopwd"><code class="flex name class">
<span>class <span class="ident">innopwd</span></span>
</code></dt>
<dd>
<section class="desc"><p>This unit emulates an InnoSetup installer in an attempt to determine the installer password.
This works only when the password is contained within the script, but several malware samples
are known to use this technique.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/innopwd.py#L9-L45" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class innopwd(Unit):
    &#34;&#34;&#34;
    This unit emulates an InnoSetup installer in an attempt to determine the installer password.
    This works only when the password is contained within the script, but several malware samples
    are known to use this technique.
    &#34;&#34;&#34;
    def process(self, data: bytearray):
        if data.startswith(IFPSFile.Magic):
            inno = IFPSFile(data)
            self.log_info(&#39;running in script-only mode; cannot check passwords&#39;)
            file = None
        else:
            inno = InnoArchive(data, self)
            file = inno.get_encrypted_sample()
            if file is None:
                self.log_info(&#39;the archive is not password-protected, password is empty&#39;)
                return None
            self.log_info(&#39;password type:&#39;, file.password_type.name)
            self.log_info(&#39;password hash:&#39;, file.password_hash)
            self.log_info(&#39;password salt:&#39;, file.password_salt)

        emulator = InnoSetupEmulator(inno)

        for password in emulator.emulate_installation():
            if not isinstance(password, NewPassword):
                continue
            if file and not inno.check_password(file, password):
                self.log_info(&#39;discarding password:&#39;, password)
                continue
            yield password.encode(self.codec)
            if file is not None:
                self.log_info(&#39;aborting emulation after validating password&#39;)
                return

    @classmethod
    def handles(cls, data):
        return is_inno_setup(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.isaac"><code class="flex name class">
<span>class <span class="ident">isaac</span></span>
<span>(</span><span>key, discard=0, stateful=False)</span>
</code></dt>
<dd>
<section class="desc"><p>The ISAAC (Indirection, Shift, Accumulate, Add, Count) cipher.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/isaac.py#L10-L67" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class isaac(StreamCipherUnit):
    &#34;&#34;&#34;
    The ISAAC (Indirection, Shift, Accumulate, Add, Count) cipher.
    &#34;&#34;&#34;

    def keystream(self) -&gt; Iterable[int]:
        key = self.args.key

        A: int = 0
        B: int = 0
        C: int = 0
        S: list[int] = [0x9E3779B9] * 8
        T: list[int] = []
        K = list(chunks.unpack(key + bytearray(0x400 - len(key)), 4, bigendian=False))
        U = 0xFFFFFFFF

        def _mix_state():
            a, b, c, d, e, f, g, h = S
            a ^= (b &lt;&lt; 0x0B) &amp; U; d = d + a &amp; U; b = b + c &amp; U # noqa
            b ^= (c &gt;&gt; 0x02) &amp; U; e = e + b &amp; U; c = c + d &amp; U # noqa
            c ^= (d &lt;&lt; 0x08) &amp; U; f = f + c &amp; U; d = d + e &amp; U # noqa
            d ^= (e &gt;&gt; 0x10) &amp; U; g = g + d &amp; U; e = e + f &amp; U # noqa
            e ^= (f &lt;&lt; 0x0A) &amp; U; h = h + e &amp; U; f = f + g &amp; U # noqa
            f ^= (g &gt;&gt; 0x04) &amp; U; a = a + f &amp; U; g = g + h &amp; U # noqa
            g ^= (h &lt;&lt; 0x08) &amp; U; b = b + g &amp; U; h = h + a &amp; U # noqa
            h ^= (a &gt;&gt; 0x09) &amp; U; c = c + h &amp; U; a = a + b &amp; U # noqa
            S[:] = a, b, c, d, e, f, g, h
            return S

        def _initialize_with(R: list[int]):
            for i in range(0, 0x100, 8):
                S[:] = (x + R[j] &amp; U for j, x in enumerate(S, i))
                T[i:i + 8] = _mix_state()

        for _ in range(4):
            _mix_state()

        _initialize_with(K)
        _initialize_with(T)

        operations = [
            (__lshift__, 0x0D),
            (__rshift__, 0x06),
            (__lshift__, 0x02),
            (__rshift__, 0x10),
        ]

        while True:
            C = (C + 1) &amp; U
            B = (B + C) &amp; U
            for i in range(0x100):
                X = T[i]
                shift, k = operations[i % 4]
                A = (A ^ shift(A, k)) &amp; U
                A = (A + T[i ^ 0x80]) &amp; U
                Y = T[+i] = T[X // 4 &amp; 0xFF] + A + B &amp; U
                B = K[~i] = X + T[Y // 1024 &amp; 0xFF] &amp; U
            yield from chunks.pack(K, 4, True)</code></pre>
</details>
</dd>
<dt id="refinery.shell.jamv"><code class="flex name class">
<span>class <span class="ident">jamv</span></span>
<span>(</span><span>name, data=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Short for "Join as Meta Variables": It joins all chunks in the current frame into a single one
by storing the contents of each chunk as the contents of a meta variable in the output.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/jamv.py#L7-L51" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class jamv(Unit):
    &#34;&#34;&#34;
    Short for &#34;Join as Meta Variables&#34;: It joins all chunks in the current frame into a single one
    by storing the contents of each chunk as the contents of a meta variable in the output.
    &#34;&#34;&#34;
    def __init__(
        self,
        name: Param[str, Arg.String(metavar=&#39;format&#39;, help=(
            &#39;A format string that specifies the variable name for storing the chunk.&#39;))],
        data: Param[buf, Arg.Binary(metavar=&#39;data&#39;, help=(
            &#39;Optionally specify the body of the fused output chunk; empty by default.&#39;))] = None,
    ):
        super().__init__(name=name, data=data)

    def process(self, data: Chunk):
        try:
            meta = data.temp
        except Exception:
            meta = None
        if not isinstance(meta, dict):
            raise RuntimeError(&#39;this unit can only be used inside a frame&#39;)
        data.meta.update(meta)
        data[:] = self.args.data or B&#39;&#39;
        return data

    def filter(self, inputs):
        head = None
        spec = self.args.name
        meta = {}
        for chunk in inputs:
            if not chunk.visible:
                yield chunk
                continue
            used = set()
            name = chunk.meta.format_str(spec, self.codec, [chunk], used=used)
            if head is None:
                for u in used:
                    chunk.meta.discard(u)
                head = chunk
            if name in meta:
                self.log_warn(&#39;overwriting duplicate variable:&#39;, name, clip=True)
            meta[name] = chunk
        if head:
            head.temp = meta
            yield head</code></pre>
</details>
</dd>
<dt id="refinery.shell.jcalg"><code class="flex name class">
<span>class <span class="ident">jcalg</span></span>
<span>(</span><span>ignore_header=False)</span>
</code></dt>
<dd>
<section class="desc"><p>JCALG decompression.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/jcalg.py#L9-L124" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class jcalg(Unit):
    &#34;&#34;&#34;
    JCALG decompression.
    &#34;&#34;&#34;
    def __init__(
        self,
        ignore_header: Param[bool, Arg(&#39;-g&#39;, help=(
            &#39;Keep decompressing even after the output has reached the final size as given by the header value.&#39;))] = False,
    ):
        super().__init__(ignore_header=ignore_header)

    def process(self, data: bytearray):
        with MemoryFile() as output, StructReader(data) as reader:
            if reader.read(2) != B&#39;JC&#39;:
                self.log_warn(&#39;data does not begin with magic sequence, assuming that header is missing&#39;)
                reader.seek(0)
                size = checksum = None
            else:
                size = reader.u32()
                checksum = reader.u32()
            if self.args.ignore_header:
                size = None
            self._decompress(output, reader, size)
            if size is not None:
                if len(output) &gt; size:
                    self.log_info(F&#39;tuncating to size {size}&#39;)
                    output.truncate(size)
                elif len(output) &lt; size:
                    self.log_warn(F&#39;header size was {size}, but only {len(data)} bytes were decompressed&#39;)
            data = output.getvalue()
            if checksum:
                c = self._checksum(data)
                if c != checksum:
                    self.log_warn(F&#39;header checksum was {checksum:08X}, computed value is {c:08X}&#39;)
            return data

    @classmethod
    def handles(cls, data):
        if data[:2] == B&#39;JC&#39;:
            return True

    def _checksum(self, data):
        from refinery.lib import chunks
        checksum = 0
        it = chunks.unpack(data, 4)
        if len(data) % 4:
            import itertools
            it = itertools.chain(it, (int.from_bytes(data[-4:], &#39;little&#39;),))
        for chunk in it:
            checksum += chunk
            checksum ^= ((chunk &amp; 0x7FFFFFFF) &lt;&lt; 1) + (chunk &gt;&gt; 31) + 1
            checksum &amp;= 0xFFFFFFFF
        return checksum

    def _decompress(self, writer: MemoryFile, reader_: StructReader[bytearray], size: int | None = None):
        index = 1
        base = 8
        literal_bits = None
        literal_offset = None
        flags = BitBufferedReader(reader_, 32)

        while True:
            if size and len(writer) &gt;= size:
                break
            if flags.next():
                b = flags.read(literal_bits) + literal_offset
                b = b &amp; 0xFF
                writer.write_byte(b)
                continue
            if flags.next():
                high = flags.variable_length_integer()
                if high == 2:
                    match_length = flags.variable_length_integer()
                else:
                    index = ((high - 3) &lt;&lt; base) + flags.read(base)
                    match_length = flags.variable_length_integer()
                    if index &gt;= 0x10000:
                        match_length += 3
                    elif index &gt;= 0x37FF:
                        match_length += 2
                    elif index &gt;= 0x27F:
                        match_length += 1
                    elif index &lt;= 127:
                        match_length += 4
                writer.replay(index, match_length)
                continue
            if not flags.next():
                new_index = flags.read(7)
                match_length = 2 + flags.read(2)
                if new_index == 0:
                    if match_length == 2:
                        break
                    base = flags.read(match_length + 1)
                else:
                    index = new_index
                    writer.replay(index, match_length)
                continue
            one_byte_phrase_value = flags.read(4) - 1
            if one_byte_phrase_value == 0:
                writer.write_byte(0)
            elif one_byte_phrase_value &gt; 0:
                b = writer.getvalue()[-one_byte_phrase_value]
                writer.write_byte(b)
            else:
                if not flags.next():
                    literal_bits = 7 + flags.next()
                    literal_offset = 0
                    if literal_bits != 8:
                        literal_offset = flags.read(8)
                    continue
                while True:
                    for _ in range(0x100):
                        b = flags.read(8)
                        writer.write_byte(b)
                    if not flags.next():
                        break</code></pre>
</details>
</dd>
<dt id="refinery.shell.jvdasm"><code class="flex name class">
<span>class <span class="ident">jvdasm</span></span>
<span>(</span><span>*paths, gray=False, path=b'path', regex=False, exact=False, fuzzy=0, drop_path=False, join_path=False, list=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Disassembles the JVM bytecode instructions of methods of classes defined in Java class
files. The unit is implemented as a path extractor and each path name corresponds to the
name of one method defined in the class file.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/java/jvdasm.py#L63-L215" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class jvdasm(PathExtractorUnit):
    &#34;&#34;&#34;
    Disassembles the JVM bytecode instructions of methods of classes defined in Java class
    files. The unit is implemented as a path extractor and each path name corresponds to the
    name of one method defined in the class file.
    &#34;&#34;&#34;
    _OPC_STRLEN = max(len(op.name) for op in opc)

    def _hex(self, bytestring, sep=&#39;&#39;):
        return sep.join(F&#39;{x:02x}&#39; for x in bytestring)

    def __init__(
        self, *paths,
        gray: Param[bool, Arg.Switch(&#39;-g&#39;, help=&#39;Disable colored output.&#39;)] = False,
        **keywords
    ):
        super().__init__(*paths, gray=gray, **keywords)

    def unpack(self, data):
        def _name(method: JvClassMember):
            name = method.name
            if name == &#39;&lt;init&gt;&#39;:
                _, _, name = str(jc.this).rpartition(&#39;/&#39;)
            elif m := re.fullmatch(&#39;&lt;(.*?)&gt;&#39;, name):
                name = F&#39;.{m[0]}&#39;
            return name

        def _path(method: JvClassMember):
            return F&#39;{jc.this!s}/{_name(method)}&#39;
        try:
            if self.args.gray or not self.isatty():
                raise ImportError
            import colorama
        except ImportError:
            class _FG():
                def __getattr__(self, _):
                    return &#39;&#39;
            FG = _FG()
            RS = &#39;&#39;
        else:
            FG = colorama.Fore
            RS = colorama.Style.RESET_ALL
        finally:
            c_none = RS
            c_space = FG.LIGHTCYAN_EX
            c_types = FG.LIGHTCYAN_EX
            c_member = FG.LIGHTYELLOW_EX
            c_kwd = FG.LIGHTYELLOW_EX
            c_const = FG.LIGHTRED_EX
            c_string = FG.LIGHTRED_EX
            c_address = FG.LIGHTBLACK_EX
            c_label = RS

        def _color(arg, offset):
            if isinstance(arg, (str, JvString)):
                color = c_string
            elif isinstance(arg, (JvClassProperty, JvTypePath)):
                ns, dd, prop = str(arg).partition(&#39;::&#39;)
                if not dd:
                    return repr(arg)
                ns = ns.split(&#39;.&#39;)
                ns = &#39;.&#39;.join(F&#39;{c_space}{p}{c_none}&#39; for p in ns)
                return F&#39;{ns}{dd}{c_member}{prop}{c_none}&#39;
            elif isinstance(arg, int) and arg + offset in labels:
                return F&#39;{c_label}0x{arg + offset:08X}{c_none}&#39;
            elif isinstance(arg, (bool, int, float)):
                color = c_const
            elif isinstance(arg, JvBaseType):
                color = c_kwd
            else:
                return repr(arg)
            return F&#39;{color}{arg!r}{c_none}&#39;

        jc = JvClassFile(data)
        tab = &#39; &#39;
        namespace = &#39;.&#39;.join(str(jc.this).split(&#39;/&#39;))
        opcw = self._OPC_STRLEN
        path_counter = collections.defaultdict(int)
        path_index = collections.defaultdict(int)

        for method in jc.methods:
            path_counter[_path(method)] += 1
        for method in jc.methods:
            for attribute in method.attributes:
                if attribute.name == &#39;Code&#39;:
                    break
            else:
                self.log_warn(F&#39;no code found for method: {method.name}&#39;)
                continue
            code: JvCode = attribute.parse(JvCode)
            with io.StringIO() as display:
                rv, args = _parse_descriptor(method.descriptor, c_none, c_space, c_types, c_kwd)
                args = &#39;, &#39;.join(args)
                print(
                    F&#39;{c_types}{rv}{c_none} {c_space}{namespace}{c_none}&#39;
                    F&#39;::{c_member}{_name(method)}{c_none}({args})&#39;, file=display)
                offset = 0
                labels = set()
                addresses = set()

                for op in code.disassembly:
                    addresses.add(offset)
                    if op.table:
                        labels.update(offset + jmp for jmp in op.table.values())
                    elif op.code in (opc.goto, opc.goto_w):
                        labels.update(offset + arg for arg in op.arguments if isinstance(arg, int))
                    offset += len(op.raw)

                offset = 0
                labels = labels &amp; addresses

                for op in code.disassembly:
                    if offset in labels:
                        label = F&#39;{c_label}{offset:08X}{c_none}:&#39;
                    else:
                        label = F&#39;{c_address}{offset:08X}{c_none}:&#39;
                    addr = offset
                    olen = len(op.raw)
                    offset += olen
                    if op.table is None:
                        args = &#39;, &#39;.join(_color(a, addr) for a in op.arguments)
                    else:
                        ow = 4 if op.code is opc.tableswitch else 8
                        olen = olen - (len(op.table) - 1) * ow
                        args = F&#39;___default =&gt; {c_label}{op.table[None] + addr:#010x}{c_none}&#39;
                        jmps = []
                        for k, (key, jmp) in enumerate(op.table.items()):
                            if key is None:
                                continue
                            raw = self._hex(op.raw[olen + k * ow: olen + k * ow + ow], &#39; &#39;)
                            jmps.append(
                                F&#39;{label}{tab}&#39;
                                F&#39;{raw!s:&lt;{opcw + 15}} &#39;
                                F&#39;{c_const}{key:#010x}{c_none} =&gt; &#39;
                                F&#39;{c_label}{jmp + addr:#010x}{c_none}&#39;)
                        args = &#39;\n&#39;.join((args, *jmps))
                    opch = self._hex(op.raw[:olen], &#39; &#39;)
                    if len(opch) &gt; 14:
                        opch += F&#39;\n{label}{tab}{tab:&lt;15}&#39;
                    print(
                        F&#39;{label}{tab}&#39;
                        F&#39;{opch:&lt;15}&#39;
                        F&#39;{c_kwd}{op.code!r:&lt;{opcw}}{c_none} {args}&#39;, file=display)
                path = _path(method)
                if path_counter[path] &gt; 1:
                    k = path_index[path]
                    path_index[path] = k + 1
                    path = F&#39;{path}[{k}]&#39;
                yield UnpackResult(path, display.getvalue().encode(self.codec))

    @classmethod
    def handles(cls, data):
        return data.startswith(B&#39;\xCA\xFE\xBA\xBE&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.jvstr"><code class="flex name class">
<span>class <span class="ident">jvstr</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extract string constants from Java class files.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/java/jvstr.py#L7-L18" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class jvstr(Unit):
    &#34;&#34;&#34;
    Extract string constants from Java class files.
    &#34;&#34;&#34;
    def process(self, data):
        jc = JvClassFile(data)
        for string in jc.strings:
            yield string.encode(self.codec)

    @classmethod
    def handles(cls, data):
        return data.startswith(B&#39;\xCA\xFE\xBA\xBE&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.kblob"><code class="flex name class">
<span>class <span class="ident">kblob</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts a key from a Microsoft Crypto API BLOB structure.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/keyderive/kblob.py#L7-L21" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class kblob(Unit):
    &#34;&#34;&#34;
    Extracts a key from a Microsoft Crypto API BLOB structure.
    &#34;&#34;&#34;

    def process(self, data):
        blob = CRYPTOKEY(data)
        try:
            return self.labelled(
                bytes(blob.key),
                type=blob.header.type.name,
                algorithm=blob.header.algorithm.name
            )
        except AttributeError as A:
            raise ValueError(F&#39;unable to derive key from {blob.header.type!s}&#39;) from A</code></pre>
</details>
</dd>
<dt id="refinery.shell.keccak"><code class="flex name class">
<span>class <span class="ident">keccak</span></span>
<span>(</span><span>reps=1, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the KECCAK hash of the input data.</p></section>
</dd>
<dt id="refinery.shell.kramer"><code class="flex name class">
<span>class <span class="ident">kramer</span></span>
</code></dt>
<dd>
<section class="desc"><p>Deobfuscate Python samples obfuscated with Kramer.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/malware/kramer.py#L15-L108" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class kramer(Unit):
    &#34;&#34;&#34;
    Deobfuscate Python samples obfuscated with Kramer.
    &#34;&#34;&#34;

    _LINEBREAK_MAGIC = 950

    def process(self, data):
        kramer = &#39;&#39;
        secret = set()
        _pyver = None

        def crawl(code: CodeType, depth=1):
            nonlocal kramer
            for instruction in disassemble_code(code, _pyver):
                arg = instruction.argval
                if arg is None:
                    continue
                if isinstance(arg, tuple):
                    continue
                if isinstance(arg, str):
                    if len(arg) &gt; len(kramer):
                        kramer = arg
                    continue
                if isinstance(arg, int):
                    secret.add(arg)
                    continue
                try:
                    crawl(arg, depth + 1)
                except Exception as E:
                    self.log_info(F&#39;error crawling arg of type {type(arg).__name__} at depth {depth}: {E}&#39;)

        for code in extract_code_from_buffer(bytes(data)):
            _pyver = code.version
            crawl(code.container)

        if not kramer:
            raise ValueError(&#39;could not find the encoded string&#39;)

        separator = re.search(&#39;[^a-fA-F0-9]+&#39;, kramer)

        if not separator:
            raise ValueError(&#39;no separator detected; encoding method may have changed&#39;)

        def rotchar(c: int):
            if c in range(0x61, 0x7a) or c in range(0x30, 0x39):
                return c + 1
            if c == 0x7a:
                return 0x30
            if c == 0x39:
                return 0x61
            return c

        def decrypt(c: int, k: int):
            if c &gt;= k:
                out = rotchar(c - k)
                if out not in range(0x100):
                    raise _WrongKey
                return out
            if c == self._LINEBREAK_MAGIC:
                return 0x0A
            raise _WrongKey

        def decrypt_with_key(key: int):
            decrypted = bytearray(decrypt(c, key) for c in encrypted)
            if not re.fullmatch(B&#39;[\\s!-~]+&#39;, decrypted):
                raise _WrongKey
            return decrypted

        separator = separator.group(0)
        encrypted = [ord(bytes.fromhex(e).decode()) for e in kramer.split(separator)]

        ubound = min(x for x in encrypted if x != self._LINEBREAK_MAGIC)
        lbound = ubound - 0xFF

        secret = {k for k in secret if k &gt; lbound and k &lt; ubound}
        self.log_debug(&#39;potential secrets from code:&#39;, secret)

        for key in sorted(secret, reverse=True):
            try:
                return decrypt_with_key(key)
            except _WrongKey:
                pass

        self.log_info(F&#39;all candidates failed, searching [{lbound}, {ubound}]&#39;)

        for key in range(ubound, lbound - 1, -1):
            try:
                self.log_debug(&#39;attempting key:&#39;, key)
                return decrypt_with_key(key)
            except _WrongKey:
                pass

        raise RuntimeError(&#39;could not find decryption key&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.lnk"><code class="flex name class">
<span>class <span class="ident">lnk</span></span>
<span>(</span><span>tabular=False, details=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Parse Windows Shortcuts (LNK files) and returns the parsed information in JSON format. This
unit is a thin wrapper around the LnkParse3 library.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/lnk.py#L11-L60" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class lnk(Unit):
    &#34;&#34;&#34;
    Parse Windows Shortcuts (LNK files) and returns the parsed information in JSON format. This
    unit is a thin wrapper around the LnkParse3 library.
    &#34;&#34;&#34;

    @Unit.Requires(&#39;LnkParse3&gt;=1.4.0&#39;, [&#39;formats&#39;, &#39;default&#39;, &#39;extended&#39;])
    def _LnkParse3():
        import LnkParse3
        return LnkParse3

    _PATHS = {
        &#39;data&#39;: ...,
        &#39;header&#39;: {&#39;creation_time&#39;, &#39;accessed_time&#39;, &#39;modified_time&#39;, &#39;windowstyle&#39;},
        &#39;link_info&#39;: {&#39;local_base_path&#39;, &#39;location&#39;},
    }

    def __init__(
        self,
        tabular: Param[bool, Arg(&#39;-t&#39;, help=&#39;Print information in a table rather than as JSON.&#39;)] = False,
        details: Param[bool, Arg(&#39;-d&#39;, help=&#39;Print all details; some properties are hidden by default.&#39;)] = False,
    ):
        super().__init__(tabular=tabular, details=details)

    def process(self, data):
        with NoLogging():
            parsed = self._LnkParse3.lnk_file(MemoryFile(data)).get_json()
        if not self.args.details:
            paths = self._PATHS
            noise = [key for key in parsed if key not in paths]
            for key in noise:
                del parsed[key]
            for path, scope in paths.items():
                if scope is (...):
                    continue
                try:
                    section = parsed[path]
                except KeyError:
                    continue
                noise = [key for key in section if key not in scope]
                for key in noise:
                    del section[key]
        with JSONEncoderEx as encoder:
            pp = ppjson(tabular=self.args.tabular)
            yield from pp._pretty_output(
                parsed, indent=4, cls=encoder, ensure_ascii=False)

    @classmethod
    def handles(cls, data):
        return data[:20] == B&#39;L\0\0\0\01\x14\02\0\0\0\0\0\xC0\0\0\0\0\0\0F&#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.loop"><code class="flex name class">
<span>class <span class="ident">loop</span></span>
<span>(</span><span>iterations, statements, do_while, do_until, fullmatch=False, multiline=False, ignorecase=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Applies a given multibin suffix to the input chunk repeatedly. For example, the following
command would carve the largest base64-encoded buffer from the input, decode it, and then
decompress the result 20 times:</p>
<pre><code>emit data | loop 20 csd[b64]:zl
</code></pre>
<p>Notably, the argument after the iterations is a suffix, which means that handlers are applied
from left to right (not from right to left). The loop is aborted and the previous result
returned if the newly computed result is empty. If the an error occurs while computing
the statements and the unit is lenient (i.e. the <code>-L</code> switch is set), the last known result
is returned.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/loop.py#L10-L81" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class loop(RegexUnit):
    &#34;&#34;&#34;
    Applies a given multibin suffix to the input chunk repeatedly. For example, the following
    command would carve the largest base64-encoded buffer from the input, decode it, and then
    decompress the result 20 times:

        emit data | loop 20 csd[b64]:zl

    Notably, the argument after the iterations is a suffix, which means that handlers are applied
    from left to right (not from right to left). The loop is aborted and the previous result
    returned if the newly computed result is empty. If the an error occurs while computing
    the statements and the unit is lenient (i.e. the `-L` switch is set), the last known result
    is returned.
    &#34;&#34;&#34;

    def __init__(
        self,
        iterations: Param[int, Arg.Number(metavar=&#39;iterations&#39;,
            help=&#39;The number of repeated applications of the statements.&#39;)],
        statements: Param[str, Arg.String(metavar=&#39;statements&#39;,
            help=&#39;A multibin expression suffix representing the loop statements.&#39;)],
        do_while: Param[str, Arg.RegExp(&#39;-w&#39;, &#39;--while&#39;, metavar=&#39;RE&#39;,
            help=&#39;Halt when the given regular expression does not match the data.&#39;)],
        do_until: Param[str, Arg.RegExp(&#39;-u&#39;, &#39;--until&#39;, metavar=&#39;RE&#39;,
            help=&#39;Halt when the given regular expression matches the data.&#39;)],
        fullmatch=False, multiline=False, ignorecase=False,
    ):
        super().__init__(
            iterations=iterations,
            statements=statements,
            do_while=do_while,
            do_until=do_until,
            fullmatch=fullmatch,
            multiline=multiline,
            ignorecase=ignorecase,
        )

    def process(self, data):
        _count = self.args.iterations
        _width = len(str(_count))
        _while = self._while
        _until = self._until

        for k in range(_count):
            if _while and not _while(data):
                self.log_info(F&#39;step {k:0{_width}}: stopping, while-condition violated&#39;)
                break
            if _until and _until(data):
                self.log_info(F&#39;step {k:0{_width}}: stopping, until-condition satisfied&#39;)
                break
            try:
                out = DelayedBinaryArgument(
                    self.args.statements, reverse=True, seed=data)(data)
            except Exception as error:
                self.log_info(F&#39;step {k:0{_width}}: error;&#39;, exception_to_string(error))
                msg = F&#39;Stopped after {k} steps, increase verbosity for additional details.&#39;
                raise RefineryPartialResult(msg, data) from error
            if not out:
                self.log_info(F&#39;step {k:0{_width}}: stopping after empty result&#39;)
                break
            data[:] = out
            self.log_debug(F&#39;step {k:0{_width}}: data =&#39;, data, clip=True)

        return data

    @property
    def _while(self):
        return self._make_matcher(self.args.do_while)

    @property
    def _until(self):
        return self._make_matcher(self.args.do_until)</code></pre>
</details>
</dd>
<dt id="refinery.shell.lz4"><code class="flex name class">
<span>class <span class="ident">lz4</span></span>
</code></dt>
<dd>
<section class="desc"><p>LZ4 block decompression. See also:
<a href="https://github.com/lz4/lz4/blob/master/doc/lz4_Block_format.md#compressed-block-format">https://github.com/lz4/lz4/blob/master/doc/lz4_Block_format.md#compressed-block-format</a></p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/lz4.py#L21-L150" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class lz4(Unit):
    &#34;&#34;&#34;
    LZ4 block decompression. See also:
    https://github.com/lz4/lz4/blob/master/doc/lz4_Block_format.md#compressed-block-format
    &#34;&#34;&#34;
    def _read_block(self, reader: StructReader, output: io.BytesIO, ubound=None):
        entry = reader.tell()
        lastend = 0

        def ubound_check():
            if ubound is None:
                return False
            consumed = reader.tell() - entry
            if consumed &gt; ubound:
                raise ValueError(F&#39;upper bound {ubound} exceeded by {consumed - ubound} in LZ4 block&#39;)
            return consumed == ubound

        while not reader.eof:
            reflen = reader.read_nibble()
            litlen = reader.read_nibble()
            litlen = reader.read_size(litlen)
            literal = reader.read(litlen)
            output.write(literal)
            if ubound_check():
                break
            try:
                refpos = reader.u16()
            except EOFError:
                break
            if refpos - 1 not in range(output.tell()):
                with StreamDetour(output, lastend):
                    if output.read(len(literal)) == literal:
                        # This literal could have been encoded in the last match, but it wasn&#39;t.
                        # Therefore, it is very likely that we have reached the end of the stream.
                        break
                position = reader.tell()
                remaining = len(literal) - position
                raise RefineryPartialResult(
                    F&#39;encountered invalid match offset value {refpos} at position {position} with {remaining} bytes remaining&#39;,
                    partial=output.getvalue())
            reflen = reader.read_size(reflen)
            if ubound_check():
                raise ValueError(&#39;last sequence in block contained a match&#39;)
            reflen += 4
            available_bytes = min(refpos, reflen)
            q, r = divmod(reflen, available_bytes)
            with StreamDetour(output, -refpos, io.SEEK_CUR):
                match = output.read(available_bytes)
                match = q * match + match[:r]
                assert len(match) == reflen
                lastend = output.tell() - available_bytes + r
            output.write(match)

    def process(self, data):
        output = io.BytesIO()
        reader = LZ4Reader(memoryview(data))
        try:
            magic = reader.u32() == 0x184D2204
        except EOFError:
            magic = False
        if not magic:
            reader.seek(0)
            self._read_block(reader, output)
            return output.getvalue()

        (v1, v2, blocks_independent, blocks_checksummed,
            content_size, content_checksummed, rsrv1, dict_id) = reader.read_bits(8)
        rsrv2 = reader.read_nibble()
        try:
            block_maximum = {
                7: 0x400000,
                6: 0x100000,
                5: 0x040000,
                4: 0x010000,
            }[reader.read_integer(3)]
        except KeyError:
            raise ValueError(&#39;unknown maximum block size value in LZ4 frame header&#39;)
        rsrv3 = reader.read_bit()
        if any((rsrv1, rsrv2, rsrv3)):
            self.log_warn(&#39;nonzero reserved value in LZ4 frame header&#39;)
        if (v1, v2) != (0, 1):
            self.log_warn(F&#39;invalid version ({v1},{v2}) in LZ4 frame header&#39;)
        content_size = content_size and reader.u64() or None
        dict_id = dict_id and reader.u32() or None
        # Header Checksum
        xxh = xxhash(data[4:reader.tell()]).intdigest() &gt;&gt; 8 &amp; 0xFF
        chk = reader.read_byte()
        if chk != xxh:
            self.log_warn(F&#39;header checksum {chk:02X} does not match computed value {xxh:02X}&#39;)

        self.log_debug(lambda: F&#39;dictionary id: {dict_id}&#39;)
        self.log_debug(lambda: F&#39;block max: 0x{block_maximum:X}&#39;)
        if content_size is not None:
            self.log_debug(lambda: F&#39;chunk max: 0x{content_size:X}&#39;)
        self.log_debug(lambda: F&#39;blocks independent: {bool(blocks_independent)}&#39;)
        self.log_debug(lambda: F&#39;blocks checksummed: {bool(blocks_checksummed)}&#39;)

        blockindex = 0

        while True:
            blockindex += 1
            size = reader.read_integer(31)
            uncompressed = reader.read_bit()
            if not size:
                assert not uncompressed
                break
            self.log_info(F&#39;reading block of size 0x{size:06X}&#39;)
            assert reader.byte_aligned
            assert size &lt;= block_maximum, &#39;block size exceeds maximum size&#39;
            if uncompressed:
                output.write(reader.read_exactly(size))
            else:
                self._read_block(reader, output, size)
            if blocks_checksummed:
                with StreamDetour(reader, -size, io.SEEK_CUR):
                    xxh = xxhash(reader.read_exactly(size)).intdigest()
                chk = reader.u32()
                if chk != xxh:
                    self.log_warn(F&#39;block {blockindex} had checksum {chk:08X} which did not match computed value {xxh:08X}&#39;)
        value = output.getvalue()
        if content_checksummed:
            self.log_info(&#39;computing checksum&#39;)
            xxh = xxhash(value).intdigest()
            chk = reader.u32()
            if chk != xxh:
                self.log_warn(F&#39;the given checksum {chk:08X} did not match the computed checksum {xxh:08X}&#39;)
        if not reader.eof:
            pos = reader.tell()
            self.log_warn(F&#39;found {len(data) - pos} additional bytes starting at position 0x{pos:X} after compressed data&#39;)
        return value</code></pre>
</details>
</dd>
<dt id="refinery.shell.lzf"><code class="flex name class">
<span>class <span class="ident">lzf</span></span>
<span>(</span><span>fast=False)</span>
</code></dt>
<dd>
<section class="desc"><p>This unit implements LZF compression and decompression.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/lzf.py#L37-L203" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class lzf(Unit):
    &#34;&#34;&#34;
    This unit implements LZF compression and decompression.
    &#34;&#34;&#34;

    def __init__(self, fast: Param[bool, Arg.Switch(&#39;-x&#39;, help=&#39;Enable fast compression mode.&#39;)] = False):
        super().__init__(fast=fast)

    def reverse(self, data):
        def FRST(p: memoryview) -&gt; int:
            return ((p[0]) &lt;&lt; 8) | p[1]

        def NEXT(v: int, p: memoryview) -&gt; int:
            return ((v &lt;&lt; 8) | p[2]) &amp; 0xFFFFFFFF

        def DELTA(p: memoryview):
            return view.nbytes - p.nbytes

        if self.args.fast:
            def HIDX(h: int) -&gt; int:
                return (((h &gt;&gt; (3 * 8 - _HSLOG)) - h * 5) &amp; (_HSIZE - 1))
        else:
            def HIDX(h: int) -&gt; int:
                q = (h ^ (h &lt;&lt; 5))
                return (((q &gt;&gt; (3 * 8 - _HSLOG)) - h * 5) &amp; (_HSIZE - 1))

        if not data:
            return data

        ip = view = memoryview(data)
        op = bytearray()

        if len(data) == 1:
            op.append(0)
            op.extend(data)
            return op

        hval = FRST(ip)
        htab = [0] * _HSIZE
        fast = 1 if self.args.fast else 0

        lit = 0

        def begin_literal():
            nonlocal lit
            op.append(0)
            lit = 0

        def advance_literal():
            nonlocal lit, ip
            lit += 1
            op.append(ip[0])
            ip = ip[1:]
            if lit == _MAX_LIT:
                op[-lit - 1] = lit - 1
                begin_literal()

        def commit_literal():
            if lit &gt; 0:
                op[-lit - 1] = lit - 1
            else:
                op.pop()

        begin_literal()

        while ip.nbytes &gt; 2:
            hval = NEXT(hval, ip)
            hpos = HIDX(hval)
            ipos = DELTA(ip)
            length = 2
            r, htab[hpos] = htab[hpos], ipos
            off = ipos - r - 1
            ref = view[r:]

            if off &gt;= _MAX_OFF or r &lt;= 0 or ref[:3] != ip[:3]:
                advance_literal()
                continue
            else:
                commit_literal()

            maxlen = min(_MAX_REF, ip.nbytes - length)

            while True:
                length += 1
                if length &gt;= maxlen or ref[length] != ip[length]:
                    length -= 2
                    break

            if length &lt; 7:
                op.append((off &gt;&gt; 8) + (length &lt;&lt; 5))
            else:
                op.append((off &gt;&gt; 8) + (7 &lt;&lt; 5))
                op.append(length - 7)

            op.append(off &amp; 0xFF)
            begin_literal()

            if ip.nbytes &lt;= length + 3:
                ip = ip[length + 2:]
                break
            if fast:
                ip = ip[length:]
                hval = FRST(ip)
                for _ in range(2):
                    hval = NEXT(hval, ip)
                    htab[HIDX(hval)] = DELTA(ip)
                    ip = ip[1:]
            else:
                ip = ip[1:]
                for _ in range(length + 1):
                    hval = NEXT(hval, ip)
                    htab[HIDX(hval)] = DELTA(ip)
                    ip = ip[1:]
        while ip.nbytes:
            advance_literal()
        commit_literal()
        return op

    def _decompress_chunk(self, data: memoryview, out: MemoryFile):
        ip = StructReader(data)
        while not ip.eof:
            ctrl = ip.u8()
            if ctrl &lt; 0B100000:
                ctrl += 1
                out.write(ip.read_exactly(ctrl))
            else:
                length = ctrl &gt;&gt; 5
                offset = 1 + ((ctrl &amp; 0B11111) &lt;&lt; 8)
                if length == 7:
                    length += ip.u8()
                offset += ip.u8()
                length += 2
                out.replay(offset, length)

    def process(self, data):
        mem = memoryview(data)
        out = MemoryFile()

        try:
            reader = StructReader(mem)
            header = LZFHeader(reader)
        except Exception:
            self.log_info(&#39;no header detected, decompressing as raw stream&#39;)
            self._decompress_chunk(mem, out)
            return out.getvalue()

        for k in itertools.count(1):
            self.log_info(F&#39;chunk: e=0x{header.encoded_size:04X} d=0x{header.decoded_size:04X}&#39;)
            chunk = reader.read(header.encoded_size)
            if header.compressed:
                self._decompress_chunk(chunk, out)
            else:
                out.write(chunk)
            if reader.eof:
                break
            try:
                header = LZFHeader(reader)
            except Exception as E:
                msg = F&#39;failed parsing next header after {k} chunks: {E!s}&#39;
                raise RefineryPartialResult(msg, out.getvalue())

        return out.getvalue()

    @classmethod
    def handles(cls, data):
        if data[:2] == LZFHeader.MAGIC:
            return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.lzg"><code class="flex name class">
<span>class <span class="ident">lzg</span></span>
</code></dt>
<dd>
<section class="desc"><p>LZG decompression.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/lzg.py#L177-L192" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class lzg(Unit):
    &#34;&#34;&#34;
    LZG decompression.
    &#34;&#34;&#34;
    def process(self, data: bytearray):
        stream = LZGStream(data)
        out = stream.decompress()
        if len(out) != stream.decoded_size:
            msg = F&#39;LZG header announced {stream.decoded_size} bytes, but decompressed buffer had size {len(out)}.&#39;
            raise RefineryPartialResult(msg, out)
        return out

    @classmethod
    def handles(cls, data):
        if data[:3] == B&#39;LZG&#39;:
            return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.lzip"><code class="flex name class">
<span>class <span class="ident">lzip</span></span>
</code></dt>
<dd>
<section class="desc"><p>LZIP decompression</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/lzip.py#L327-L374" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class lzip(Unit):
    &#34;&#34;&#34;
    LZIP decompression
    &#34;&#34;&#34;
    def process(self, data: bytearray):
        view = memoryview(data)
        with MemoryFile() as output, StructReader(view) as reader:
            for k in count(1):
                if reader.eof:
                    break
                trailing_size = len(data) - reader.tell()
                try:
                    ID, VN, DS = reader.read_struct(&#39;4sBB&#39;)
                    if ID != B&#39;LZIP&#39;:
                        if k &gt; 1:
                            raise EOF
                        else:
                            self.log_warn(F&#39;ignoring invalid LZIP signature: {ID.hex()}&#39;)
                    if VN != 1:
                        self.log_warn(F&#39;ignoring invalid LZIP version: {VN}&#39;)
                    dict_size = 1 &lt;&lt; (DS &amp; 0x1F)
                    dict_size -= (dict_size // 16) * ((DS &gt;&gt; 5) &amp; 7)
                    if dict_size not in range(_MIN_DICT_SIZE, _MAX_DICT_SIZE + 1):
                        raise ValueError(
                            F&#39;The dictionary size {dict_size} is out of the valid range &#39;
                            F&#39;[{_MIN_DICT_SIZE}, {_MAX_DICT_SIZE}]; unable to proceed.&#39;
                        )
                    decoder = MemberDecoder(dict_size, reader, output)
                    if not decoder():
                        raise ValueError(F&#39;Data error in stream {k}.&#39;)
                    crc32, data_size, member_size = reader.read_struct(&#39;&lt;LQQ&#39;)
                    if crc32 != decoder.crc32:
                        self.log_warn(F&#39;checksum in stream {k} was {decoder.crc:08X}, should have been {crc32:08X}.&#39;)
                    if member_size - 20 != decoder.member_position:
                        self.log_warn(F&#39;member size in stream {k} was {decoder.member_position}, should have been {member_size}.&#39;)
                    if data_size != decoder.data_position:
                        self.log_warn(F&#39;data size in stream {k} was {decoder.data_position}, should have been {data_size}.&#39;)
                except EOFError:
                    if k &lt;= 1:
                        raise
                    self.log_info(F&#39;silently ignoring {trailing_size} bytes of trailing data&#39;)
                    break

            return output.getvalue()

    @classmethod
    def handles(cls, data):
        return data[:4] == B&#39;LZIP&#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.lzjb"><code class="flex name class">
<span>class <span class="ident">lzjb</span></span>
</code></dt>
<dd>
<section class="desc"><p>LZJB compression and decompression. This LZ-type compression is used in the ZFS file system.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/lzjb.py#L14-L79" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class lzjb(Unit):
    &#34;&#34;&#34;
    LZJB compression and decompression. This LZ-type compression is used in the ZFS file system.
    &#34;&#34;&#34;
    def reverse(self, src):
        # https://web.archive.org/web/20100807223517/ ..
        # .. http://cvs.opensolaris.org/source/xref/onnv/onnv-gate/usr/src/uts/common/fs/zfs/lzjb.c
        output = bytearray()
        lempel = [0] * _LEMPEL_SIZE
        copymask = 0x80
        position = 0
        while position &lt; len(src):
            copymask &lt;&lt;= 1
            if copymask &gt;= 0x100:
                copymask = 1
                copymap = len(output)
                output.append(0)
            if position &gt; len(src) - _MATCH_MAX:
                output.append(src[position])
                position += 1
                continue
            hsh = (src[position] &lt;&lt; 16) + (src[position + 1] &lt;&lt; 8) + src[position + 2]
            hsh += hsh &gt;&gt; 9
            hsh += hsh &gt;&gt; 5
            hsh %= len(lempel)
            offset = (position - lempel[hsh]) &amp; _OFFSET_MASK
            lempel[hsh] = position
            cpy = position - offset
            if cpy &gt;= 0 and cpy != position and src[position:position + 3] == src[cpy:cpy + 3]:
                output[copymap] |= copymask
                for mlen in range(_MATCH_MIN, min(len(src) - position, _MATCH_MAX)):
                    if src[position + mlen] != src[cpy + mlen]:
                        break
                output.append(((mlen - _MATCH_MIN) &lt;&lt; (8 - _MATCH_LEN)) | (offset &gt;&gt; 8))
                output.append(offset &amp; 255)
                position += mlen
            else:
                output.append(src[position])
                position += 1
        return output

    def process(self, data):
        dst = bytearray()
        src = StructReader(data)
        while not src.eof:
            copy = src.read_byte()
            for mask in (0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80):
                if src.eof:
                    break
                if not copy &amp; mask:
                    dst.append(src.read_byte())
                    continue
                elif not dst:
                    raise ValueError(&#39;copy requested against empty buffer&#39;)
                with src.be:
                    match_len = src.read_integer(6) + _MATCH_MIN
                    match_pos = src.read_integer(10)
                if not match_pos or match_pos &gt; len(dst):
                    raise RuntimeError(F&#39;invalid match offset at position {src.tell()}&#39;)
                match_pos = len(dst) - match_pos
                while match_len &gt; 0:
                    match = dst[match_pos:match_pos + match_len]
                    dst.extend(match)
                    match_pos += len(match)
                    match_len -= len(match)
        return dst</code></pre>
</details>
</dd>
<dt id="refinery.shell.lzma"><code class="flex name class">
<span>class <span class="ident">lzma</span></span>
<span>(</span><span>raw=False, alone=False, xz=False, level=9, delta=0)</span>
</code></dt>
<dd>
<section class="desc"><p>LZMA compression and decompression.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/lz.py#L32-L166" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class lzma(Unit):
    &#34;&#34;&#34;
    LZMA compression and decompression.
    &#34;&#34;&#34;

    _SEARCH_MIN_DICT = 0x1_0000
    _SEARCH_MAX_DICT = 0x1000_0000
    _SEARCH_MAX_BLOW = 1.2
    _SEARCH_SKIP1 = 0x08
    _SEARCH_SKIP2 = 0x10
    _ATTEMPT_PARTIAL = True

    def __init__(
        self,
        raw: Param[bool, Arg.Switch(&#39;-r&#39;, group=&#39;MODE&#39;, help=&#39;Use raw (no container) format.&#39;)] = False,
        alone: Param[bool, Arg.Switch(&#39;-a&#39;, group=&#39;MODE&#39;, help=&#39;Use the lzma container format.&#39;)] = False,
        xz: Param[bool, Arg.Switch(&#39;-x&#39;, group=&#39;MODE&#39;, help=&#39;Use the default xz format.&#39;)] = False,
        level: Param[int, Arg.Number(&#39;-l&#39;, bound=(0, 9), help=&#39;The compression level preset; between 0 and 9.&#39;)] = 9,
        delta: Param[int, Arg.Number(&#39;-d&#39;, help=&#39;Add a delta filter when compressing.&#39;)] = 0,
    ):
        if (raw, alone, xz).count(True) &gt; 1:
            raise ValueError(&#39;Only one container format can be enabled.&#39;)
        if level not in range(10):
            raise ValueError(&#39;Compression level must be a number between 0 and 9.&#39;)
        super().__init__(filter=filter, raw=raw, alone=alone, xz=xz, delta=delta,
            level=level | PRESET_EXTREME)

    def reverse(self, data):
        filters = []
        if self.args.delta &gt; 0:
            self.log_debug(&#39;adding delta filter&#39;)
            filters.append({&#39;id&#39;: FILTER_DELTA, &#39;dist&#39;: self.args.delta})
        if self.args.alone:
            self.log_debug(&#39;setting alone format&#39;)
            mode = FORMAT_ALONE
            filters.append({&#39;id&#39;: FILTER_LZMA1, &#39;preset&#39;: self.args.level})
        elif self.args.raw:
            self.log_debug(&#39;setting raw format&#39;)
            mode = FORMAT_RAW
            filters.append({&#39;id&#39;: FILTER_LZMA2, &#39;preset&#39;: self.args.level})
        else:
            if not self.args.xz:
                self.log_info(&#39;choosing default .xz container format for compression&#39;)
            mode = FORMAT_XZ
            filters.append({&#39;id&#39;: FILTER_LZMA2, &#39;preset&#39;: self.args.level})
        lz = LZMACompressor(mode, filters=filters)
        output = lz.compress(data)
        output += lz.flush()
        return output

    def _decompress(self, data: bytearray, lz: LZMADecompressor, partial: bool = False):
        temp = bytearray()
        sizes = repeat(1) if partial else [len(data)]
        with MemoryFile(temp) as output:
            with MemoryFile(data) as stream:
                for size in sizes:
                    if stream.eof or stream.closed:
                        break
                    try:
                        offset = stream.tell()
                        output.write(lz.decompress(stream.read(size)))
                    except (EOFError, LZMAError):
                        raise RefineryPartialResult(
                            F&#39;compression failed at offset {offset}&#39;, temp)
        if n := len(lz.unused_data):
            raise RefineryPartialResult(F&#39;Data stream is truncated, {n} bytes unused.&#39;, temp)
        return temp

    def _process(self, data: bytearray, partial=False):
        try:
            dc = LZMADecompressor()
            return self._decompress(data, dc, partial)
        except RefineryPartialResult as pe:
            best = pe
        except Exception:
            best = None
            self.log_info(&#39;default LZMA decompressor failed, brute-forcing custom header&#39;)
        view = memoryview(data)
        min_original_size = {
            # https://sourceforge.net/p/sevenzip/discussion/45797/thread/b6bd62f8/
            1: int((len(data) - 64_000) / 1.100), # noqa
            2: int((len(data) -  1_000) / 1.001), # noqa
        }
        for (version, p), offset_prop, to_data in product(
            ((1, 5),
             (2, 1)),
            range(self._SEARCH_SKIP1 + 1),
            range(self._SEARCH_SKIP2 + 1),
        ):
            if offset_prop + to_data &gt; p + 20:
                # expect no more than a 20 byte header on top of the properties
                # that would be enough for, e.g. compressed &amp; uncompressed size
                # each filling a full 64bit integer and 4 additional bytes.
                continue
            try:
                filter = parse_lzma_properties(
                    view[offset_prop:offset_prop + p],
                    version,
                    min_dict=self._SEARCH_MIN_DICT,
                    max_dict=self._SEARCH_MAX_DICT,
                )
                self.log_debug(F&#39;attempt LZMA{version} at {offset_prop:02d}, skipping {to_data:02d}, filter: {filter!r}&#39;)
                engine = LZMADecompressor(FORMAT_RAW, filters=[filter])
                result = self._decompress(view[offset_prop + p + to_data:], engine, partial)
            except RefineryPartialResult as pe:
                if best is None:
                    best = pe
                elif len(best.partial) &lt; len(pe.partial):
                    best = pe
                continue
            except Exception:
                continue
            if len(result) &lt; min_original_size[version]:
                continue
            if len(result) * self._SEARCH_MAX_BLOW &lt; len(data):
                continue
            self.log_info(
                F&#39;success with LZMA{version} properties at {offset_prop} and raw stream starting at {to_data + offset_prop + p}&#39;)
            return result
        if partial or not self._ATTEMPT_PARTIAL:
            if best and len(best.partial) &gt; 0:
                raise best
            raise ValueError(&#39;unable to find an LZMA stream&#39;)

    def process(self, data: bytearray):
        if out := self._process(data):
            return out
        return self._process(data, partial=True)

    @classmethod
    def handles(cls, data):
        if data[:4] == B&#39;\x5D\0\0\0&#39;:
            return True
        if data[:5] == B&#39;\xFD7zXZ&#39;:
            return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.lznt1"><code class="flex name class">
<span>class <span class="ident">lznt1</span></span>
<span>(</span><span>chunk_size=4096)</span>
</code></dt>
<dd>
<section class="desc"><p>LZNT1 compression and decompression. This compression algorithm is expected
by the Win32 API routine <code>RtlDecompressBuffer</code>, for example.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/lznt1.py#L11-L154" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class lznt1(Unit):
    &#34;&#34;&#34;
    LZNT1 compression and decompression. This compression algorithm is expected
    by the Win32 API routine `RtlDecompressBuffer`, for example.
    &#34;&#34;&#34;

    def _decompress_chunk(self, chunk):
        out = B&#39;&#39;
        while chunk:
            flags = chunk[0]
            chunk = chunk[1:]
            for i in range(8):
                if not (flags &gt;&gt; i &amp; 1):
                    out += chunk[:1]
                    chunk = chunk[1:]
                else:
                    flag = struct.unpack(&#39;&lt;H&#39;, chunk[:2])[0]
                    pos = len(out) - 1
                    l_mask = 0xFFF
                    o_shift = 12
                    while pos &gt;= 0x10:
                        l_mask &gt;&gt;= 1
                        o_shift -= 1
                        pos &gt;&gt;= 1
                    length = (flag &amp; l_mask) + 3
                    offset = (flag &gt;&gt; o_shift) + 1
                    if length &gt;= offset:
                        tmp = out[-offset:] * (0xFFF // len(out[-offset:]) + 1)
                        out += tmp[:length]
                    else:
                        out += out[-offset:length - offset]
                    chunk = chunk[2:]
                if len(chunk) == 0:
                    break
        return out

    def _find(self, src, target, max_len):
        result_offset = 0
        result_length = 0
        for i in range(1, max_len):
            offset = src.rfind(target[:i])
            if offset == -1:
                break
            tmp_offset = len(src) - offset
            tmp_length = i
            if tmp_offset == tmp_length:
                tmp = src[offset:] * (0xFFF // len(src[offset:]) + 1)
                for j in range(i, max_len + 1):
                    offset = tmp.rfind(target[:j])
                    if offset == -1:
                        break
                    tmp_length = j
            if tmp_length &gt; result_length:
                result_offset = tmp_offset
                result_length = tmp_length
        if result_length &lt; 3:
            return 0, 0
        return result_offset, result_length

    def _compress_chunk(self, chunk):
        blob = copy.copy(chunk)
        out = B&#39;&#39;
        pow2 = 0x10
        l_mask3 = 0x1002
        o_shift = 12
        while len(blob) &gt; 0:
            bits = 0
            tmp = B&#39;&#39;
            for i in range(8):
                bits &gt;&gt;= 1
                while pow2 &lt; (len(chunk) - len(blob)):
                    pow2 &lt;&lt;= 1
                    l_mask3 = (l_mask3 &gt;&gt; 1) + 1
                    o_shift -= 1
                if len(blob) &lt; l_mask3:
                    max_len = len(blob)
                else:
                    max_len = l_mask3
                offset1, length1 = self._find(
                    chunk[:len(chunk) - len(blob)], blob, max_len)
                # try to find more compressed pattern
                offset2, length2 = self._find(
                    chunk[:len(chunk) - len(blob) + 1], blob[1:], max_len)
                if length1 &lt; length2:
                    length1 = 0
                if length1 &gt; 0:
                    symbol = ((offset1 - 1) &lt;&lt; o_shift) | (length1 - 3)
                    tmp += struct.pack(&#39;&lt;H&#39;, symbol)
                    bits |= 0x80  # set the highest bit
                    blob = blob[length1:]
                else:
                    tmp += blob[:1]
                    blob = blob[1:]
                if len(blob) == 0:
                    break
            out += struct.pack(&#39;B&#39;, bits &gt;&gt; (7 - i))
            out += tmp
        return out

    def reverse(self, buf):
        out = B&#39;&#39;
        while buf:
            chunk = buf[:self.args.chunk_size]
            compressed = self._compress_chunk(chunk)
            if len(compressed) &lt; len(chunk):  # chunk is compressed
                flags = 0xB000
                header = struct.pack(&#39;&lt;H&#39;, flags | (len(compressed) - 1))
                out += header + compressed
            else:
                flags = 0x3000
                header = struct.pack(&#39;&lt;H&#39;, flags | (len(chunk) - 1))
                out += header + chunk
            buf = buf[self.args.chunk_size:]
        return out

    def process(self, data):
        out = io.BytesIO()
        offset = 0
        while offset &lt; len(data):
            try:
                header, = struct.unpack(&#39;&lt;H&#39;, data[offset:offset + 2])
            except struct.error as err:
                raise RefineryPartialResult(str(err), partial=out.getvalue())
            offset += 2
            size = (header &amp; 0xFFF) + 1
            if size + 1 &gt;= len(data):
                raise RefineryPartialResult(
                    F&#39;chunk header indicates size {size}, but only {len(data)} bytes remain.&#39;,
                    partial=out.getvalue()
                )
            chunk = data[offset:offset + size]
            offset += size
            if header &amp; 0x8000:
                chunk = self._decompress_chunk(chunk)
            out.write(chunk)
        return out.getvalue()

    def __init__(
        self,
        chunk_size: Param[int, Arg.Number(&#39;-c&#39;, help=(
            &#39;Optionally specify the chunk size for compression, default is 0x1000.&#39;)
        )] = 0x1000
    ):
        super().__init__(chunk_size=chunk_size)</code></pre>
</details>
</dd>
<dt id="refinery.shell.lzo"><code class="flex name class">
<span>class <span class="ident">lzo</span></span>
</code></dt>
<dd>
<section class="desc"><p>LZO decompression. The code works against simple test cases, but it is known to fail for
certain outputs produced by the lzop command-line tool when high compression ratio is
favoured (i.e. when the -9 switch is used).</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/lzo.py#L148-L282" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class lzo(Unit):
    &#34;&#34;&#34;
    LZO decompression. The code works against simple test cases, but it is known to fail for
    certain outputs produced by the lzop command-line tool when high compression ratio is
    favoured (i.e. when the -9 switch is used).
    &#34;&#34;&#34;
    def decompress_stream(self, data: buf, LZOv1: bool = False) -&gt; bytearray:
        &#34;&#34;&#34;
        An implementation of LZO decompression. We use the article
        &#34;[LZO stream format as understood by Linux&#39;s LZO decompressor](https://www.kernel.org/doc/html/latest/staging/lzo.html)&#34;
        as a reference since no proper specification is available.
        &#34;&#34;&#34;
        def integer() -&gt; int:
            length = 0
            while True:
                byte = src.read_byte()
                if byte:
                    return length + byte
                length += 0xFF
                if length &gt; 0x100000:
                    raise LZOError(&#39;Too many zeros in integer encoding.&#39;)

        def literal(count):
            dst.write(src.read_bytes(count))

        def copy(distance: int, length: int):
            if distance &gt; len(dst):
                raise LZOError(F&#39;Distance {distance} &gt; bufsize {len(dst)}&#39;)
            buffer = dst.getvalue()
            if distance &gt; length:
                start = len(buffer) - distance
                end = start + length
                dst.write(buffer[start:end])
            else:
                block = buffer[-distance:]
                while len(block) &lt; length:
                    block += block[:length - len(block)]
                if len(block) &gt; length:
                    block[length:] = ()
                dst.write(block)

        src = StructReader(memoryview(data))
        dst = MemoryFile()

        state = 0
        first = src.read_byte()

        if first == 0x10:
            raise LZOError(&#39;Invalid first stream byte 0x10.&#39;)
        elif first &lt;= 0x12:
            src.seekrel(-1)
        elif first &lt;= 0x15:
            state = first - 0x11
            literal(state)
        else:
            state = 4
            literal(first - 0x11)

        while True:
            instruction = src.read_byte()
            if instruction &lt; 0x10:
                if state == 0:
                    length = instruction or integer() + 15
                    state = length + 3
                    if state &lt; 4:
                        raise LZOError(&#39;Literal encoding is too short.&#39;)
                else:
                    state = instruction &amp; 0b0011
                    D = (instruction &amp; 0b1100) &gt;&gt; 2
                    H = src.read_byte()
                    distance = (H &lt;&lt; 2) + D + 1
                    if state &gt;= 4:
                        distance += 0x800
                        length = 3
                    else:
                        length = 2
                    copy(distance, length)
            elif instruction &lt; 0x20:
                L = instruction &amp; 0b0111
                H = instruction &amp; 0b1000
                length = L or integer() + 7
                argument = src.u16()
                state = argument &amp; 3
                distance = (H &lt;&lt; 11) + (argument &gt;&gt; 2)
                if not distance:
                    return dst.getvalue()
                if LZOv1 and distance &amp; 0x803F == 0x803F and length in range(261, 265):
                    raise LZOError(&#39;Compressed data contains sequence that is banned in LZOv1.&#39;)
                if LZOv1 and distance == 0xBFFF:
                    X = src.read_byte()
                    count = ((X &lt;&lt; 3) | L) + 4
                    self.log_debug(F&#39;Writing run of {X} zero bytes according to LZOv1.&#39;)
                    dst.write(B&#39;\0&#39; * count)
                else:
                    copy(distance + 0x4000, length + 2)
            elif instruction &lt; 0x40:
                L = instruction &amp; 0b11111
                length = L or integer() + 31
                argument = src.u16()
                state = argument &amp; 3
                distance = (argument &gt;&gt; 2) + 1
                copy(distance, length + 2)
            else:
                if instruction &lt; 0x80:
                    length = 3 + ((instruction &gt;&gt; 5) &amp; 1)
                else:
                    length = 5 + ((instruction &gt;&gt; 5) &amp; 3)
                H = src.read_byte()
                D = (instruction &amp; 0b11100) &gt;&gt; 2
                state = instruction &amp; 3
                distance = (H &lt;&lt; 3) + D + 1
                copy(distance, length)
            if state:
                literal(state)

    def process(self, data):
        try:
            lzo = LZO(data)
        except LZOError:
            self.log_info(&#39;Not an LZO archive, processing raw stream.&#39;)
            return self.decompress_stream(data)
        with MemoryFile() as output:
            for k, chunk in enumerate(lzo, 1):
                self.log_debug(F&#39;decompressing chunk {k}&#39;)
                output.write(self.decompress_stream(chunk.data))
            return self.labelled(
                output.getvalue(),
                path=lzo.name,
                date=date_from_timestamp(lzo.mtime)
            )

    @classmethod
    def handles(cls, data) -&gt; bool | None:
        if data[:len(LZO.SIGNATURE)] == LZO.SIGNATURE:
            return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.lzw"><code class="flex name class">
<span>class <span class="ident">lzw</span></span>
</code></dt>
<dd>
<section class="desc"><p>LZW decompression based on ancient Unix sources.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/lzw.py#L21-L140" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class lzw(Unit):
    &#39;&#39;&#39;
    LZW decompression based on ancient Unix sources.
    &#39;&#39;&#39;

    _MAGIC = B&#39;\x1F\x9D&#39;

    def process(self, data: bytearray):
        out = MemoryFile()
        inf = StructReader(memoryview(data))

        if inf.peek(2) != self._MAGIC:
            self.log_info(&#39;No LZW signature found, assuming raw stream.&#39;)
            maxbits = LZW.BITS
            block_mode = True
        else:
            inf.seekrel(2)
            maxbits = inf.read_integer(5)
            if inf.read_integer(2) != 0:
                self.log_info(&#39;reserved bits were set in LZW header&#39;)
            block_mode = bool(inf.read_bit())

        if maxbits &gt; LZW.BITS:
            raise ValueError(F&#39;Compressed with {maxbits} bits; cannot handle file.&#39;)

        maxmaxcode = 1 &lt;&lt; maxbits

        ibuf = inf.read()

        tab_suffix = bytearray(LZW.WSIZE * 2)
        tab_prefix = array(&#39;H&#39;, itertools.repeat(0, 1 &lt;&lt; LZW.BITS))

        n_bits = LZW.INIT_BITS
        maxcode = (1 &lt;&lt; n_bits) - 1
        bitmask = (1 &lt;&lt; n_bits) - 1
        oldcode = ~0
        finchar = +0
        posbits = +0

        free_entry = LZW.FIRST if block_mode else 0x100
        tab_suffix[:0x100] = range(0x100)
        resetbuf = True

        while resetbuf:
            resetbuf = False

            ibuf = ibuf[posbits &gt;&gt; 3:]
            insize = len(ibuf)
            posbits = 0
            inbits = (insize &lt;&lt; 3) - (n_bits - 1)

            while inbits &gt; posbits:
                if free_entry &gt; maxcode:
                    n = n_bits &lt;&lt; 3
                    p = posbits - 1
                    posbits = p + (n - (p + n) % n)
                    n_bits += 1
                    if (n_bits == maxbits):
                        maxcode = maxmaxcode
                    else:
                        maxcode = (1 &lt;&lt; n_bits) - 1
                    bitmask = (1 &lt;&lt; n_bits) - 1
                    resetbuf = True
                    break

                p = ibuf[posbits &gt;&gt; 3:]
                code = int.from_bytes(p[:3], &#39;little&#39;) &gt;&gt; (posbits &amp; 7) &amp; bitmask
                posbits += n_bits

                if oldcode == -1:
                    if code &gt;= 256:
                        raise ValueError(&#39;corrupt input.&#39;)
                    oldcode = code
                    finchar = oldcode
                    out.write_byte(finchar)
                    continue

                if code == LZW.CLEAR and block_mode:
                    tab_prefix[:0x100] = array(&#39;H&#39;, itertools.repeat(0, 0x100))
                    free_entry = LZW.FIRST - 1
                    n = n_bits &lt;&lt; 3
                    p = posbits - 1
                    posbits = p + (n - (p + n) % n)
                    n_bits = LZW.INIT_BITS
                    maxcode = (1 &lt;&lt; n_bits) - 1
                    bitmask = (1 &lt;&lt; n_bits) - 1
                    resetbuf = True
                    break

                incode = code
                stack = bytearray()

                if code &gt;= free_entry:
                    if code &gt; free_entry:
                        raise RefineryPartialResult(&#39;corrupt input.&#39;, out.getvalue())
                    stack.append(finchar)
                    code = oldcode
                while code &gt;= 256:
                    stack.append(tab_suffix[code])
                    code = tab_prefix[code]

                finchar = tab_suffix[code]
                stack.append(finchar)
                stack.reverse()
                out.write(stack)
                code = free_entry

                if code &lt; maxmaxcode:
                    tab_prefix[code] = oldcode &amp; 0xFFFF
                    tab_suffix[code] = finchar &amp; 0x00FF
                    free_entry = code + 1

                oldcode = incode

        return out.getvalue()

    @classmethod
    def handles(cls, data) -&gt; bool | None:
        if data[:len(cls._MAGIC)] == cls._MAGIC:
            return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.lzx"><code class="flex name class">
<span>class <span class="ident">lzx</span></span>
<span>(</span><span>window=15, wim=False)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/lzx.py#L8-L26" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class lzx(Unit):

    def __init__(
        self,
        window: Param[int, Arg(help=&#39;Optionally specify the window size; the default is {default}.&#39;)] = 15,
        wim: Param[bool, Arg(&#39;-w&#39;, help=&#39;Use the WIM flavor of LZX.&#39;)] = False,
    ):
        super().__init__(window=window, wim=wim)

    def process(self, data):
        lzx = LzxDecoder(self.args.wim)
        lzx.set_params_and_alloc(self.args.window)

        try:
            return lzx.decompress(memoryview(data))
        except Exception as E:
            if out := lzx.get_output_data():
                raise RefineryPartialResult(str(E), out) from E
            raise</code></pre>
</details>
</dd>
<dt id="refinery.shell.m2h"><code class="flex name class">
<span>class <span class="ident">m2h</span></span>
<span>(</span><span>seed=0, reps=1, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the 32bit Murmur Hash, Version 2.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/hash/murmur.py#L26-L31" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class m2h(MurMurHash):
    &#34;&#34;&#34;
    Returns the 32bit Murmur Hash, Version 2.
    &#34;&#34;&#34;
    def _algorithm(self, data) -&gt; bytes:
        return v2_mmh32digest(data, self.args.seed)</code></pre>
</details>
</dd>
<dt id="refinery.shell.m2h64a"><code class="flex name class">
<span>class <span class="ident">m2h64a</span></span>
<span>(</span><span>seed=0, reps=1, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the 64bit Murmur Hash, Version 2, Variant A.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/hash/murmur.py#L42-L47" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class m2h64a(MurMurHash):
    &#34;&#34;&#34;
    Returns the 64bit Murmur Hash, Version 2, Variant A.
    &#34;&#34;&#34;
    def _algorithm(self, data) -&gt; bytes:
        return v2_mmh64digestA(data, self.args.seed)</code></pre>
</details>
</dd>
<dt id="refinery.shell.m2h64b"><code class="flex name class">
<span>class <span class="ident">m2h64b</span></span>
<span>(</span><span>seed=0, reps=1, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the 64bit Murmur Hash, Version 2, Variant B.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/hash/murmur.py#L50-L55" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class m2h64b(MurMurHash):
    &#34;&#34;&#34;
    Returns the 64bit Murmur Hash, Version 2, Variant B.
    &#34;&#34;&#34;
    def _algorithm(self, data) -&gt; bytes:
        return v2_mmh64digestB(data, self.args.seed)</code></pre>
</details>
</dd>
<dt id="refinery.shell.m2ha"><code class="flex name class">
<span>class <span class="ident">m2ha</span></span>
<span>(</span><span>seed=0, reps=1, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the 32bit Murmur Hash, Version 2, Variant A.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/hash/murmur.py#L34-L39" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class m2ha(MurMurHash):
    &#34;&#34;&#34;
    Returns the 32bit Murmur Hash, Version 2, Variant A.
    &#34;&#34;&#34;
    def _algorithm(self, data) -&gt; bytes:
        return v2_mmh32digestA(data, self.args.seed)</code></pre>
</details>
</dd>
<dt id="refinery.shell.m3h"><code class="flex name class">
<span>class <span class="ident">m3h</span></span>
<span>(</span><span>seed=0, reps=1, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the 32bit Murmur Hash, Version 3.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/hash/murmur.py#L58-L63" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class m3h(MurMurHash):
    &#34;&#34;&#34;
    Returns the 32bit Murmur Hash, Version 3.
    &#34;&#34;&#34;
    def _algorithm(self, data) -&gt; bytes:
        return v3_mmh32digest(data, self.args.seed)</code></pre>
</details>
</dd>
<dt id="refinery.shell.m3h32"><code class="flex name class">
<span>class <span class="ident">m3h32</span></span>
<span>(</span><span>seed=0, reps=1, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the 128bit Murmur Hash, Version 3, 32bit digest size.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/hash/murmur.py#L74-L79" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class m3h32(MurMurHash):
    &#34;&#34;&#34;
    Returns the 128bit Murmur Hash, Version 3, 32bit digest size.
    &#34;&#34;&#34;
    def _algorithm(self, data) -&gt; bytes:
        return v3_mmh128digest32(data, self.args.seed)</code></pre>
</details>
</dd>
<dt id="refinery.shell.m3h64"><code class="flex name class">
<span>class <span class="ident">m3h64</span></span>
<span>(</span><span>seed=0, reps=1, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the 128bit Murmur Hash, Version 3, 64bit digest size.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/hash/murmur.py#L66-L71" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class m3h64(MurMurHash):
    &#34;&#34;&#34;
    Returns the 128bit Murmur Hash, Version 3, 64bit digest size.
    &#34;&#34;&#34;
    def _algorithm(self, data) -&gt; bytes:
        return v3_mmh128digest64(data, self.args.seed)</code></pre>
</details>
</dd>
<dt id="refinery.shell.machometa"><code class="flex name class">
<span>class <span class="ident">machometa</span></span>
<span>(</span><span>all=True, header=False, linked_images=False, signatures=False, version=False, load_commands=False, exports=False, imports=False, tabular=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract metadata from Mach-O files.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/macho/machometa.py#L137-L354" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class machometa(Unit):
    &#34;&#34;&#34;
    Extract metadata from Mach-O files.
    &#34;&#34;&#34;
    def __init__(
        self, all: Param[bool, Arg(&#39;-c&#39;, &#39;--custom&#39;,
            help=&#39;Unless enabled, all default categories will be extracted.&#39;)] = True,
        header: Param[bool, Arg(&#39;-H&#39;, help=&#39;Parse basic data from the Mach-O header.&#39;)] = False,
        linked_images: Param[bool, Arg(&#39;-K&#39;, help=&#39;Parse all library images linked by the Mach-O.&#39;)] = False,
        signatures: Param[bool, Arg(&#39;-S&#39;, help=&#39;Parse signature and entitlement information.&#39;)] = False,
        version: Param[bool, Arg(&#39;-V&#39;, help=&#39;Parse version information from the Mach-O load commands.&#39;)] = False,
        load_commands: Param[bool, Arg(&#39;-D&#39;, help=&#39;Parse load commands from the Mach-O header.&#39;)] = False,
        exports: Param[bool, Arg(&#39;-E&#39;, help=&#39;List all exported functions.&#39;)] = False,
        imports: Param[bool, Arg(&#39;-I&#39;, help=&#39;List all imported functions.&#39;)] = False,
        tabular: Param[bool, Arg(&#39;-t&#39;, help=&#39;Print information in a table rather than as JSON&#39;)] = False,
    ):
        super().__init__(
            header=all or header,
            linked_images=all or linked_images,
            version=all or version,
            signatures=all or signatures,
            load_commands=load_commands,
            imports=imports,
            exports=exports,
            tabular=tabular,
        )

    def compute_symhash(self, macho: lief.MachO.Binary) -&gt; dict:
        def _symbols(symbols: Iterable[lief.MachO.Symbol]):
            for sym in symbols:
                if sym.category != lief.MachO.Symbol.CATEGORY.UNDEFINED:
                    continue
                yield lief.string(sym.name)
        symbols = sorted(set(_symbols(macho.symbols)))
        symbols: str = &#39;,&#39;.join(symbols)
        return md5(symbols.encode(&#39;utf8&#39;)).hexdigest()

    def parse_macho_header(self, macho: lief.MachO.Binary, data=None) -&gt; dict:
        info = {}
        if header := macho.header:
            st = header.cpu_subtype &amp; 0x7FFFFFFF
            ht = &#39;mach_header_64&#39; if header.magic in {
                lief.MachO.MACHO_TYPES.CIGAM_64,
                lief.MachO.MACHO_TYPES.MAGIC_64,
            } else &#39;mach_header&#39;
            info[&#39;Type&#39;] = ht
            info[&#39;Magic&#39;] = header.magic.value
            info[&#39;CPUType&#39;] = header.cpu_type.__name__.upper()
            info[&#39;CPUSubType&#39;] = _CPU_SUBTYPES.get(header.cpu_type, {}).get(st, st)
            info[&#39;FileType&#39;] = header.file_type.__name__
            info[&#39;LoadCount&#39;] = header.nb_cmds
            info[&#39;LoadSize&#39;] = header.sizeof_cmds
            info[&#39;Flags&#39;] = sorted(flag.__name__ for flag in header.flags_list)
            info[&#39;Reserved&#39;] = header.reserved
        return info

    def parse_linked_images(self, macho: lief.MachO.Binary, data=None) -&gt; dict:
        load_command_images = {}
        load_commands: Iterable[lief.MachO.LoadCommand] = macho.commands
        for load_command in load_commands:
            if not isinstance(load_command, lief.MachO.DylibCommand):
                continue
            images: list[str] = load_command_images.setdefault(load_command.command.__name__, [])
            images.append(load_command.name)
        return load_command_images

    def parse_signature(self, macho_image: lief.MachO.Binary, data=None) -&gt; dict:

        if not macho_image.has_code_signature:
            return {}

        info = {}
        reader = StructReader(macho_image.code_signature.content)
        super_blob = SuperBlob(reader)

        for blob in super_blob.blobs:

            if blob.type == BlobType.CODEDIRECTORY:
                codedirectory_blob = CodeDirectoryBlob(blob.data)
                if codedirectory_blob.flags &amp; CS_ADHOC != 0:
                    info[&#39;AdHocSigned&#39;] = True
                else:
                    info[&#39;AdHocSigned&#39;] = False
                reader.seekset(codedirectory_blob.identOffset + blob.offset)
                info[&#39;SignatureIdentifier&#39;] = reader.read_c_string(&#39;utf8&#39;)
                continue

            if blob.type == BlobType.CMS_SIGNATURE:
                reader.seekset(blob.offset)
                cms_signature = blob.data
                if not cms_signature:
                    continue
                try:
                    parsed_cms_signature = pemeta.parse_signature(bytearray(cms_signature))
                    info[&#39;Signature&#39;] = parsed_cms_signature
                except ValueError as pkcs7_parse_error:
                    self.log_warn(F&#39;Could not parse the data in CSSLOT_CMS_SIGNATURE as valid PKCS7 data: {pkcs7_parse_error!s}&#39;)
                continue

            if blob.type == BlobType.REQUIREMENTS:
                # TODO: Parse the requirements blob,
                # which is encoded according to the code signing requirements language:
                # https://developer.apple.com/library/archive/documentation/Security
                #        /Conceptual/CodeSigningGuide/RequirementLang/RequirementLang.html
                info[&#39;Requirements&#39;] = blob.data.hex()
                continue

            if blob.type == BlobType.XML_ENTITLEMENTS:
                entitlements = bytes(blob.data)
                if not entitlements:
                    continue
                try:
                    entitlements = plistlib.loads(entitlements)
                except Exception as error:
                    self.log_warn(F&#39;failed to parse entitlements: {error!s}&#39;)
                else:
                    info[&#39;Entitlements&#39;] = entitlements

        return info

    def parse_version(self, macho: lief.MachO.Binary, data=None) -&gt; dict:
        info = {}
        load_commands: Iterable[lief.MachO.LoadCommand] = macho.commands
        for load_command in load_commands:
            if load_command.command == lief.MachO.LoadCommand.TYPE.SOURCE_VERSION:
                if &#39;SourceVersion&#39; not in info:
                    cmd: lief.MachO.SourceVersion = load_command
                    info[&#39;SourceVersion&#39;] = cmd.version[0]
                else:
                    self.log_warn(&#39;More than one load command of type SOURCE_VERSION found; the MachO file is possibly malformed&#39;)
                continue
            if load_command.command == lief.MachO.LoadCommand.TYPE.BUILD_VERSION:
                if &#39;BuildVersion&#39; not in info:
                    cmd: lief.MachO.BuildVersion = load_command
                    info[&#39;BuildVersion&#39;] = {}
                    info[&#39;BuildVersion&#39;][&#39;Platform&#39;] = cmd.platform.__name__
                    info[&#39;BuildVersion&#39;][&#39;MinOS&#39;] = &#39;.&#39;.join(str(v) for v in cmd.minos)
                    info[&#39;BuildVersion&#39;][&#39;SDK&#39;] = &#39;.&#39;.join(str(v) for v in cmd.sdk)
                    info[&#39;BuildVersion&#39;][&#39;Ntools&#39;] = len(cmd.tools)
                else:
                    self.log_warn(&#39;More than one load command of type BUILD_VERSION found; the MachO file is possibly malformed&#39;)
                continue
        return info

    def parse_load_commands(self, macho: lief.MachO.Binary, data=None) -&gt; list:
        info = []
        load_commands: Iterable[lief.MachO.LoadCommand] = macho.commands
        for load_command in load_commands:
            info.append(dict(
                Type=load_command.command.__name__,
                Size=load_command.size,
                Data=load_command.data.hex(),
            ))
        return info

    def parse_imports(self, macho: lief.MachO.Binary, data=None) -&gt; list:
        info = []
        imports: Iterable[lief.MachO.Symbol] = macho.imported_symbols
        for imp in imports:
            info.append(lief.string(imp.name))
        return info

    def parse_exports(self, macho: lief.MachO.Binary, data=None) -&gt; list:
        info = []
        exports: Iterable[lief.MachO.Symbol] = macho.exported_symbols
        for exp in exports:
            info.append(lief.string(exp.name))
        return info

    def process(self, data: bytearray):
        result = {}
        slices = []
        macho = lief.load_macho(data)
        macho_slices: list[lief.MachO.Binary] = []

        for k in itertools.count():
            if not (ms := macho.at(k)):
                break
            macho_slices.append(ms)

        result[&#39;FileType&#39;] = &#39;FAT&#39; if len(macho_slices) &gt; 1 else &#39;THIN&#39;

        for image in macho_slices:
            slice_result = {}

            for switch, resolver, name in [
                (self.args.header,          self.parse_macho_header,  &#39;Header&#39;),       # noqa
                (self.args.linked_images,   self.parse_linked_images, &#39;LinkedImages&#39;), # noqa
                (self.args.signatures,      self.parse_signature,     &#39;Signatures&#39;),   # noqa
                (self.args.version,         self.parse_version,       &#39;Version&#39;),      # noqa
                (self.args.load_commands,   self.parse_load_commands, &#39;LoadCommands&#39;), # noqa
                (self.args.imports,         self.parse_imports,       &#39;Imports&#39;),      # noqa
                (self.args.exports,         self.parse_exports,       &#39;Exports&#39;),      # noqa
            ]:
                if not switch:
                    continue
                self.log_debug(F&#39;parsing: {name}&#39;)
                try:
                    info = resolver(image, data)
                except Exception as E:
                    self.log_info(F&#39;failed to obtain {name}: {E!s}&#39;)
                    continue
                if info:
                    slice_result[name] = info

            if image.uuid is not None:
                uuid = bytes(image.uuid.uuid)
                slice_result[&#39;UUID&#39;] = uuid.hex()
            slice_result[&#39;SymHash&#39;] = self.compute_symhash(image)
            if fileset_name := image.fileset_name:
                slice_result[&#39;FilesetName&#39;] = fileset_name
            slices.append(slice_result)

        if slices:
            result[&#39;Slices&#39;] = slices
            yield from ppjson(
                tabular=self.args.tabular
            )._pretty_output(result, indent=4, ensure_ascii=False)</code></pre>
</details>
</dd>
<dt id="refinery.shell.map"><code class="flex name class">
<span>class <span class="ident">map</span></span>
<span>(</span><span>index, image, default=(), blocksize=1)</span>
</code></dt>
<dd>
<section class="desc"><p>Each block of the input data which occurs as a block of the index argument is replaced by the
corresponding block of the image argument. If a block size is specified, and if the index or
image argument are byte sequences, they are unpacked into chunks of that size, and excess bytes
that are not an integer multiple of the block size are discarded. To prevent any automatic
chunking, the <code><a title="refinery.lib.argformats.DelayedArgument.btoi" href="lib/argformats.html#refinery.lib.argformats.DelayedArgument.btoi">DelayedArgument.btoi()</a></code> handler can be used.
An optional default value can be provided to serve as inserts for any blocks in the input that
do not occur in the index sequence. If this argument is not specified, such blocks are left
unchanged.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/blockwise/map.py#L11-L80" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class map(BlockTransformation):
    &#34;&#34;&#34;
    Each block of the input data which occurs as a block of the index argument is replaced by the
    corresponding block of the image argument. If a block size is specified, and if the index or
    image argument are byte sequences, they are unpacked into chunks of that size, and excess bytes
    that are not an integer multiple of the block size are discarded. To prevent any automatic
    chunking, the `refinery.lib.argformats.DelayedArgument.btoi` handler can be used.
    An optional default value can be provided to serve as inserts for any blocks in the input that
    do not occur in the index sequence. If this argument is not specified, such blocks are left
    unchanged.
    &#34;&#34;&#34;
    _map: dict[int, int]

    def __init__(
        self,
        index   : Param[isq, Arg.NumSeq(help=&#39;index characters&#39;)],
        image   : Param[isq, Arg.NumSeq(help=&#39;image characters&#39;)],
        default : Param[isq, Arg.NumSeq(help=&#39;default value&#39;)] = (),
        blocksize=1
    ):
        super().__init__(blocksize=blocksize, index=index, image=image, default=default, _truncate=2)
        self._map = {}

    def reverse(self, data):
        return self._process(data, self.args.image, self.args.index, self.args.default)

    def process(self, data):
        return self._process(data, self.args.index, self.args.image, self.args.default)

    def _process(self, data: bytearray, index: Sequence[int], image: Sequence[int], default: Sequence[int]):
        if not self.bytestream:
            if isbuffer(index):
                self.log_info(F&#39;chunking index sequence into blocks of size {self.blocksize}&#39;)
                index = list(self.chunk(index))
                self.log_debug(F&#39;index sequence: {index}&#39;)
            if isbuffer(image):
                self.log_info(F&#39;chunking image sequence into blocks of size {self.blocksize}&#39;)
                image = list(self.chunk(image))
                self.log_debug(F&#39;image sequence: {image}&#39;)
            if isbuffer(default):
                self.log_info(F&#39;chunking default sequence into blocks of size {self.blocksize}&#39;)
                default = list(self.chunk(default))
                self.log_debug(F&#39;default sequence: {default}&#39;)
        if len(set(index)) != len(index):
            raise ValueError(&#39;The index sequence contains duplicates.&#39;)
        if len(index) &gt; len(image):
            raise ValueError(&#39;The index sequence is longer than the image sequence.&#39;)

        if self.bytestream:
            mapping = dict(zip(index, image))
            if default:
                d = iter(cycle(default))
                mapping = bytes(mapping.get(c, d) for c in range(0x100))
            else:
                mapping = bytes(mapping.get(c, c) for c in range(0x100))
            if not isinstance(data, bytearray):
                data = bytearray(data)
            data[:] = (mapping[b] for b in data)
            return data
        try:
            self.log_info(default)
            self._def = cycle(default) if default else None
            self._map = dict(zip(index, image))
            return super().process(data)
        finally:
            self._map = {}

    def process_block(self, block):
        default = next(it) if (it := self._def) else block
        return self._map.get(block, default)</code></pre>
</details>
</dd>
<dt id="refinery.shell.maru"><code class="flex name class">
<span>class <span class="ident">maru</span></span>
<span>(</span><span>seed=0, reps=1, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the 64bit maru hash of the input data.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/hash/maru.py#L8-L21" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class maru(HashUnit):
    &#34;&#34;&#34;
    Returns the 64bit maru hash of the input data.
    &#34;&#34;&#34;
    def __init__(
        self,
        seed: Param[int, Arg.Number(help=&#39;optional seed value&#39;)] = 0,
        reps=1,
        text=False,
    ):
        super().__init__(seed=seed, text=text, reps=reps)

    def _algorithm(self, data) -&gt; bytes:
        return maru32digest(data, self.args.seed)</code></pre>
</details>
</dd>
<dt id="refinery.shell.max_"><code class="flex name class">
<span>class <span class="ident">max_</span></span>
<span>(</span><span>key=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Picks the maximum of all elements in the current <code><a title="refinery.lib.frame" href="lib/frame.html">refinery.lib.frame</a></code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/max.py#L10-L63" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class max_(Unit):
    &#34;&#34;&#34;
    Picks the maximum of all elements in the current `refinery.lib.frame`.
    &#34;&#34;&#34;

    def __init__(
        self,
        key: Param[str, Arg.String(&#39;key&#39;, help=&#39;A meta variable expression to sort by instead of sorting the content.&#39;)] = None,
    ):
        super().__init__(key=key)

    def filter(self, chunks: Iterable[Chunk]):
        def get_value(chunk: Chunk):
            if key is None:
                return chunk
            return metavars(chunk).get(key)

        key = self.args.key
        it = iter(chunks)

        for max_chunk in it:
            if not max_chunk.visible:
                yield max_chunk
            else:
                max_index = 0
                max_value = get_value(max_chunk)
                break
        else:
            return

        for index, chunk in enumerate(chunks, 1):
            if not chunk.visible:
                yield chunk
                continue
            value = get_value(chunk)
            try:
                is_max = value &gt; max_value
            except TypeError:
                if max_value is None:
                    self.log_info(
                        F&#39;Discarding chunk {max_index} in favor of {index} because {key} was not &#39;
                        F&#39;set on the former; new maximum is {value!r}.&#39;)
                    is_max = True
                else:
                    self.log_info(
                        F&#39;Discarding chunk {index} because {key} had value {value!r}; it could not &#39;
                        F&#39;be compared to the current maximum {max_value!r} on chunk {max_index}.&#39;)
                    is_max = False
            if is_max:
                max_value = value
                max_chunk = chunk
                max_index = index

        yield max_chunk</code></pre>
</details>
</dd>
<dt id="refinery.shell.md2"><code class="flex name class">
<span>class <span class="ident">md2</span></span>
<span>(</span><span>reps=1, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the MD2 hash of the input data.</p></section>
</dd>
<dt id="refinery.shell.md4"><code class="flex name class">
<span>class <span class="ident">md4</span></span>
<span>(</span><span>reps=1, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the MD4 hash of the input data.</p></section>
</dd>
<dt id="refinery.shell.mimewords"><code class="flex name class">
<span>class <span class="ident">mimewords</span></span>
</code></dt>
<dd>
<section class="desc"><p>Implements the decoding of MIME encoded-word syntax from RFC-2047.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/pattern/mimewords.py#L12-L31" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class mimewords(Unit):
    &#34;&#34;&#34;
    Implements the decoding of MIME encoded-word syntax from RFC-2047.
    &#34;&#34;&#34;
    @classmethod
    def convert(cls, word: str) -&gt; str:
        &#34;&#34;&#34;
        Converts the MIME word.
        &#34;&#34;&#34;
        def replacer(match):
            decoded, = decode_header(match[0])
            raw, codec = decoded
            if not isinstance(codec, str):
                codec = cls.codec
            return codecs.decode(raw, codec, errors=&#39;surrogateescape&#39;)
        return re.sub(R&#34;=(?:\?[^\?]*){3}\?=&#34;, replacer, word)

    @unicoded
    def process(self, data):
        return self.convert(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.min_"><code class="flex name class">
<span>class <span class="ident">min_</span></span>
<span>(</span><span>key=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Picks the minimum of all elements in the current <code><a title="refinery.lib.frame" href="lib/frame.html">refinery.lib.frame</a></code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/min.py#L10-L63" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class min_(Unit):
    &#34;&#34;&#34;
    Picks the minimum of all elements in the current `refinery.lib.frame`.
    &#34;&#34;&#34;

    def __init__(
        self,
        key: Param[str, Arg.String(&#39;key&#39;, help=&#39;A meta variable expression to sort by instead of sorting the content.&#39;)] = None,
    ):
        super().__init__(key=key)

    def filter(self, chunks: Iterable[Chunk]):
        def get_value(chunk: Chunk):
            if key is None:
                return chunk
            return metavars(chunk).get(key)

        key = self.args.key
        it = iter(chunks)

        for min_chunk in it:
            if not min_chunk.visible:
                yield min_chunk
            else:
                min_index = 0
                min_value = get_value(min_chunk)
                break
        else:
            return

        for index, chunk in enumerate(chunks, 1):
            if not chunk.visible:
                yield chunk
                continue
            value = get_value(chunk)
            try:
                is_min = value &lt; min_value
            except TypeError:
                if min_value is None:
                    self.log_info(
                        F&#39;Discarding chunk {min_index} in favor of {index} because {key} was not &#39;
                        F&#39;set on the former; new minimum is {value!r}.&#39;)
                    is_min = True
                else:
                    self.log_info(
                        F&#39;Discarding chunk {index} because {key} had value {value!r}; it could not &#39;
                        F&#39;be compared to the current minimum {min_value!r} on chunk {min_index}.&#39;)
                    is_min = False
            if is_min:
                min_value = value
                min_chunk = chunk
                min_index = index

        yield min_chunk</code></pre>
</details>
</dd>
<dt id="refinery.shell.morse"><code class="flex name class">
<span>class <span class="ident">morse</span></span>
<span>(</span><span>language=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Morse encoding and decoding. All tokens in the input data which consist of dashes and dots are
replaced by their Morse decoding.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/morse.py#L36-L334" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class morse(Unit):
    &#34;&#34;&#34;
    Morse encoding and decoding. All tokens in the input data which consist of dashes and dots are
    replaced by their Morse decoding.
    &#34;&#34;&#34;
    def __init__(
        self,
        language: Param[str, Arg.Option(choices=MorseLanguage, help=(
            &#39;Optionally choose a language. If none is specified, the unit will attempt to detect &#39;
            &#39;the language automatically. Options are: {choices}&#39;))] = None,
    ):
        super().__init__(language=Arg.AsOption(language, MorseLanguage))

    @classmethod
    def handles(cls, data):
        if re.fullmatch(BR&#39;[-.\s]+&#39;, data, re.DOTALL):
            return True

    @unicoded
    def process(self, data: str):
        language: MorseLanguage = self.args.language
        parsed = re.split(&#39;(\\s+)&#39;, data)
        tokens = {t for t in parsed[::2] if t}
        tables = [
            self._DECODE_SYMBOL,
            self._DECODE_DIGITS,
        ]

        if language is not None:
            tables.append(self._DECODE[language])
        else:
            special = set(self._DECODE_SYMBOL) | set(self._DECODE_DIGITS)
            best_ratio = 1 # number of unused codes
            best_table = None
            for language in MorseLanguage:
                table = self._DECODE[language]
                codes = set(table)
                if not tokens &lt;= codes | special:
                    continue
                if language == MorseLanguage.EN:
                    best_table = table
                    break
                ratio = len(codes - tokens) / len(codes)
                if ratio &lt; best_ratio:
                    best_ratio = ratio
                    best_table = table
            if best_table is None:
                raise LookupError(&#39;Unable to determine language, please specify it manually.&#39;)
            tables.append(best_table)

        with io.StringIO() as out:
            for k, string in enumerate(parsed):
                if k % 2 == 1:
                    string = string[1:]
                    if len(string) &gt; 1:
                        string = string[:-1]
                    out.write(string)
                    continue
                if not string:
                    continue
                for table in tables:
                    try:
                        out.write(table[string])
                        break
                    except KeyError:
                        continue
                else:
                    raise ValueError(F&#39;invalid token: {string}&#39;)
            return out.getvalue()

    @unicoded
    def reverse(self, data: str):
        language: MorseLanguage = self.args.language
        tables = [
            self._ENCODE_SYMBOL,
            self._ENCODE_DIGITS,
        ]
        if language is not None:
            tables.append(self._ENCODE[language])
        else:
            tables.extend(self._ENCODE.values())

        def _encode(letter):
            for table in tables:
                try:
                    return table[letter]
                except KeyError:
                    continue
            else:
                raise ValueError(F&#39;cannot encode letter &#34;{letter}&#34;&#39;)

        with io.StringIO() as out:
            for k, word in enumerate(re.split(&#39;(\\s+)&#39;, data)):
                if k % 2 == 1:
                    out.write(F&#39; {word} &#39;)
                    continue
                out.write(&#39; &#39;.join(_encode(letter) for letter in word.lower()))
            return out.getvalue()

    _ENCODE = {
        MorseLanguage.EN: {
            &#39;a&#39;: &#39;.-&#39;,
            &#39;b&#39;: &#39;-...&#39;,
            &#39;c&#39;: &#39;-.-.&#39;,
            &#39;d&#39;: &#39;-..&#39;,
            &#39;e&#39;: &#39;.&#39;,
            &#39;f&#39;: &#39;..-.&#39;,
            &#39;g&#39;: &#39;--.&#39;,
            &#39;h&#39;: &#39;....&#39;,
            &#39;i&#39;: &#39;..&#39;,
            &#39;j&#39;: &#39;.---&#39;,
            &#39;k&#39;: &#39;-.-&#39;,
            &#39;l&#39;: &#39;.-..&#39;,
            &#39;m&#39;: &#39;--&#39;,
            &#39;n&#39;: &#39;-.&#39;,
            &#39;o&#39;: &#39;---&#39;,
            &#39;p&#39;: &#39;.--.&#39;,
            &#39;q&#39;: &#39;--.-&#39;,
            &#39;r&#39;: &#39;.-.&#39;,
            &#39;s&#39;: &#39;...&#39;,
            &#39;t&#39;: &#39;-&#39;,
            &#39;u&#39;: &#39;..-&#39;,
            &#39;v&#39;: &#39;...-&#39;,
            &#39;w&#39;: &#39;.--&#39;,
            &#39;x&#39;: &#39;-..-&#39;,
            &#39;y&#39;: &#39;-.--&#39;,
            &#39;z&#39;: &#39;--..&#39;,
        }
    }
    _ENCODE[MorseLanguage.ES] = _extend_dictionary(_ENCODE[MorseLanguage.EN], {
        &#39;á&#39;: &#39;.--.-&#39;,
        &#39;é&#39;: &#39;..-..&#39;,
        &#39;í&#39;: &#39;..&#39;,
        &#39;ñ&#39;: &#39;--.--&#39;,
        &#39;ó&#39;: &#39;---.&#39;,
        &#39;ú&#39;: &#39;..-&#39;,
        &#39;ü&#39;: &#39;..--&#39;,
        &#39;¿&#39;: &#39;..-.-&#39;,
        &#39;¡&#39;: &#39;--...-&#39;,
    })
    _ENCODE[MorseLanguage.DE] = _extend_dictionary(_ENCODE[MorseLanguage.EN], {
        &#39;ä&#39;: &#39;.-.-&#39;,
        &#39;ö&#39;: &#39;---.&#39;,
        &#39;ü&#39;: &#39;..--&#39;,
        &#39;ß&#39;: &#39;...--..&#39;,
    })
    _ENCODE[MorseLanguage.FR] = _extend_dictionary(_ENCODE[MorseLanguage.EN], {
        &#39;à&#39;: &#39;.--.-&#39;,
        &#39;â&#39;: &#39;.--.-&#39;,
        &#39;ç&#39;: &#39;-.-..&#39;,
        &#39;è&#39;: &#39;.-..-&#39;,
        &#39;é&#39;: &#39;..-..&#39;,
        &#39;ê&#39;: &#39;-..-.&#39;,
        &#39;ë&#39;: &#39;..-..&#39;,
        &#39;î&#39;: &#39;..&#39;,
        &#39;ï&#39;: &#39;-..--&#39;,
        &#39;ô&#39;: &#39;---&#39;,
        &#39;ù&#39;: &#39;..-&#39;,
        &#39;ü&#39;: &#39;..--&#39;,
    })
    _ENCODE[MorseLanguage.RU] = {
        &#39;а&#39;: &#39;.-&#39;,
        &#39;б&#39;: &#39;-...&#39;,
        &#39;в&#39;: &#39;.--&#39;,
        &#39;г&#39;: &#39;--.&#39;,
        &#39;д&#39;: &#39;-..&#39;,
        &#39;е&#39;: &#39;.&#39;,
        &#39;ё&#39;: &#39;.&#39;,
        &#39;ж&#39;: &#39;...-&#39;,
        &#39;з&#39;: &#39;--..&#39;,
        &#39;и&#39;: &#39;..&#39;,
        &#39;й&#39;: &#39;.---&#39;,
        &#39;к&#39;: &#39;-.-&#39;,
        &#39;л&#39;: &#39;.-..&#39;,
        &#39;м&#39;: &#39;--&#39;,
        &#39;н&#39;: &#39;-.&#39;,
        &#39;о&#39;: &#39;---&#39;,
        &#39;п&#39;: &#39;.--.&#39;,
        &#39;р&#39;: &#39;.-.&#39;,
        &#39;с&#39;: &#39;...&#39;,
        &#39;т&#39;: &#39;-&#39;,
        &#39;у&#39;: &#39;..-&#39;,
        &#39;ф&#39;: &#39;..-.&#39;,
        &#39;х&#39;: &#39;....&#39;,
        &#39;ц&#39;: &#39;-.-.&#39;,
        &#39;ч&#39;: &#39;---.&#39;,
        &#39;ш&#39;: &#39;----&#39;,
        &#39;щ&#39;: &#39;--.-&#39;,
        &#39;ъ&#39;: &#39;--.--&#39;,
        &#39;ы&#39;: &#39;-.--&#39;,
        &#39;ь&#39;: &#39;-..-&#39;,
        &#39;э&#39;: &#39;..-..&#39;,
        &#39;ю&#39;: &#39;..--&#39;,
        &#39;я&#39;: &#39;.-.-&#39;,
    }
    _ENCODE[MorseLanguage.UA] = _extend_dictionary(_ENCODE[MorseLanguage.RU], {
        &#39;ґ&#39;: &#39;--.&#39;,
        &#39;и&#39;: &#39;-.--&#39;,
        &#39;ї&#39;: &#39;.---.&#39;,
    })
    _ENCODE[MorseLanguage.UA][&#39;є&#39;] = _ENCODE[MorseLanguage.UA].pop(&#39;э&#39;)
    _ENCODE[MorseLanguage.UA][&#39;і&#39;] = _ENCODE[MorseLanguage.UA].pop(&#39;и&#39;)

    _ENCODE[MorseLanguage.HE] = {
        &#39;א&#39;: &#39;.-&#39;,
        &#39;ב&#39;: &#39;-...&#39;,
        &#39;ג&#39;: &#39;--.&#39;,
        &#39;ד&#39;: &#39;-..&#39;,
        &#39;ה&#39;: &#39;---&#39;,
        &#39;ו&#39;: &#39;.&#39;,
        &#39;ז&#39;: &#39;--..&#39;,
        &#39;ח&#39;: &#39;....&#39;,
        &#39;ט&#39;: &#39;..--&#39;,
        &#39;י&#39;: &#39;..&#39;,
        &#39;כ&#39;: &#39;-.&#39;,
        &#39;ל&#39;: &#39;.-..&#39;,
        &#39;מ&#39;: &#39;--&#39;,
        &#39;נ&#39;: &#39;--.&#39;,
        &#39;ס&#39;: &#39;-.-.&#39;,
        &#39;ע&#39;: &#39;.---&#39;,
        &#39;פ&#39;: &#39;.--.&#39;,
        &#39;צ&#39;: &#39;.--&#39;,
        &#39;ק&#39;: &#39;--.-&#39;,
        &#39;ר&#39;: &#39;.-.&#39;,
        &#39;ש&#39;: &#39;...&#39;,
        &#39;ת&#39;: &#39;-&#39;,
    }

    _ENCODE[MorseLanguage.AR] = {
        &#39;ا&#39;: &#39;.-&#39;,
        &#39;ب&#39;: &#39;-...&#39;,
        &#39;ت&#39;: &#39;-&#39;,
        &#39;ث&#39;: &#39;-.-.&#39;,
        &#39;ج&#39;: &#39;.---&#39;,
        &#39;ح&#39;: &#39;....&#39;,
        &#39;خ&#39;: &#39;---&#39;,
        &#39;د&#39;: &#39;-..&#39;,
        &#39;ذ&#39;: &#39;--..&#39;,
        &#39;ر&#39;: &#39;.-.&#39;,
        &#39;ز&#39;: &#39;---.&#39;,
        &#39;س&#39;: &#39;...&#39;,
        &#39;ش&#39;: &#39;----&#39;,
        &#39;ص&#39;: &#39;-..-&#39;,
        &#39;ض&#39;: &#39;...-&#39;,
        &#39;ط&#39;: &#39;..-&#39;,
        &#39;ظ&#39;: &#39;-.--&#39;,
        &#39;ع&#39;: &#39;.-.-&#39;,
        &#39;غ&#39;: &#39;--.&#39;,
        &#39;ف&#39;: &#39;..-.&#39;,
        &#39;ق&#39;: &#39;--.-&#39;,
        &#39;ك&#39;: &#39;-.-&#39;,
        &#39;ل&#39;: &#39;.-..&#39;,
        &#39;م&#39;: &#39;--&#39;,
        &#39;ن&#39;: &#39;-.&#39;,
        &#39;ه&#39;: &#39;..-..&#39;,
        &#39;و&#39;: &#39;.--&#39;,
        &#39;ي&#39;: &#39;..&#39;,
        &#39;ﺀ&#39;: &#39;.&#39;,
    }

    _ENCODE_DIGITS = {
        &#39;0&#39;: &#39;-----&#39;,
        &#39;1&#39;: &#39;.----&#39;,
        &#39;2&#39;: &#39;..---&#39;,
        &#39;3&#39;: &#39;...--&#39;,
        &#39;4&#39;: &#39;....-&#39;,
        &#39;5&#39;: &#39;.....&#39;,
        &#39;6&#39;: &#39;-....&#39;,
        &#39;7&#39;: &#39;--...&#39;,
        &#39;8&#39;: &#39;---..&#39;,
        &#39;9&#39;: &#39;----.&#39;
    }

    _ENCODE_SYMBOL = {
        &#39;_&#39;: &#39;..--.-&#39;,
        &#39;-&#39;: &#39;-....-&#39;,
        &#39;,&#39;: &#39;--..--&#39;,
        &#39;;&#39;: &#39;-.-.-.&#39;,
        &#39;:&#39;: &#39;---...&#39;,
        &#39;!&#39;: &#39;-.-.--&#39;,
        &#39;?&#39;: &#39;..--..&#39;,
        &#39;.&#39;: &#39;.-.-.-&#39;,
        &#39;&#34;&#39;: &#39;.-..-.&#39;,
        &#39;(&#39;: &#39;-.--.&#39;,
        &#39;)&#39;: &#39;-.--.-&#39;,
        &#39;@&#39;: &#39;.--.-.&#39;,
        &#39;/&#39;: &#39;-..-.&#39;,
        &#39;\\&#39;: &#39;-..-.&#39;,
        &#39;&amp;&#39;: &#39;.-...&#39;,
        &#39;+&#39;: &#39;.-.-.&#39;,
        &#39;=&#39;: &#39;-...-&#39;,
        &#39;$&#39;: &#39;...-..-&#39;,
        &#34;&#39;&#34;: &#39;.----.&#39;,
    }

    _DECODE = {
        lng: _reverse_dictionary(tbl) for lng, tbl in _ENCODE.items()}
    _DECODE_SYMBOL = _reverse_dictionary(_ENCODE_SYMBOL)
    _DECODE_DIGITS = _reverse_dictionary(_ENCODE_DIGITS)</code></pre>
</details>
</dd>
<dt id="refinery.shell.mscdk"><code class="flex name class">
<span>class <span class="ident">mscdk</span></span>
<span>(</span><span>size, hash='MD5')</span>
</code></dt>
<dd>
<section class="desc"><p>An implementation of the CryptDeriveKey routine available from the Win32 API.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/keyderive/mscdk.py#L11-L38" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class mscdk(KeyDerivation):
    &#34;&#34;&#34;
    An implementation of the CryptDeriveKey routine available from the Win32 API.
    &#34;&#34;&#34;

    def __init__(self, size, hash=&#39;MD5&#39;):
        super().__init__(size=size, salt=None, hash=hash)

    def process(self, data):
        def digest(x):
            return self.hash.new(x).digest()
        size = self.args.size
        if self.args.hash in (HASH.SHA224, HASH.SHA256, HASH.SHA384, HASH.SHA512):
            buffer = digest(data)
            max_size = len(buffer)
        else:
            max_size = 2 * self.hash.digest_size
            value = digest(data)
            del data
            buffer1 = bytearray([0x36] * 64)
            buffer2 = bytearray([0x5C] * 64)
            for k, b in enumerate(value):
                buffer1[k] ^= b
                buffer2[k] ^= b
            buffer = digest(buffer1) + digest(buffer2)
        if size &gt; max_size:
            raise RefineryPartialResult(F&#39;too many bytes requested, can only provide {max_size}&#39;, partial=buffer)
        return buffer[:size]</code></pre>
</details>
</dd>
<dt id="refinery.shell.mscf"><code class="flex name class">
<span>class <span class="ident">mscf</span></span>
<span>(</span><span>mode=None)</span>
</code></dt>
<dd>
<section class="desc"><p>The Microsoft Compression Format unit implements the format and algorithms used by the Microsoft
Compression API. The implementation for LZMS is currently missing, but MSZIP and XPRESS (both
with and without Huffman table) are supported. This pure Python implementation is very slow when
compared to native code, so decompressing very large inputs can take several minutes.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/mscf.py#L36-L253" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class mscf(Unit):
    &#34;&#34;&#34;
    The Microsoft Compression Format unit implements the format and algorithms used by the Microsoft
    Compression API. The implementation for LZMS is currently missing, but MSZIP and XPRESS (both
    with and without Huffman table) are supported. This pure Python implementation is very slow when
    compared to native code, so decompressing very large inputs can take several minutes.
    &#34;&#34;&#34;

    _SIGNATURE = B&#39;\x0A\x51\xE5\xC0&#39;

    def __init__(
        self,
        mode: Param[str | None, Arg.Option(choices=MODE, help=(
            &#39;Manually select decompression mode ({choices}); by default the unit attempts to derive the &#39;
            &#39;mode from the header, but this will fail for raw streams. However, even if a header is &#39;
            &#39;found, a manually specified mode will take precedence.&#39;))] = None,
    ):
        super().__init__(mode=Arg.AsOption(mode, MODE))

    def process(self, data):
        mode: MODE = self.args.mode
        with StructReader(memoryview(data)) as reader, MemoryFile() as writer:
            reader: StructReader[memoryview]
            check = zlib.crc32(reader.peek(6))
            magic = reader.read(4)
            if magic != self._SIGNATURE:
                if mode is None:
                    self.log_warn(
                        F&#39;data starts with {magic.hex().upper()} rather than the expected sequence &#39;
                        F&#39;{self._SIGNATURE.hex().upper()}; this could be a raw stream.&#39;)
                else:
                    reader.seek(0)
                    handler = self._get_handler(mode)
                    handler(reader, writer, None)
                    return writer.getvalue()

            header_size = reader.u16()
            if header_size != 24:
                self.log_warn(F&#39;the header size {header_size} was not equal to 24&#39;)

            crc32byte = reader.u8()
            check = zlib.crc32(reader.peek(0x11), check) &amp; 0xFF
            if check != crc32byte:
                self.log_warn(F&#39;the CRC32 check byte was {crc32byte}, computed value was {check}&#39;)

            _mode_code = reader.u8()

            try:
                _mode = MODE(_mode_code)
            except ValueError:
                msg = F&#39;header contains unknown compression type code {_mode_code}&#39;
                if mode is None:
                    raise ValueError(msg)
                else:
                    self.log_warn(msg)
            else:
                if mode is not None and mode != _mode:
                    logger = self.log_warn
                else:
                    logger = self.log_info
                    mode = _mode
                logger(F&#39;header specifies algorithm {_mode.name}&#39;)

            self.log_info(F&#39;using algorithm {mode.name}&#39;)
            decompress = self._get_handler(mode)

            final_size = reader.u32()
            _unknown_1 = reader.u32()
            chunk_size = reader.u32()
            _unknown_2 = reader.u32()

            if _unknown_1 != 0:
                self.log_warn(F&#39;unknown value 1 was unexpectedly nonzero: 0x{_unknown_1:08X}&#39;)
            if _unknown_2 != 0:
                self.log_warn(F&#39;unknown value 2 was unexpectedly nonzero: 0x{_unknown_2:08X}&#39;)

            self.log_debug(F&#39;final size: 0x{final_size:08X}&#39;)
            self.log_debug(F&#39;chunk size: 0x{chunk_size:08X}&#39;)

            if chunk_size &gt; COMPRESS_MAX_CHUNK:
                raise ValueError(&#39;the header chunk size is greater than the maximum value&#39;)

            while len(writer) &lt; final_size:
                src_size = reader.u32()
                src_data = reader.read(src_size)
                if len(src_data) != src_size:
                    raise IndexError(F&#39;Attempted to read {src_size} bytes, but got only {len(src_data)}.&#39;)
                if src_size + len(writer) == final_size:
                    self.log_debug(F&#39;final chunk is uncompressed, appending {src_size} raw bytes to output&#39;)
                    writer.write(src_data)
                    break
                self.log_debug(F&#39;reading chunk of size {src_size}&#39;)
                start = writer.tell()
                chunk = StructReader(src_data)
                target = min(chunk_size, final_size - len(writer))
                decompress(chunk, writer, target)
                writer.flush()
                written = writer.tell() - start
                if written != target:
                    raise RuntimeError(F&#39;decompressed output had unexpected size {written} instead of {chunk_size}&#39;)

            if not reader.eof:
                self.log_info(F&#39;compression complete with {reader.remaining_bytes} bytes remaining in input&#39;)
            return writer.getvalue()

    def _get_handler(self, mode: MODE) -&gt; Callable[[StructReader, MemoryFile, int | None], None]:
        decompress = {
            mode.MSZIP       : self._decompress_mszip,
            mode.XPRESS_HUFF : self._decompress_xpress_huffman,
            mode.XPRESS      : self._decompress_xpress,
        }.get(mode, None)
        if decompress is None:
            raise NotImplementedError(F&#39;algorithm {mode.name} is not yet implemented&#39;)
        return decompress

    def _decompress_mszip(self, reader: StructReader, writer: MemoryFile, target: int | None = None):
        header = bytes(reader.read(2))
        if header != B&#39;CK&#39;:
            raise ValueError(F&#39;chunk did not begin with CK header, got {header!r} instead&#39;)
        decompress = zlib.decompressobj(-zlib.MAX_WBITS, zdict=writer.getvalue())
        writer.write(decompress.decompress(reader.read()))
        writer.write(decompress.flush())

    def _decompress_xpress_huffman(
        self,
        reader: StructReader,
        writer: MemoryFile,
        target: int | None = None,
        max_chunk_size: int = 0x10000
    ) -&gt; None:
        limit = writer.tell()
        if target is not None:
            target += limit

        while not reader.eof:

            if reader.remaining_bytes &lt; XPRESS_NUM_SYMBOLS // 2:
                raise IndexError(
                    F&#39;There are only {reader.remaining_bytes} bytes reamining in the input buffer,&#39;
                    F&#39; but at least {XPRESS_NUM_SYMBOLS // 2} are required to read a Huffman table.&#39;)

            table = bytearray(reader.read_integer(4) for _ in range(XPRESS_NUM_SYMBOLS))
            table = make_huffman_decode_table(table, XPRESS_TABLEBITS, XPRESS_MAX_CODEWORD_LEN)
            limit = limit + max_chunk_size
            flags = BitBufferedReader(reader, 16)

            while True:
                position = writer.tell()
                if position == target:
                    if reader.remaining_bytes:
                        self.log_info(F&#39;chunk decompressed with {reader.remaining_bytes} bytes remaining in input buffer&#39;)
                    return
                if position &gt;= limit:
                    if position &gt; limit:
                        limit = position
                        self.log_info(F&#39;decompression of one chunk generated more than the limit of {max_chunk_size} bytes&#39;)
                    flags.collect()
                    break
                try:
                    sym = read_huffman_symbol(flags, table, XPRESS_TABLEBITS, XPRESS_MAX_CODEWORD_LEN)
                except EOFError:
                    self.log_debug(&#39;end of file while reading huffman symbol&#39;)
                    break
                if sym &lt; XPRESS_NUM_CHARS:
                    writer.write_byte(sym)
                    continue
                length = sym &amp; 0xF
                offsetlog = (sym &gt;&gt; 4) &amp; 0xF
                flags.collect()
                if reader.eof:
                    break
                offset = (1 &lt;&lt; offsetlog) | flags.read(offsetlog)
                if length == 0xF:
                    nudge = reader.read_byte()
                    if nudge &lt; 0xFF:
                        length += nudge
                    else:
                        length = reader.u16() or reader.u32()
                length += XPRESS_MIN_MATCH_LEN
                writer.replay(offset, length)

    def _decompress_xpress(self, reader: StructReader, writer: MemoryFile, target: int | None = None) -&gt; bytearray:
        if target is not None:
            target += writer.tell()
        flags = BitBufferedReader(reader)
        nibble_cache = None
        while not reader.eof:
            if target is not None and writer.tell() &gt;= target:
                return
            if not flags.next():
                writer.write(reader.read(1))
                continue
            offset, length = divmod(reader.u16(), 8)
            offset += 1
            if length == 7:
                length = nibble_cache
                if length is None:
                    length_pair = reader.u8()
                    nibble_cache = length_pair &gt;&gt; 4
                    length = length_pair &amp; 0xF
                else:
                    nibble_cache = None
                if length == 15:
                    length = reader.u8()
                    if length == 0xFF:
                        length = reader.u16() or reader.u32()
                        length -= 22
                        if length &lt; 0:
                            raise RuntimeError(F&#39;Invalid match length of {length} for long delta sequence&#39;)
                    length += 15
                length += 7
            length += 3
            writer.replay(offset, length)

    @classmethod
    def handles(cls, data) -&gt; bool | None:
        if data[:len(cls._SIGNATURE)] == cls._SIGNATURE:
            return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.msgpack"><code class="flex name class">
<span>class <span class="ident">msgpack</span></span>
</code></dt>
<dd>
<section class="desc"><p>Converts a message-pack (msgpack) buffer to JSON and vice-versa.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/msgpack.py#L12-L37" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class msgpack(Unit):
    &#34;&#34;&#34;
    Converts a message-pack (msgpack) buffer to JSON and vice-versa.
    &#34;&#34;&#34;
    def reverse(self, data):
        try:
            data = json.loads(data)
        except Exception as E:
            try:
                data = json.loads(B&#39;[%s]&#39; % data)
            except Exception:
                raise E
        return mp.dumps(data)

    def process(self, data):
        unpacker: mp.fallback.Unpacker = mp.Unpacker(MemoryFile(data, output=bytes))
        for k in itertools.count():
            try:
                last = unpacker.tell()
                item = unpacker.unpack()
            except Exception as E:
                if isinstance(E, mp.OutOfData) and k == 1:
                    break
                raise RefineryPartialResult(str(E), memoryview(data)[last:]) from E
            else:
                yield json.dumps(item).encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.mspdb"><code class="flex name class">
<span>class <span class="ident">mspdb</span></span>
<span>(</span><span>size, salt, iter=100, hash='SHA1')</span>
</code></dt>
<dd>
<section class="desc"><p>An implementation of the PasswordDeriveBytes routine available from the .NET
standard library. According to documentation, it is an extension of PBKDF1.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/keyderive/mspdb.py#L6-L25" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class mspdb(KeyDerivation):
    &#34;&#34;&#34;
    An implementation of the PasswordDeriveBytes routine available from the .NET
    standard library. According to documentation, it is an extension of PBKDF1.
    &#34;&#34;&#34;
    def __init__(self, size, salt, iter=100, hash=&#39;SHA1&#39;):
        self.superinit(super(), **vars())

    def process(self, data):
        if self.codec != &#39;UTF8&#39;:
            data = data.decode(self.codec).encode(&#39;UTF8&#39;)
        data += self.args.salt
        for _ in range(self.args.iter - 1):
            data = self.hash.new(data).digest()
        counter, seedhash = 1, data
        data = self.hash.new(data).digest()
        while len(data) &lt; self.args.size:
            data += self.hash.new(B&#39;%d%s&#39; % (counter, seedhash)).digest()
            counter += 1
        return data[:self.args.size]</code></pre>
</details>
</dd>
<dt id="refinery.shell.mvg"><code class="flex name class">
<span>class <span class="ident">mvg</span></span>
<span>(</span><span>*names, top=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Short for "Make Variable Global": This unit can move meta variables into the scope of the
parent frame. If used at the end of a frame, the variables will be moved the scope of the
frame that the pipeline will return to. Otherwise and if the &ndash;top switch is being used,
variables will be moved to scope 0, i.e. to the topmost frame in the current tree.</p>
<p>Note that it is not possible to promote a variable to a parent frame if that variable does not
have the same value on all chunks in the current frame - such variables will always be removed
when the frame closes.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/mvg.py#L8-L43" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class mvg(Unit):
    &#34;&#34;&#34;
    Short for &#34;Make Variable Global&#34;: This unit can move meta variables into the scope of the
    parent frame. If used at the end of a frame, the variables will be moved the scope of the
    frame that the pipeline will return to. Otherwise and if the --top switch is being used,
    variables will be moved to scope 0, i.e. to the topmost frame in the current tree.

    Note that it is not possible to promote a variable to a parent frame if that variable does not
    have the same value on all chunks in the current frame - such variables will always be removed
    when the frame closes.
    &#34;&#34;&#34;
    def __init__(
        self,
        *names: Param[str, Arg.String(metavar=&#39;name&#39;, help=(
            &#39;Name of a variable to be removed. If no variables are explicitly specified, all &#39;
            &#39;variables in the current chunk will be rescoped.&#39;
        ))],
        top: Param[bool, Arg.Switch(&#39;-t&#39;, help=&#39;Move the variable(s) to the topmost frame layer.&#39;)] = False
    ):
        super().__init__(names=names, top=top)

    def process(self, data):
        meta = metavars(data)
        nest = self.args.nesting
        if nest &lt; 0 and not self.args.top:
            spot = meta.scope + nest
        else:
            spot = 1
        for name in self.args.names or meta.variable_names():
            try:
                if meta.get_scope(name) &lt;= spot:
                    continue
                meta.set_scope(name, spot)
            except KeyError:
                self.log_info(F&#39;variable not defined: {name}&#39;)
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.n40"><code class="flex name class">
<span>class <span class="ident">n40</span></span>
<span>(</span><span>key)</span>
</code></dt>
<dd>
<section class="desc"><p>Decrypts hex-encoded strings in various latin-american banker families, including N40.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/malware/n40.py#L11-L24" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class n40(Unit):
    &#34;&#34;&#34;
    Decrypts hex-encoded strings in various latin-american banker families, including N40.
    &#34;&#34;&#34;
    def __init__(self, key: Param[buf, Arg(help=&#39;Decryption key.&#39;)]):
        ...

    def process(self, data):
        try:
            data = b16decode(data, casefold=True)
        except Error:
            self.log_info(&#39;Input was not hex-encoded; ignoring this step.&#39;)
        mask = data[1:] | xor(self.args.key) | bytearray
        return bytearray(0xFF + b - a if b &lt;= a else b - a for a, b in zip(data, mask))</code></pre>
</details>
</dd>
<dt id="refinery.shell.neg"><code class="flex name class">
<span>class <span class="ident">neg</span></span>
<span>(</span><span>bigendian=False, blocksize=1)</span>
</code></dt>
<dd>
<section class="desc"><p>Each block of the input data is negated bitwise. This is sometimes
also called the bitwise complement or inverse.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/blockwise/neg.py#L6-L12" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class neg(UnaryOperation):
    &#34;&#34;&#34;
    Each block of the input data is negated bitwise. This is sometimes
    also called the bitwise complement or inverse.
    &#34;&#34;&#34;
    def operate(self, a): return ~a
    def inplace(self, a): a ^= self.fmask</code></pre>
</details>
</dd>
<dt id="refinery.shell.netbios"><code class="flex name class">
<span>class <span class="ident">netbios</span></span>
<span>(</span><span>key=b'A')</span>
</code></dt>
<dd>
<section class="desc"><p>Encodes and decodes strings using the same algorithm that is used for NetBIOS
labels. Each byte 0xUL is encoded as two bytes, which are the sum of 0xU and
0xL with an offset character, respectively. The default offset is the capital
letter A.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/netbios.py#L7-L39" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class netbios(Unit):
    &#34;&#34;&#34;
    Encodes and decodes strings using the same algorithm that is used for NetBIOS
    labels. Each byte 0xUL is encoded as two bytes, which are the sum of 0xU and
    0xL with an offset character, respectively. The default offset is the capital
    letter A.
    &#34;&#34;&#34;

    def __init__(self, key: Param[buf, Arg(help=&#34;Provide a single letter to use as the offset.&#34;)] = B&#39;A&#39;):
        if len(key) != 1:
            raise ValueError(&#34;The key must be a binary string of length exactly 1&#34;)
        super().__init__(key=key[0])

    def reverse(self, data):
        result = bytearray(2 * len(data))
        for k, byte in enumerate(data):
            hi, lo = byte &gt;&gt; 4, byte &amp; 15
            result[2 * k + 0] = hi + self.args.key
            result[2 * k + 1] = lo + self.args.key
        return result

    def process(self, data):
        def merge(it):
            while True:
                try:
                    hi = next(it) - self.args.key
                    lo = next(it) - self.args.key
                    if hi not in range(16) or lo not in range(16):
                        raise ValueError(F&#39;Invalid character encoding detected: hi={hi:X}, lo={lo:X}.&#39;)
                    yield (hi &lt;&lt; 4) | lo
                except StopIteration:
                    break
        return bytearray(merge(iter(data)))</code></pre>
</details>
</dd>
<dt id="refinery.shell.ngrams"><code class="flex name class">
<span>class <span class="ident">ngrams</span></span>
<span>(</span><span>size=slice(2, None, None))</span>
</code></dt>
<dd>
<section class="desc"><p>Extract all n-grams from the input. The algorithm is naive, i.e. it simply iterates all n-grams
and deduplicates using a set data structure. The number n is taken from an arbitrary range given
as a Python slice expression.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/strings/ngrams.py#L8-L32" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ngrams(Unit):
    &#34;&#34;&#34;
    Extract all n-grams from the input. The algorithm is naive, i.e. it simply iterates all n-grams
    and deduplicates using a set data structure. The number n is taken from an arbitrary range given
    as a Python slice expression.
    &#34;&#34;&#34;
    def __init__(
        self, size: Param[slice, Arg.Bounds(
            help=&#39;Specifies the sizes of each n-gram, i.e. the number n. Defaults to {default}.&#39;)] = slice(2, None),
    ):
        super().__init__(size=size)

    def process(self, data: bytearray):
        for n in integers_of_slice(self.args.size):
            self.log_info(F&#39;emitting {n}-grams&#39;)
            if n &gt; len(data):
                break
            deduplicator = set()
            view = memoryview(data)
            for index in range(len(data) - n + 1):
                block = bytes(view[index:index + n])
                if block in deduplicator:
                    continue
                deduplicator.add(block)
                yield self.labelled(block, offset=index)</code></pre>
</details>
</dd>
<dt id="refinery.shell.nop"><code class="flex name class">
<span>class <span class="ident">nop</span></span>
</code></dt>
<dd>
<section class="desc"><p>The unit generates the exact output that was received as input. All unknown arguments passed
to nop are completely ignored, which is different from the behavior of other units. As such,
nop can be used to comment out other units in longer refinery pipelines by simply prefixing a
command with nop.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/misc/nop.py#L14-L26" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class nop(Unit):
    &#34;&#34;&#34;
    The unit generates the exact output that was received as input. All unknown arguments passed
    to nop are completely ignored, which is different from the behavior of other units. As such,
    nop can be used to comment out other units in longer refinery pipelines by simply prefixing a
    command with nop.
    &#34;&#34;&#34;
    @classmethod
    def argparser(cls, **keywords):
        argp = NopArgParser(
            keywords, prog=cls.name, description=documentation(cls), add_help=False)
        argp.set_defaults(nesting=0)
        return cls._interface(argp)</code></pre>
</details>
</dd>
<dt id="refinery.shell.nrv2b"><code class="flex name class">
<span>class <span class="ident">nrv2b</span></span>
<span>(</span><span>bits=32)</span>
</code></dt>
<dd>
<section class="desc"><p>Decompress data using the NRV2B algorithm.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/nrv.py#L36-L64" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class nrv2b(NRVUnit):
    &#34;&#34;&#34;
    Decompress data using the NRV2B algorithm.
    &#34;&#34;&#34;
    def _decompress(self, src: StructReader, dst: MemoryFile, bb: BitBufferedReader):
        last_offset = 1
        while not src.eof:
            while next(bb):
                dst.write_byte(src.read_byte())
            offset = 2 + next(bb)
            while not next(bb):
                offset = 2 * offset + next(bb)
            if offset == 2:
                offset = last_offset
            else:
                offset = (offset - 3) * 0x100 + src.read_byte()
                if offset &amp; 0xFFFFFFFF == 0xFFFFFFFF:
                    break
                offset += 1
                last_offset = offset
            length = next(bb)
            length = 2 * length + next(bb)
            if length == 0:
                length = 2 + next(bb)
                while not next(bb):
                    length = 2 * length + next(bb)
                length += 2
            length += int(bool(offset &gt; 0xD00))
            dst.replay(offset, length + 1)</code></pre>
</details>
</dd>
<dt id="refinery.shell.nrv2d"><code class="flex name class">
<span>class <span class="ident">nrv2d</span></span>
<span>(</span><span>bits=32)</span>
</code></dt>
<dd>
<section class="desc"><p>Decompress data using the NRV2D algorithm.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/nrv.py#L67-L97" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class nrv2d(NRVUnit):
    &#34;&#34;&#34;
    Decompress data using the NRV2D algorithm.
    &#34;&#34;&#34;
    def _decompress(self, src: StructReader, dst: MemoryFile, bb: BitBufferedReader):
        last_offset = 1
        while not src.eof:
            while next(bb):
                dst.write_byte(src.read_byte())
            offset = 2 + next(bb)
            while not next(bb):
                offset = 2 * (offset - 1) + next(bb) # noqa
                offset = 2 *  offset      + next(bb) # noqa
            if offset == 2:
                offset = last_offset
                length = next(bb)
            else:
                offset = (offset - 3) * 0x100 + src.read_byte()
                if offset &amp; 0xFFFFFFFF == 0xFFFFFFFF:
                    break
                length = (offset  ^ 1) &amp; 1 # noqa
                offset = (offset &gt;&gt; 1) + 1
                last_offset = offset
            length = 2 * length + next(bb)
            if length == 0:
                length = 2 + next(bb)
                while not next(bb):
                    length = 2 * length + next(bb)
                length += 2
            length += int(bool(offset &gt; 0x500))
            dst.replay(offset, length + 1)</code></pre>
</details>
</dd>
<dt id="refinery.shell.nrv2e"><code class="flex name class">
<span>class <span class="ident">nrv2e</span></span>
<span>(</span><span>bits=32)</span>
</code></dt>
<dd>
<section class="desc"><p>Decompress data using the NRV2E algorithm.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/nrv.py#L100-L133" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class nrv2e(NRVUnit):
    &#34;&#34;&#34;
    Decompress data using the NRV2E algorithm.
    &#34;&#34;&#34;
    def _decompress(self, src: StructReader, dst: MemoryFile, bb: BitBufferedReader):
        last_offset = 1
        while not src.eof:
            while next(bb):
                dst.write_byte(src.read_byte())
            offset = 2 + next(bb)
            while not next(bb):
                offset = 2 * (offset - 1) + next(bb) # noqa
                offset = 2 *  offset      + next(bb) # noqa
            if offset == 2:
                offset = last_offset
                length = next(bb)
            else:
                offset = (offset - 3) * 0x100 + src.read_byte()
                if offset &amp; 0xFFFFFFFF == 0xFFFFFFFF:
                    break
                length = (offset ^  1) &amp; 1 # noqa
                offset = (offset &gt;&gt; 1) + 1
                last_offset = offset
            if length:
                length = 1 + next(bb)
            elif next(bb):
                length = 3 + next(bb)
            else:
                length = 2 + next(bb)
                while not next(bb):
                    length = 2 * length + next(bb)
                length += 3
            length += int(bool(offset &gt; 0x500))
            dst.replay(offset, length + 1)</code></pre>
</details>
</dd>
<dt id="refinery.shell.ntlm"><code class="flex name class">
<span>class <span class="ident">ntlm</span></span>
<span>(</span><span>reps=1, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the Windows NTLM hash of the input.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/hash/password_hashes.py#L11-L17" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ntlm(HashUnit):
    &#34;&#34;&#34;
    Returns the Windows NTLM hash of the input.
    &#34;&#34;&#34;
    def _algorithm(self, data) -&gt; bytes:
        from Cryptodome.Hash import MD4
        return MD4.new(codecs.decode(data, self.codec).encode(&#39;utf-16le&#39;)).digest()</code></pre>
</details>
</dd>
<dt id="refinery.shell.officecrypt"><code class="flex name class">
<span>class <span class="ident">officecrypt</span></span>
<span>(</span><span>password=b'VelvetSweatshop')</span>
</code></dt>
<dd>
<section class="desc"><p>A simple proxy for the <code>msoffcrypto</code> package to decrypt office documents.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/office/officecrypt.py#L8-L34" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class officecrypt(Unit):
    &#34;&#34;&#34;
    A simple proxy for the `msoffcrypto` package to decrypt office documents.
    &#34;&#34;&#34;

    def __init__(self, password: Param[buf, Arg.Binary(help=(
        &#39;The document password. By default, the Excel default password &#34;{default}&#34; is used.&#39;
    ))] = b&#39;VelvetSweatshop&#39;):
        super().__init__(password=password)

    @Unit.Requires(&#39;msoffcrypto-tool&#39;, [&#39;formats&#39;, &#39;office&#39;])
    def _msoffcrypto():
        import msoffcrypto
        return msoffcrypto

    def process(self, data):
        password: bytes = self.args.password
        with MemoryFile(data) as stream:
            doc = self._msoffcrypto.OfficeFile(stream)
            if not doc.is_encrypted():
                self.log_warn(&#39;the document is not encrypted; returning input&#39;)
                return data
            if password:
                doc.load_key(password=password.decode(self.codec))
            with MemoryFile(bytearray()) as output:
                doc.decrypt(output)
                return output.getvalue()</code></pre>
</details>
</dd>
<dt id="refinery.shell.opc"><code class="flex name class">
<span>class <span class="ident">opc</span></span>
<span>(</span><span>mode='x32', *, count=None, until=None, nvar='name', avar='addr', ovar='arg')</span>
</code></dt>
<dd>
<section class="desc"><p>Disassembles the input data using capstone and generates opcodes with metadata as output. This
is useful for programmatic disassembly, while the <code><a title="refinery.asm" href="index.html#refinery.asm">asm</a></code> unit outputs a human-readable
representation. Internally, <code><a title="refinery.asm" href="index.html#refinery.asm">asm</a></code> uses this unit and pretty-prints the output.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/exe/opc.py#L16-L97" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class opc(Unit):
    &#34;&#34;&#34;
    Disassembles the input data using capstone and generates opcodes with metadata as output. This
    is useful for programmatic disassembly, while the `refinery.asm` unit outputs a human-readable
    representation. Internally, `refinery.asm` uses this unit and pretty-prints the output.
    &#34;&#34;&#34;
    def __init__(
        self,
        mode: Param[str, Arg.Choice(
            help=&#39;Machine code architecture, default is {default}. Select from the following list: {choices}.&#39;,
            choices=_ARCHES, metavar=&#39;[x32|x64|..]&#39;)] = &#39;x32&#39;, *,
        count: Param[int, Arg.Number(&#39;-c&#39;, help=&#39;Maximum number of bytes to disassemble, infinite by default.&#39;)] = None,
        until: Param[str, Arg.String(&#39;-u&#39;, help=&#39;Disassemble until the given string appears among the disassembly.&#39;)] = None,
        nvar: Param[str, Arg.String(&#39;-n&#39;, help=(
            &#39;Variable to receive the disassembled mnemonic. Default is &#34;{default}&#34;.&#39;))] = &#39;name&#39;,
        avar: Param[str, Arg.String(&#39;-a&#39;, help=(
            &#39;Variable to receive the address of the instruction. Default is &#34;{default}&#34;.&#39;))] = &#39;addr&#39;,
        ovar: Param[str, Arg.String(&#39;-o&#39;, help=(
            &#39;Variable prefix for instruction operands. Default is &#34;{default}&#34;. The complete operand &#39;
            &#39;string will be in {default}s, the first argument in {default}1, the second in {default}2, &#39;
            &#39;and so on.&#39;))] = &#39;arg&#39;,
        **more
    ):
        super().__init__(
            mode=mode,
            count=count,
            until=until,
            nvar=nvar,
            avar=avar,
            ovar=ovar,
            **more)

    @property
    def _capstone_engine(self) -&gt; Cs:
        mode = self.args.mode.lower()
        init = {
            &#39;arm&#39;    : (cs.CS_ARCH_ARM, cs.CS_MODE_ARM),
            &#39;mips32&#39; : (cs.CS_ARCH_MIPS, cs.CS_MODE_MIPS32),
            &#39;mips64&#39; : (cs.CS_ARCH_MIPS, cs.CS_MODE_MIPS64),
            &#39;ppc32&#39;  : (cs.CS_ARCH_PPC, cs.CS_MODE_32),
            &#39;ppc64&#39;  : (cs.CS_ARCH_PPC, cs.CS_MODE_64),
            &#39;x16&#39;    : (cs.CS_ARCH_X86, cs.CS_MODE_16),
            &#39;x32&#39;    : (cs.CS_ARCH_X86, cs.CS_MODE_32),
            &#39;x64&#39;    : (cs.CS_ARCH_X86, cs.CS_MODE_64),
        }.get(mode)
        if init is not None:
            return cs.Cs(*init)
        raise AttributeError(F&#39;invalid mode: {mode}&#39;)

    def process(self, data):
        count = self.args.count or 0
        until = self.args.until
        nvar = self.args.nvar
        avar = self.args.avar
        ovar = self.args.ovar
        if isinstance(until, str):
            until = until.lower()
        for insn in self._capstone_engine.disasm(data, 0, count):
            kwargs = {
                avar: insn.address,
                nvar: insn.mnemonic,
            }
            ops: str = insn.op_str
            try:
                operands = [op.strip() for op in ops.split(&#39;,&#39;)]
            except Exception:
                operands = []
            else:
                kwargs[F&#39;{ovar}s&#39;] = ops
            for k, op in enumerate(operands, 1):
                if not op:
                    break
                try:
                    op = int(op, 0)
                except Exception:
                    pass
                kwargs[F&#39;{ovar}{k}&#39;] = op
            yield self.labelled(insn.bytes, **kwargs)
            if until is None:
                continue
            if until in ops.lower() or until in insn.mnemonic.lower():
                break</code></pre>
</details>
</dd>
<dt id="refinery.shell.p1"><code class="flex name class">
<span>class <span class="ident">p1</span></span>
</code></dt>
<dd>
<section class="desc"><p>A shortcut for <code><a title="refinery.pick" href="index.html#refinery.pick">pick</a></code> with the argument <code>0:1</code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/pick.py#L119-L124" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class p1(pick):
    &#34;&#34;&#34;
    A shortcut for `refinery.pick` with the argument `0:1`.
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__(slice(0, 1))</code></pre>
</details>
</dd>
<dt id="refinery.shell.p2"><code class="flex name class">
<span>class <span class="ident">p2</span></span>
</code></dt>
<dd>
<section class="desc"><p>A shortcut for <code><a title="refinery.pick" href="index.html#refinery.pick">pick</a></code> with the argument <code>0:2</code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/pick.py#L127-L132" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class p2(pick):
    &#34;&#34;&#34;
    A shortcut for `refinery.pick` with the argument `0:2`.
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__(slice(0, 2))</code></pre>
</details>
</dd>
<dt id="refinery.shell.p3"><code class="flex name class">
<span>class <span class="ident">p3</span></span>
</code></dt>
<dd>
<section class="desc"><p>A shortcut for <code><a title="refinery.pick" href="index.html#refinery.pick">pick</a></code> with the argument <code>0:3</code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/pick.py#L135-L140" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class p3(pick):
    &#34;&#34;&#34;
    A shortcut for `refinery.pick` with the argument `0:3`.
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__(slice(0, 3))</code></pre>
</details>
</dd>
<dt id="refinery.shell.pack"><code class="flex name class">
<span>class <span class="ident">pack</span></span>
<span>(</span><span>base=0, prefix=False, strict=False, width=0, single_floats=False, double_floats=False, bigendian=False, blocksize=1)</span>
</code></dt>
<dd>
<section class="desc"><p>Scans the input data for numeric constants and packs them into a binary format. This is useful
to convert the textual representation of an array of numbers into its binary form. For example,
<code>123,34,256,12,1,234</code> would be transformed into the byte sequence <code>7B22000C01EA</code>, where <code>256</code>
was wrapped and packed as a null byte because the default block size is one byte. If the above
sequence would be packed with options -EB2, the result is <code>007B00220100000C000100EA</code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/blockwise/pack.py#L21-L152" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pack(BlockTransformationBase):
    &#34;&#34;&#34;
    Scans the input data for numeric constants and packs them into a binary format. This is useful
    to convert the textual representation of an array of numbers into its binary form. For example,
    `123,34,256,12,1,234` would be transformed into the byte sequence `7B22000C01EA`, where `256`
    was wrapped and packed as a null byte because the default block size is one byte. If the above
    sequence would be packed with options -EB2, the result is `007B00220100000C000100EA`.
    &#34;&#34;&#34;

    def __init__(self,
        base: Param[int, Arg.Number(bound=(2, 36), help=(
            &#39;Find only numbers in given base. Default of 0 means that common expressions for &#39;
            &#39;hexadecimal, octal and binary are accepted.&#39;))] = 0,
        prefix: Param[bool, Arg.Switch(&#39;-r&#39;, group=&#39;FLT&#39;,
            help=&#39;Add numeric prefixes like 0x, 0b, and 0o in reverse mode.&#39;)] = False,
        strict: Param[bool, Arg.Switch(&#39;-s&#39;,
            help=&#39;Only parse integers that fit in one block of the given block size.&#39;)] = False,
        width: Param[int, Arg.Number(&#39;-w&#39;,
            help=&#39;Pad numbers with the specified amount of leading zeros.&#39;)] = 0,
        single_floats: Param[bool, Arg.Switch(&#39;-f&#39;, group=&#39;FLT&#39;,
            help=&#39;Pack single-precision floating-point numbers. Implies -B4.&#39;)] = False,
        double_floats: Param[bool, Arg.Switch(&#39;-d&#39;, group=&#39;FLT&#39;,
            help=&#39;Pack double-precision floating-point numbers. Implies -B8.&#39;)] = False,
        bigendian=False, blocksize=1
    ):
        if single_floats and double_floats:
            raise ValueError(&#39;The floats and doubles option are mutually exclusive.&#39;)
        elif single_floats:
            fmode = FMode.SINGLE
            blocksize = 4
        elif double_floats:
            fmode = FMode.DOUBLE
            blocksize = 8
        else:
            fmode = FMode.TO_INT
        super().__init__(
            base=base,
            prefix=prefix,
            strict=strict,
            width=width,
            bigendian=bigendian,
            blocksize=blocksize,
            fmode=fmode,
            _truncate=2,
        )

    @property
    def bytestream(self):
        # never alow bytes to be left unchunked
        return False

    def reverse(self, data):
        base = self.args.base or 10
        width = self.args.width
        mode: FMode = self.args.fmode
        prefix = B&#39;&#39;

        self.log_debug(F&#39;using base {base:d}&#39;)

        if self.args.prefix:
            prefix = {
                0x02: b&#39;0b&#39;,
                0x08: b&#39;0o&#39;,
                0x10: b&#39;0x&#39;
            }.get(base, B&#39;&#39;)

        if mode is FMode.TO_INT:
            converter = BaseUnit(
                base,
                little_endian=not self.args.bigendian,
                strip_padding=True,
            )
            for n in self.chunk_into_bytes(data):
                converted = converter.reverse(n)
                if width:
                    converted = converted.rjust(width, B&#39;0&#39;)
                if prefix:
                    converted = prefix + converted
                yield converted
            return

        elif mode is FMode.SINGLE:
            float_format = &#39;f&#39;
            float_size = 4

        elif mode is FMode.DOUBLE:
            float_format = &#39;d&#39;
            float_size = 8

        count, rest = divmod(len(data), float_size)
        if rest:
            self.log_warn(F&#39;data contained {rest} trailing bytes that were ignored&#39;)
            data = memoryview(data)[:-rest]
        float_format *= count
        if self.args.bigendian:
            float_format = F&#39;&gt;{float_format}&#39;
        else:
            float_format = F&#39;&lt;{float_format}&#39;
        for n in struct.unpack(float_format, data):
            yield str(n).encode(self.codec)

    def process(self, data):
        base: int = self.args.base
        strict: bool = self.args.strict
        mode: FMode = self.args.fmode
        ep = &#39;&gt;&#39; if self.args.bigendian else &#39;&lt;&#39;

        def evaluate_literals(literals: Iterable[bytes]):
            for literal in literals:
                if mode is FMode.TO_INT:
                    if base == 0 and literal[0] == 0x30 and literal[1:].isdigit():
                        literal = B&#39;0o%s&#39; % literal
                    N = int(literal, base)
                elif mode is FMode.SINGLE:
                    N, = struct.unpack(F&#39;{ep}I&#39;, struct.pack(F&#39;{ep}f&#39;, float(literal)))
                elif mode is FMode.DOUBLE:
                    N, = struct.unpack(F&#39;{ep}Q&#39;, struct.pack(F&#39;{ep}d&#39;, float(literal)))
                else:
                    raise TypeError(&#39;unexpected floating point mode&#39;)
                M = N &amp; self.fmask
                if strict and M != N:
                    continue
                yield M

        if base == 0:
            pattern = formats.number
        elif base &lt;= 10:
            pattern = re.compile(B&#39;[-+]?[0-%d]{1,64}&#39; % (base - 1))
        else:
            pattern = re.compile(B&#39;[-+]?[0-9a-%c]{1,20}&#39; % (0x57 + base), re.IGNORECASE)

        return self.unchunk(evaluate_literals(m[0] for m in pattern.finditer(data)))</code></pre>
</details>
</dd>
<dt id="refinery.shell.pad"><code class="flex name class">
<span>class <span class="ident">pad</span></span>
<span>(</span><span>width, padding=b'\x00', left=False, absolute=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Allows padding of the input data.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/pad.py#L7-L40" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pad(Unit):
    &#34;&#34;&#34;
    Allows padding of the input data.
    &#34;&#34;&#34;

    def __init__(
        self,
        width: Param[int, Arg.Number(help=&#39;Input is padded to the nearest multiple of this size.&#39;)],
        padding: Param[buf, Arg(help=(
            &#39;This custom binary sequence is used (repeatedly, if necessary) to pad the &#39;
            &#39;input. The default is a zero byte.&#39;))] = B&#39;\0&#39;,
        left: Param[bool, Arg.Switch(&#39;-l&#39;, help=&#39;Pad on the left instead of the right.&#39;)] = False,
        absolute: Param[bool, Arg.Switch(&#39;-a&#39;, help=(
            &#39;The width argument specifies an absolute size, not a block size.&#39;))] = False
    ):
        super().__init__(width=width, padding=padding, left=left, absolute=absolute)

    def process(self, data):
        width = self.args.width
        if self.args.absolute and len(data) &gt;= width:
            return data
        q, r = divmod(len(data), width)
        size = (q + bool(r)) * width
        missing = (size - len(data))
        if missing &lt;= 0:
            return data
        pad = self.args.padding
        if missing &gt; len(pad):
            pad *= missing // len(pad)
        if self.args.left:
            return pad[:missing] + data
        else:
            data += pad[:missing]
            return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.pbkdf1"><code class="flex name class">
<span>class <span class="ident">pbkdf1</span></span>
<span>(</span><span>size, salt=b'\x00\x00\x00\x00\x00\x00\x00\x00', iter=1000, hash='SHA1')</span>
</code></dt>
<dd>
<section class="desc"><p>PBKDF1 Key derivation</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/keyderive/pbkdf1.py#L6-L17" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pbkdf1(KeyDerivation):
    &#34;&#34;&#34;PBKDF1 Key derivation&#34;&#34;&#34;

    @Arg(&#39;salt&#39;, help=&#39;Salt for the derivation; default are 8 null bytes.&#39;)
    def __init__(self, size, salt=bytes(8), iter=1000, hash=&#39;SHA1&#39;):
        self.superinit(super(), **vars())

    def process(self, data):
        from Cryptodome.Protocol.KDF import PBKDF1
        return multidecode(data, lambda pwd: (
            PBKDF1(pwd, self.args.salt, dkLen=self.args.size, count=self.args.iter, hashAlgo=self.hash)
        ))</code></pre>
</details>
</dd>
<dt id="refinery.shell.pbkdf2"><code class="flex name class">
<span>class <span class="ident">pbkdf2</span></span>
<span>(</span><span>size, salt, iter=1000, hash='SHA1')</span>
</code></dt>
<dd>
<section class="desc"><p>PBKDF2 Key derivation. This is implemented as Rfc2898DeriveBytes in .NET
binaries.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/keyderive/pbkdf2.py#L8-L25" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pbkdf2(KeyDerivation):
    &#34;&#34;&#34;
    PBKDF2 Key derivation. This is implemented as Rfc2898DeriveBytes in .NET
    binaries.
    &#34;&#34;&#34;

    def __init__(self, size, salt, iter=1000, hash=&#39;SHA1&#39;):
        self.superinit(super(), **vars())

    def process(self, data):
        from Cryptodome.Protocol.KDF import PBKDF2
        return multidecode(data, partial(
            PBKDF2,
            salt=self.args.salt,
            dkLen=self.args.size,
            hmac_hash_module=self.hash,
            count=self.args.iter
        ))</code></pre>
</details>
</dd>
<dt id="refinery.shell.pbuf"><code class="flex name class">
<span>class <span class="ident">pbuf</span></span>
<span>(</span><span>try_repeated=False, encode=None, digest=None, arrays=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Converts a ProtoBuf message to JSON. Deserialization is ambiguous without the definition file,
so the output is partly based on heuristics. Some fields like fixed integers are never recovered, fixed 32-bit
and 64-bit data types are always recovered as floating point numbers. For variable length data,
the unit first attempts to decode the data as a printable UTF-8 string. If this fails, it will
attempt to deserialize it as ProtoBuf. If this also fails and the corresponding option is set,
it will try to reconstruct a sequence of repeated variable-length integers. The final fallback
is to return the body as a byte string.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pbuf.py#L175-L204" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pbuf(JSONEncoderUnit):
    &#34;&#34;&#34;
    Converts a ProtoBuf message to JSON. Deserialization is ambiguous without the definition file,
    so the output is partly based on heuristics. Some fields like fixed integers are never recovered, fixed 32-bit
    and 64-bit data types are always recovered as floating point numbers. For variable length data,
    the unit first attempts to decode the data as a printable UTF-8 string. If this fails, it will
    attempt to deserialize it as ProtoBuf. If this also fails and the corresponding option is set,
    it will try to reconstruct a sequence of repeated variable-length integers. The final fallback
    is to return the body as a byte string.
    &#34;&#34;&#34;
    def __init__(
        self,
        try_repeated: Param[bool, Arg.Switch(&#39;-r&#39;,
            help=&#39;Try to detect and decode repeated integer fields.&#39;)] = False,
        encode=None,
        digest=None,
        arrays=False,
    ):
        super().__init__(
            encode=encode,
            digest=digest,
            arrays=arrays,
            try_repeated=try_repeated
        )

    def process(self, data):
        reader = ProtoBufReader(memoryview(data))
        reader.try_repeated = self.args.try_repeated
        message = reader.read_message()
        return self.to_json(message)</code></pre>
</details>
</dd>
<dt id="refinery.shell.pcap"><code class="flex name class">
<span>class <span class="ident">pcap</span></span>
<span>(</span><span>merge=False, client=False, server=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Performs TCP stream reassembly from packet capture (PCAP) files. By default, the unit emits the parts of
each TCP conversation, attaching several pieces of metadata to each such output: Included are the source
and destination socket address as well as the variable <code>stream</code> which identifies the conversation which
it was part of. The chunks are returned in the order that the bytes were exchanged between source and
destination. When the <code>--merge</code> parameter is specified, the unit instead collects all bytes going forward
and backwards, respectively, and emitting these as two chunks, for each TCP conversation that took place.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pcap.py#L61-L187" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pcap(Unit):
    &#34;&#34;&#34;
    Performs TCP stream reassembly from packet capture (PCAP) files. By default, the unit emits the parts of
    each TCP conversation, attaching several pieces of metadata to each such output: Included are the source
    and destination socket address as well as the variable `stream` which identifies the conversation which
    it was part of. The chunks are returned in the order that the bytes were exchanged between source and
    destination. When the `--merge` parameter is specified, the unit instead collects all bytes going forward
    and backwards, respectively, and emitting these as two chunks, for each TCP conversation that took place.
    &#34;&#34;&#34;

    def __init__(
        self,
        merge: Param[bool, Arg.Switch(&#39;-m&#39;, help=&#39;Merge both parts of each TCP conversation into one chunk.&#39;)] = False,
        client: Param[bool, Arg.Switch(&#39;-c&#39;, group=&#39;D&#39;, help=&#39;Show only the client part of each conversation.&#39;)] = False,
        server: Param[bool, Arg.Switch(&#39;-s&#39;, group=&#39;D&#39;, help=&#39;Show only the server part of each conversation.&#39;)] = False,
    ):
        super().__init__(merge=merge, client=client, server=server)

    @Unit.Requires(&#39;pypcapkit[scapy]&gt;=1.3&#39;, [&#39;all&#39;])
    def _pcapkit():
        with NoLogging():
            import importlib
            importlib.import_module(&#39;scapy.layers.tls.session&#39;)
            import pcapkit
            return pcapkit

    @Unit.Requires(&#39;scapy&#39;, [&#39;all&#39;])
    def _scapy():
        import scapy
        import scapy.packet
        return scapy

    def process(self, data):
        pcapkit = self._pcapkit
        merge = self.args.merge

        with NoLogging(), VirtualFileSystem() as fs:
            vf = VirtualFile(fs, data, &#39;pcap&#39;)
            pcap = pcapkit.extract(
                fin=vf.path,
                engine=&#39;scapy&#39;,
                store=True,
                nofile=True,
                extension=False,
                ip=True,
                tcp=True,
                reassembly=True,
                reasm_strict=True,
            )
            tcp: list[Datagram] = list(pcap.reassembly.tcp)
            tcp.sort(key=lambda p: min(p.index, default=0))

        count, convo = 0, None
        src_buffer = MemoryFile()
        dst_buffer = MemoryFile()

        self.log_debug(F&#39;extracted {len(pcap.frame)} packets, assembled {len(tcp)} datagrams&#39;)
        PT = self._scapy.packet

        def payload(packet: Packet):
            circle = set()
            while True:
                try:
                    inner = packet.payload
                except AttributeError:
                    break
                if isinstance(packet, PT.Raw) and not isinstance(packet, (PT.NoPayload, PT.Padding)):
                    return packet.original
                if id(inner) in circle:
                    break
                packet = inner
                circle.add(id(inner))
            return B&#39;&#39;

        def sequence(i: int):
            packet = pcap.frame[i - 1]
            while len(packet):
                try:
                    return packet.seq
                except AttributeError:
                    pass
                try:
                    packet = packet.payload
                except AttributeError:
                    break
            return 0

        client = self.args.client
        server = self.args.server

        def commit():
            if src_buffer.tell():
                if not server:
                    assert convo is not None
                    yield self.labelled(src_buffer.getvalue(), **convo.src_to_dst())
                src_buffer.truncate(0)
            if dst_buffer.tell():
                if not client:
                    assert convo is not None
                    yield self.labelled(dst_buffer.getvalue(), **convo.dst_to_src())
                dst_buffer.truncate(0)

        for datagram in tcp:
            self.log_info(datagram.header)

            this_convo = Conversation.FromID(datagram.id)
            if this_convo != convo:
                if count and merge:
                    yield from commit()
                count = count + 1
                convo = this_convo
            assert convo is not None
            data = bytearray()
            for index in sorted(datagram.index, key=sequence):
                data.extend(payload(pcap.frame[index - 1]))
            if not data:
                continue
            if not merge:
                yield self.labelled(data, **this_convo.src_to_dst(), stream=count)
            elif this_convo.src == convo.src:
                src_buffer.write(data)
            elif this_convo.dst == convo.src:
                dst_buffer.write(data)
            else:
                raise RuntimeError(F&#39;direction of packet {convo!s} in conversation {count} is unknown&#39;)

        yield from commit()</code></pre>
</details>
</dd>
<dt id="refinery.shell.pcap_http"><code class="flex name class">
<span>class <span class="ident">pcap_http</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts HTTP payloads from packet capture (PCAP) files.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pcap_http.py#L45-L85" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pcap_http(Unit):
    &#34;&#34;&#34;
    Extracts HTTP payloads from packet capture (PCAP) files.
    &#34;&#34;&#34;
    def process(self, data):
        http_parser = httpresponse()
        requests: list[_HTTP_Request] = []
        responses: list[bytearray] = []

        def lookup(src, dst):
            for k, request in enumerate(requests):
                if request.src == dst and request.dst == src:
                    requests.pop(k)
                    return self.labelled(data, url=request.url)
            return None

        for stream in data | pcap():
            try:
                data = http_parser.process(stream)
            except Exception:
                try:
                    rq = _parse_http_request(stream)
                    requests.append(rq)
                except _HTTPParseError as E:
                    self.log_info(F&#39;error parsing http request: {E!s}&#39;)
                except Exception:
                    pass
                continue
            if not data:
                continue
            src, dst = stream[&#39;src&#39;], stream[&#39;dst&#39;]
            item = lookup(src, dst)
            if item is None:
                responses.append((src, dst, data))
                continue
            yield item

        while responses:
            src, dst, data = responses.pop()
            item = lookup(src, dst)
            yield data if item is None else item</code></pre>
</details>
</dd>
<dt id="refinery.shell.pdfcrypt"><code class="flex name class">
<span>class <span class="ident">pdfcrypt</span></span>
<span>(</span><span>owner='', user='')</span>
</code></dt>
<dd>
<section class="desc"><p>The unit removes password protection from a PDF document. If the document is encrypted, either
the correct user or owner password must be specified to decrypt it. When the unit is operated
in reverse, the output is encrypted using the AES-256 mode.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pdfcrypt.py#L8-L65" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pdfcrypt(Unit):
    &#34;&#34;&#34;
    The unit removes password protection from a PDF document. If the document is encrypted, either
    the correct user or owner password must be specified to decrypt it. When the unit is operated
    in reverse, the output is encrypted using the AES-256 mode.
    &#34;&#34;&#34;

    @Unit.Requires(&#39;pymupdf&#39;, [&#39;formats&#39;, &#39;default&#39;, &#39;extended&#39;])
    def _mupdf():
        import os
        for setting in (&#39;PYMUPDF_MESSAGE&#39;, &#39;PYMUPDF_LOG&#39;):
            os.environ[setting] = F&#39;path:{os.devnull}&#39;
        import pymupdf
        import pymupdf.mupdf
        return pymupdf

    def __init__(
        self,
        owner: Param[str, Arg.String(&#39;-w&#39;, metavar=&#39;PWD&#39;, help=&#39;Optionally specify an owner password.&#39;)] = &#39;&#39;,
        user: Param[str, Arg.String(&#39;-u&#39;, metavar=&#39;PWD&#39;, help=&#39;Optionally specify a user password.&#39;)] = &#39;&#39;,
    ):
        super().__init__(user=user, owner=owner)

    def _ingest(self, data):
        pdf = self._mupdf.open(stream=data, filetype=&#39;pdf&#39;)
        given = 0
        if pdf.is_encrypted and (pwd := self.args.user):
            given += 1
            pdf.authenticate(pwd)
        if pdf.is_encrypted and (pwd := self.args.owner):
            given += 1
            pdf.authenticate(pwd)
        if pdf.is_encrypted:
            msg = {
                0: &#39;no password was specified&#39;,
                1: &#39;the given password was incorrect&#39;,
                2: &#39;neither of the given passwords worked&#39;
            }[given]
            raise ValueError(F&#39;The input data is encrypted and {msg}.&#39;)
        return pdf

    def process(self, data):
        with self._ingest(data) as pdf, MemoryFile() as out:
            pdf.save(out, encryption=self._mupdf.mupdf.PDF_ENCRYPT_NONE)
            return out.getvalue()

    def reverse(self, data):
        u = self.args.user
        w = self.args.owner
        if not u and not w:
            raise ValueError(&#39;Cannot encrypt document without a password.&#39;)
        with self._ingest(data) as pdf, MemoryFile() as out:
            pdf.save(out, encryption=self._mupdf.mupdf.PDF_ENCRYPT_AES_256, user_pw=u, owner_pw=w)
            return out.getvalue()

    @classmethod
    def handles(cls, data):
        return data[:5] == B&#39;%PDF-&#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.pecdb"><code class="flex name class">
<span>class <span class="ident">pecdb</span></span>
</code></dt>
<dd>
<section class="desc"><p>Short for "PE: Clear Dynamic Base"; this unit will clear the bit in the PE header that allows
for address space layout randomization. It will also set the integrity flag. With both bits
set, this DLL when loaded into memory will usually be loaded at its header-defined base address,
which can make debugging easier.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pe/pecdb.py#L6-L24" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pecdb(Unit):
    &#34;&#34;&#34;
    Short for &#34;PE: Clear Dynamic Base&#34;; this unit will clear the bit in the PE header that allows
    for address space layout randomization. It will also set the integrity flag. With both bits
    set, this DLL when loaded into memory will usually be loaded at its header-defined base address,
    which can make debugging easier.
    &#34;&#34;&#34;
    @Unit.Requires(&#39;pefile&#39;, [&#39;default&#39;, &#39;extended&#39;])
    def _pefile():
        import pefile
        return pefile

    def process(self, data: bytearray):
        pe = self._pefile.PE(data=data, fast_load=True)
        dc = pe.OPTIONAL_HEADER.DllCharacteristics
        dc = dc &amp; ~0x40 # IMAGE_DLLCHARACTERISTICS_DYNAMIC_BASE
        dc = dc &amp; +0x80 # IMAGE_DLLCHARACTERISTICS_FORCE_INTEGRITY
        pe.OPTIONAL_HEADER.DllCharacteristics = dc
        return pe.write()</code></pre>
</details>
</dd>
<dt id="refinery.shell.pedebloat"><code class="flex name class">
<span>class <span class="ident">pedebloat</span></span>
<span>(</span><span>*names, certificate=False, directories=False, memdump=False, resources=False, sections=False, trim_code=False, trim_rsrc=False, threshold=0.05, size_limit=10.0 MB, keep_limit=False, aggressive=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Removes junk or excess data from PE files and returns the stripped executable. By default, only
the PE overlay is considered; use the flags <code>-r</code> and <code>-s</code> to also consider resources and entire
sections. Any buffer is only considered for removal if it exceeds a certain size. If this
condition is met, a binary search is performed to determine the offset inside the buffer up to
which the compression ratio is above a certain threshold; everything beyond that point is then
removed. By setting the threshold compression ratio to 1, each large buffer is removed entirely.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pe/pedebloat.py#L30-L360" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pedebloat(OverlayUnit):
    &#34;&#34;&#34;
    Removes junk or excess data from PE files and returns the stripped executable. By default, only
    the PE overlay is considered; use the flags `-r` and `-s` to also consider resources and entire
    sections. Any buffer is only considered for removal if it exceeds a certain size. If this
    condition is met, a binary search is performed to determine the offset inside the buffer up to
    which the compression ratio is above a certain threshold; everything beyond that point is then
    removed. By setting the threshold compression ratio to 1, each large buffer is removed entirely.
    &#34;&#34;&#34;
    def __init__(
        self,
        *names: Param[str, Arg.String()],
        certificate=False,
        directories=False,
        memdump=False,
        resources: Param[bool, Arg.Switch(&#39;-r&#39;, help=&#39;Strip large resources.&#39;)] = False,
        sections: Param[bool, Arg.Switch(&#39;-s&#39;, help=&#39;Strip large sections.&#39;)] = False,
        trim_code: Param[bool, Arg.Switch(&#39;-X&#39;, help=&#39;Lift the exception on code sections for stripping.&#39;)] = False,
        trim_rsrc: Param[bool, Arg.Switch(&#39;-Y&#39;, help=&#39;Lift the exception on rsrc sections for stripping.&#39;)] = False,
        threshold: Param[float, Arg.Double(&#39;-t&#39;, metavar=&#39;T&#39;, help=(
            &#39;Trailing data from resources and sections is stripped until the compression ratio &#39;
            &#39;of the remaining data rises above this threshold. The default value is {default}. &#39;
            &#39;Set this to 1 to ignore the limit entirely and trim every structure as much as &#39;
            &#39;possible without violating alignment. Setting this value to 0 will only strip repeated &#39;
            &#39;occurrences of the last byte.&#39;))] = 0.05,
        size_limit: Param[int, Arg.Number(&#39;-l&#39;, help=(
            &#39;Structures below this size are not stripped. Default is {default!r}.&#39;))] = _STRIP,
        keep_limit: Param[bool, Arg.Switch(&#39;-k&#39;, help=(
            &#39;Do not strip structures to below the above size limit.&#39;))] = False,
        aggressive: Param[bool, Arg.Switch(&#39;-a&#39;, help=(
            &#39;Equivalent to -srt1: Strip large sections and resources aggressively.&#39;))] = False,
    ):
        if aggressive:
            sections = True
            resources = True
            threshold = 1

        super().__init__(
            certificate,
            directories,
            memdump,
            sections=sections,
            resources=resources,
            size_limit=size_limit,
            keep_limit=keep_limit,
            threshold=threshold,
            trim_rsrc=trim_rsrc,
            trim_code=trim_code,
            names=names,
        )

    @OverlayUnit.Requires(&#39;pefile&#39;, [&#39;default&#39;, &#39;extended&#39;])
    def _pefile():
        import pefile
        return pefile

    def _right_strip_data(self, data: memoryview, alignment=1, block_size=_MB) -&gt; int:
        if not data:
            return 0
        threshold = self.args.threshold
        data_overhang = len(data) % alignment
        result = data_overhang

        if 0 &lt; threshold &lt; 1:
            def compression_ratio(offset: int):
                ratio = len(zlib.compress(data[:offset], level=1)) / offset
                self.log_debug(F&#39;compressing {SizeInt(offset)!r} ratio={ratio:6.4f}&#39;)
                return ratio
            upper = len(data)
            lower = result
            if compression_ratio(upper) &lt;= threshold:
                while block_size &lt; upper - lower:
                    pivot = (lower + upper) // 2
                    ratio = compression_ratio(pivot)
                    if ratio &gt; threshold:
                        lower = pivot + 1
                        continue
                    upper = pivot
                    if abs(ratio - threshold) &lt; 1e-10:
                        break
            result = upper
        elif threshold == 0:
            result = len(data)
        elif threshold == 1:
            result = 0

        while result &gt; 1 and data[result - 2] == data[result - 1]:
            result -= 1

        result = max(result, data_overhang)

        if self.args.keep_limit:
            result = max(result, self.args.size_limit)

        result = result + (data_overhang - result) % alignment

        if result &gt; len(data):
            excess = result - len(data)
            excess = excess + (-excess % alignment)
            result = result - excess

        return result

    def _adjust_offsets(self, pe: PE, gap_offset: int, gap_size: int):
        base = pe.OPTIONAL_HEADER.ImageBase
        alignment = pe.OPTIONAL_HEADER.FileAlignment
        rva_offset = pe.get_rva_from_offset(gap_offset)
        tva_offset = rva_offset + base

        section = pe.get_section_by_offset(gap_offset)
        new_section_size = section.SizeOfRawData - gap_size
        if new_section_size % alignment != 0:
            raise RuntimeError(
                F&#39;trimming 0x{gap_size:X} bytes from section {_ASCII(section.Name)} of size 0x{section.SizeOfRawData:X} &#39;
                F&#39;violates required section alignment of 0x{alignment:X} bytes&#39;)
        inside_section_offset = gap_offset - section.PointerToRawData
        if inside_section_offset &gt; new_section_size:
            overlap = inside_section_offset - new_section_size
            raise RuntimeError(F&#39;trimming from section {_ASCII(section.Name)}; data extends {overlap} beyond section&#39;)

        rva_lbound = section.VirtualAddress
        rva_ubound = section.VirtualAddress + section.Misc_VirtualSize - 1
        tva_lbound = rva_lbound + base
        tva_ubound = rva_ubound + base

        def adjust_attributes_of_structure(
            structure: Structure,
            gap_offset: int,
            valid_values_lower_bound: int | None,
            valid_values_upper_bound: int | None,
            attributes: Iterable[str]
        ):
            for attribute in attributes:
                old_value = getattr(structure, attribute, 0)
                if old_value &lt;= gap_offset:
                    continue
                if valid_values_lower_bound is not None and old_value &lt; valid_values_lower_bound:
                    continue
                if valid_values_upper_bound is not None and old_value &gt; valid_values_upper_bound:
                    continue
                new_value = old_value - gap_size
                if new_value &lt; gap_offset:
                    raise BrokenLink(F&#39;attribute {attribute} points into removed region&#39;)
                self.log_debug(F&#39;adjusting field in {structure.name}: {attribute}&#39;)
                setattr(structure, attribute, new_value)

        it: Iterable[Structure] = iter(pe.__structures__)
        structure_class = self._pefile.SectionStructure
        remove = []

        for index, structure in enumerate(it):
            old_offset = structure.get_file_offset()
            new_offset = old_offset - gap_offset

            if old_offset &gt; gap_offset:
                if old_offset &lt; gap_offset + gap_size:
                    self.log_debug(F&#39;removing structure {structure.name}; starts inside removed region&#39;)
                    remove.append(index)
                    continue
                if isinstance(structure, structure_class) and new_offset % alignment != 0:
                    raise RuntimeError(
                        F&#39;structure {structure.name} would be moved to offset 0x{new_offset:X}, &#39;
                        F&#39;violating section alignment value 0x{alignment:X}.&#39;)
                structure.set_file_offset(new_offset)

            try:
                adjust_attributes_of_structure(structure, rva_offset, rva_lbound, rva_ubound, (
                    &#39;OffsetToData&#39;,
                    &#39;AddressOfData&#39;,
                    &#39;VirtualAddress&#39;,
                    &#39;AddressOfNames&#39;,
                    &#39;AddressOfNameOrdinals&#39;,
                    &#39;AddressOfFunctions&#39;,
                    &#39;AddressOfEntryPoint&#39;,
                    &#39;AddressOfRawData&#39;,
                    &#39;BaseOfCode&#39;,
                    &#39;BaseOfData&#39;,
                ))
                adjust_attributes_of_structure(structure, tva_offset, tva_lbound, tva_ubound, (
                    &#39;StartAddressOfRawData&#39;,
                    &#39;EndAddressOfRawData&#39;,
                    &#39;AddressOfIndex&#39;,
                    &#39;AddressOfCallBacks&#39;,
                ))
                adjust_attributes_of_structure(structure, gap_offset, None, None, (
                    &#39;OffsetModuleName&#39;,
                    &#39;PointerToRawData&#39;,
                ))
            except BrokenLink as error:
                self.log_debug(F&#39;removing structure {structure.name}; {error!s}&#39;)
                remove.append(index)
                continue

            for attribute in (
                &#39;CvHeaderOffset&#39;,
                &#39;OffsetIn2Qwords&#39;,
                &#39;OffsetInQwords&#39;,
                &#39;Offset&#39;,
                &#39;OffsetLow&#39;,
                &#39;OffsetHigh&#39;
            ):
                if not hasattr(structure, attribute):
                    continue
                self.log_warn(F&#39;potential offset in structure {structure.name} ignored: {attribute}&#39;)

        while remove:
            index = remove.pop()
            pe.__structures__[index:index + 1] = []

        section.SizeOfRawData = new_section_size

    def _trim_sections(self, pe: PE, data: bytearray) -&gt; int:
        S = self.args.size_limit
        P = self.args.names
        trimmed = 0
        for section in pe.sections:
            section: SectionStructure
            offset = section.PointerToRawData
            name = _ASCII(section.Name)
            if not self.args.trim_code and name.lower() in (&#39;.text&#39;, &#39;.code&#39;):
                self.log_debug(F&#39;skipping code section {name}; specify --trim-code to override.&#39;)
                continue
            if not self.args.trim_rsrc and name.lower() == &#39;.rsrc&#39;:
                self.log_debug(F&#39;skipping rsrc section {name}; specify --trim-rsrc to override.&#39;)
                continue
            old_size = section.SizeOfRawData
            if old_size &lt;= S and not any(fnmatch(name, p) for p in P):
                self.log_debug(F&#39;criteria not satisfied for section: {SizeInt(old_size)!r} {name}&#39;)
                continue
            new_size = self._right_strip_data(
                memoryview(data)[offset:offset + old_size],
                pe.OPTIONAL_HEADER.FileAlignment)
            if new_size == old_size:
                continue
            self.log_info(F&#39;stripping section {name} from {TI(old_size)!r} to {TI(new_size)!r}&#39;)
            gap_size = old_size - new_size
            gap_offset = offset + new_size
            if gap_size &lt;= 0:
                continue
            self._adjust_offsets(pe, gap_offset, gap_size)
            trimmed += gap_size
            data[gap_offset:gap_offset + gap_size] = []
        return trimmed

    def _trim_pe_resources(self, pe: PE, data: bytearray) -&gt; int:
        S = self.args.size_limit
        P = self.args.names
        trimmed = 0

        def find_bloated_resources(pe: PE, directory, level: int = 0, *path) -&gt; Generator[Structure]:
            for entry in directory.entries:
                name = getattr(entry, &#39;name&#39;)
                numeric = getattr(entry, &#39;id&#39;)
                if not name:
                    if level == 0 and numeric in iter(RSRC):
                        name = RSRC(entry.id)
                    elif numeric is not None:
                        name = str(numeric)
                name = name and str(name) or &#39;?&#39;
                if entry.struct.DataIsDirectory:
                    yield from find_bloated_resources(pe, entry.directory, level + 1, *path, name)
                    continue
                struct: Structure = entry.data.struct
                name = &#39;/&#39;.join((*path, name))
                if struct.Size &lt;= S and not any(fnmatch(name, p) for p in P):
                    self.log_debug(F&#39;criteria not satisfied for resource: {SizeInt(struct.Size)!r} {name}&#39;)
                    continue
                yield name, struct

        RSRC_INDEX = self._pefile.DIRECTORY_ENTRY[&#39;IMAGE_DIRECTORY_ENTRY_RESOURCE&#39;]
        pe.parse_data_directories(directories=[RSRC_INDEX])

        try:
            resources = pe.DIRECTORY_ENTRY_RESOURCE
        except AttributeError:
            return 0
        for name, resource in find_bloated_resources(pe, resources):
            offset = pe.get_offset_from_rva(resource.OffsetToData)
            old_size = resource.Size
            new_size = self._right_strip_data(
                memoryview(data)[offset:offset + old_size],
                pe.OPTIONAL_HEADER.FileAlignment)
            self.log_info(F&#39;stripping resource {name} from {old_size} to {new_size}&#39;)
            gap_size = old_size - new_size
            gap_offset = offset + new_size
            if gap_size &lt;= 0:
                continue
            resource.Size = new_size
            self._adjust_offsets(pe, gap_offset, gap_size)
            trimmed += gap_size
            data[gap_offset:gap_offset + gap_size] = []

        pe.OPTIONAL_HEADER.DATA_DIRECTORY[RSRC_INDEX].Size -= trimmed
        self.log_info(F&#39;trimming size of resource data directory by {TI(trimmed)!r}&#39;)
        return trimmed

    def process(self, data: bytearray) -&gt; bytearray:
        overlay_offset = self._get_size(data)
        if len(data) - overlay_offset &gt;= self.args.size_limit:
            view = memoryview(data)
            overlay_length = self._right_strip_data(view[overlay_offset:])
            body_size = overlay_offset + overlay_length
            try:
                data[body_size:] = []
            except Exception:
                data = data[:body_size]
        if not self.args.resources and not self.args.sections:
            return data
        pe = self._pefile.PE(data=data, fast_load=True)
        total = len(data)
        trimmed = 0
        view = pe.__data__
        copy = False
        if not isinstance(view, bytearray):
            view = memoryview(view)
            try:
                view[0] = 0x4D
            except Exception:
                copy = True
                view = bytearray(pe.__data__)
        if self.args.resources:
            trimmed += self._trim_pe_resources(pe, view)
        if self.args.sections:
            trimmed += self._trim_sections(pe, view)
        if copy:
            pe.__data__ = view
        data = pe.write()
        end = total - trimmed
        if end &lt; len(data):
            self.log_warn(F&#39;output contains {len(data) - end} trailing bytes&#39;)
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.peek"><code class="flex name class">
<span>class <span class="ident">peek</span></span>
<span>(</span><span>lines=10, all=False, brief=False, decode=0, escape=False, bare=False, meta=0, gray=False, index=False, stdout=False, narrow=False, blocks=1, dense=False, expand=False, width=0)</span>
</code></dt>
<dd>
<section class="desc"><p>The unit extracts preview information of the input data and displays it on the standard error stream. If the standard
output of this unit is connected by a pipe, the incoming data is forwarded. However, if the unit outputs to a terminal,
the data is discarded instead.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/sinks/peek.py#L26-L339" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class peek(HexViewer):
    &#34;&#34;&#34;
    The unit extracts preview information of the input data and displays it on the standard error stream. If the standard
    output of this unit is connected by a pipe, the incoming data is forwarded. However, if the unit outputs to a terminal,
    the data is discarded instead.
    &#34;&#34;&#34;

    def __init__(
        self,
        lines: Param[int, Arg.Number(&#39;-l&#39;, group=&#39;SIZE&#39;, help=&#39;Specify number N of lines in the preview, default is 10.&#39;)] = 10,
        all: Param[bool, Arg.Switch(&#39;-a&#39;, group=&#39;SIZE&#39;, help=&#39;Output all possible preview lines without restriction&#39;)] = False,
        brief: Param[bool, Arg.Switch(&#39;-b&#39;, group=&#39;SIZE&#39;, help=&#39;One line peek, implies --lines=1.&#39;)] = False,
        decode: Param[int, Arg.Counts(&#39;-d&#39;, group=&#39;MODE&#39;, help=(
            &#39;Attempt to decode and display printable data. Specify twice to enable line wrapping.&#39;))] = 0,
        escape: Param[bool, Arg.Switch(&#39;-e&#39;, group=&#39;MODE&#39;, help=&#39;Always peek data as string, escape characters if necessary.&#39;)] = False,
        bare: Param[bool, Arg.Switch(&#39;-r&#39;, group=&#39;META&#39;, help=&#39;Only peek the data itself, do not show a metadata preview.&#39;)] = False,
        meta: Param[int, Arg.Counts(&#39;-m&#39;, group=&#39;META&#39;, help=(
            &#39;Show more auto-derivable metadata. Specify multiple times to populate more variables.&#39;))] = 0,
        gray: Param[bool, Arg.Switch(&#39;-g&#39;, help=&#39;Do not colorize the output.&#39;)] = False,
        index: Param[bool, Arg.Switch(&#39;-i&#39;, help=&#39;Display the index of each chunk within the current frame.&#39;)] = False,
        stdout: Param[bool, Arg.Switch(&#39;-2&#39;, help=&#39;Print the peek to STDOUT rather than STDERR; the input data is lost.&#39;)] = False,
        narrow=False, blocks=1, dense=False, expand=False, width=0
    ):
        if decode and escape:
            raise ValueError(&#39;The decode and esc options are exclusive.&#39;)
        if brief:
            narrow = True
        if environment.colorless.value:
            gray = True
        lines = 1 if brief else INF if all else lines
        super().__init__(
            brief=brief,
            gray=gray,
            blocks=blocks,
            decode=decode,
            dense=dense,
            index=index,
            escape=escape,
            expand=expand,
            narrow=narrow,
            lines=lines,
            meta=meta,
            bare=bare,
            width=width,
            stdout=stdout,
        )

    @HexViewer.Requires(&#39;colorama&#39;, [&#39;display&#39;, &#39;default&#39;, &#39;extended&#39;])
    def _colorama():
        import colorama
        return colorama

    def process(self, data):
        colorize = not self.args.gray and not self.args.stdout
        lines = self._peeklines(data, colorize)

        if self.args.stdout:
            for line in lines:
                yield line.encode(self.codec)
            return

        stderr = sys.stderr

        if colorize:
            colorama = self._colorama
            if os.name == &#39;nt&#39;:
                stderr = colorama.AnsiToWin32(stderr).stream
            _erase = &#39; &#39; * get_terminal_size()
            _reset = F&#39;\r{colorama.Style.RESET_ALL}{_erase}\r&#39;
        else:
            _reset = &#39;&#39;

        try:
            for line in lines:
                print(line, file=stderr)
        except BaseException:
            stderr.write(_reset)
            raise
        if not self.isatty():
            self.log_info(&#39;forwarding input to next unit&#39;)
            yield data

    def _peekmeta(self, linewidth, sep, meta: dict, peek=None) -&gt; Generator[str]:
        if not meta and not peek:
            return
        width = max((len(name) for name in meta), default=0)
        separators = iter([sep])
        if peek is not None:
            if len(peek) &gt; linewidth:
                peek = peek[:linewidth - 3] + &#39;...&#39;
            yield from separators
            yield peek
        for name in sorted(meta, key=lambda s: (len(s) &lt;= 3, s)):
            if not self.args.index and name == LazyMetaOracle.IndexKey:
                continue
            value = meta[name]
            if value is None:
                continue
            if isinstance(value, CustomStringRepresentation):
                value = repr(value).strip()
            elif isbuffer(value):
                value = repr(ByteStringWrapper(value))
            elif isinstance(value, int):
                if value in range(-999, 1000):
                    value = str(value)
                elif value &gt; 0:
                    value = F&#39;0x{value:X}&#39;
                else:
                    value = F&#39;-0x{-value:X}&#39;
            elif isinstance(value, float):
                value = F&#39;{value:.4f}&#39;
            metavar = F&#39;{name:&gt;{width + 2}} = {value!s}&#39;
            if len(metavar) &gt; linewidth:
                metavar = metavar[:linewidth - 3] + &#39;...&#39;
            yield from separators
            yield metavar

    def _trydecode(self, data, codec: str | None, width: int, linecount: int) -&gt; str:
        remaining = linecount
        result = []
        wrap = self.args.decode &gt; 1
        if codec is None:
            from refinery.units.encoding.esc import esc
            decoded = data[:abs(width * linecount)]
            decoded = str(decoded | -esc(bare=True))
            limit = abs(min(linecount * width, len(decoded)))
            for k in range(0, limit, width):
                result.append(decoded[k:k + width])
            return result
        try:
            import unicodedata
            unprintable = {&#39;Cc&#39;, &#39;Cf&#39;, &#39;Co&#39;, &#39;Cs&#39;}
            self.log_info(F&#39;trying to decode as {codec}.&#39;)
            decoded = codecs.decode(data, codec, errors=&#39;strict&#39;)
            count = sum(unicodedata.category(c) not in unprintable for c in decoded)
            ratio = count / len(decoded)
        except UnicodeDecodeError as DE:
            self.log_info(&#39;decoding failed:&#39;, DE.reason)
            return None
        except ValueError as V:
            self.log_info(&#39;decoding failed:&#39;, V)
            return None
        if ratio &lt; 0.8:
            self.log_info(F&#39;data contains {ratio * 100:.2f}% printable characters, this is too low.&#39;)
            return None
        decoded = decoded.splitlines(False)
        if not wrap:
            for k, line in enumerate(decoded):
                line = line.replace(&#39;\t&#39;, &#39;\x20&#39; * 4)
                if len(line) &lt;= width:
                    continue
                clipped = line[:width - 3]
                if self.args.gray:
                    color = &#39;&#39;
                    reset = &#39;&#39;
                else:
                    colorama = self._colorama
                    color = colorama.Fore.LIGHTRED_EX
                    reset = colorama.Style.RESET_ALL
                decoded[k] = F&#39;{clipped}{color}...{reset}&#39;
            return decoded[:abs(linecount)]
        for paragraph in decoded:
            if not remaining:
                break
            wrapped = [
                line for chunk in textwrap.wrap(
                    paragraph,
                    width,
                    break_long_words=True,
                    break_on_hyphens=False,
                    drop_whitespace=False,
                    expand_tabs=True,
                    max_lines=abs(remaining + 1),
                    replace_whitespace=False,
                    tabsize=4,
                )
                for line in chunk.splitlines(keepends=False)
            ]
            remaining -= len(wrapped)
            result.extend(wrapped)
        return result[:abs(linecount)]

    def _peeklines(self, data: Chunk, colorize: bool) -&gt; Generator[str]:

        meta = metavars(data)

        codec = None
        lines = None
        final = data.temp or False
        empty = True

        if not self.args.index:
            index = None
        else:
            index = meta.get(&#39;index&#39;, None)

        if not self.args.brief:
            padding = 0
        else:
            padding = SizeInt.width + 2
            if index is not None:
                padding += 6

        metrics = self._get_metrics(len(data), self.args.lines, padding)

        if self.args.brief:
            metrics.address_width = 0
            metrics.fit_to_width(allow_increase=True)

        sepsize = metrics.hexdump_width
        txtsize = self.args.width or sepsize

        if self.args.lines and data:
            if self.args.escape:
                lines = self._trydecode(data, None, txtsize, metrics.line_count)
            if self.args.decode &gt; 0:
                for codec in (&#39;utf8&#39;, &#39;cp1251&#39;, &#39;cp1252&#39;, &#39;utf-16le&#39;, &#39;utf-16&#39;, &#39;utf-16be&#39;):
                    lines = self._trydecode(data, codec, txtsize, metrics.line_count)
                    if lines:
                        codec = codec
                        break
                else:
                    codec = None
            if lines is None:
                lines = list(self.hexdump(data, metrics, colorize))
            else:
                sepsize = txtsize

        def separator(title=None):
            if title is None or sepsize &lt;= len(title) + 8:
                return sepsize * &#39;-&#39;
            return &#39;-&#39; * (sepsize - len(title) - 5) + F&#39;[{title}]---&#39;

        if self.args.brief:
            final = False
        elif not self.args.bare:
            peek = repr(meta.size)
            line = separator()
            if len(data) &lt;= 5_000_000:
                peek = F&#39;{peek}; {meta.entropy!r} entropy&#39;
            peek = F&#39;{peek}; {meta.magic!s}&#39;
            if self.args.lines == 0:
                peek = None
            elif not data:
                peek = None
                line = separator(&#39;empty chunk&#39;)
            if self.args.meta &gt; 0:
                meta.derive(&#39;size&#39;)
                meta.derive(&#39;magic&#39;)
                meta.derive(&#39;entropy&#39;)
                peek = None
            if self.args.meta &gt; 1:
                meta.derive(&#39;crc32&#39;)
                meta.derive(&#39;sha256&#39;)
            if self.args.meta &gt; 2:
                for name in meta.derivations:
                    meta[name]
            for line in self._peekmeta(metrics.hexdump_width, line, meta, peek=peek):
                empty = False
                yield line

        if lines:
            empty = False
            if not self.args.brief:
                yield separator(codec or None)
                yield from lines
            else:
                brief = next(iter(lines))
                brief = F&#39;{SizeInt(len(data))!r}: {brief}&#39;
                if index is not None:
                    brief = F&#39;#{index:03d}: {brief}&#39;
                yield brief

        if final and (self.args.bare or not empty):
            yield separator()

    def filter(self, chunks):
        try:
            self._colorama.init(wrap=False)
        except ImportError:
            pass

        discarded = 0

        if self.args.brief:
            for chunk in chunks:
                if not chunk.visible and self.isatty():
                    discarded += 1
                    continue
                self.log_debug(chunk)
                yield chunk
        else:
            it = iter(chunks)
            buffer = collections.deque(itertools.islice(it, 0, 2))
            buffer.reverse()
            while buffer:
                if self.isatty() and not buffer[0].visible:
                    buffer.popleft()
                    discarded += 1
                else:
                    item = buffer.pop()
                    last = not bool(buffer)
                    item.temp = last
                    if not item.visible and self.isatty():
                        discarded += 1
                    else:
                        yield item
                try:
                    buffer.appendleft(next(it))
                except StopIteration:
                    pass

        if discarded:
            self.log_warn(F&#39;discarded {discarded} invisible chunks to prevent them from leaking into the terminal.&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.pefix"><code class="flex name class">
<span>class <span class="ident">pefix</span></span>
</code></dt>
<dd>
<section class="desc"><p>Take as input a buffer that represents a stripped PE file, i.e. magic numbers and other
relevant parts of the header have been stripped. The unit attempts to repair the damage
and return something that can be parsed.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pe/pefix.py#L55-L129" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pefix(Unit):
    &#34;&#34;&#34;
    Take as input a buffer that represents a stripped PE file, i.e. magic numbers and other
    relevant parts of the header have been stripped. The unit attempts to repair the damage
    and return something that can be parsed.
    &#34;&#34;&#34;
    @Unit.Requires(&#39;pefile&#39;, [&#39;default&#39;, &#39;extended&#39;])
    def _pefile():
        import pefile
        return pefile

    def process(self, data):
        sr = StructReader(data)
        sr.write(B&#39;MZ&#39;)
        sr.seekset(0x3C)
        nt = sr.u16()
        oh = nt + 0x18
        sr.seekset(nt)
        sr.write(B&#39;PE&#39;)
        sr.seekrel(2)
        mt = sr.u16()

        try:
            mt = MachineType(mt)
        except Exception:
            mt = None

        sr.seekset(oh)
        ms = bytes(sr.peek(2))

        try:
            ms = ImgState(ms)
        except ValueError:
            ms = {
                None: None,
                MachineType.I386  : ImgState.x32,
                MachineType.IA64  : ImgState.x64,
                MachineType.AMD64 : ImgState.x64,
            }.get(mt)

        if ms is None:
            self.log_warn(&#39;could not determine image state; nulling field&#39;)
            sr.write(B&#39;\0\0&#39;)
        else:
            sr.write(ms.value)

        if mt is None:
            if mt := {
                None: None,
                ImgState.x32: MachineType.I386,
                ImgState.x64: MachineType.AMD64,
            }.get(ms):
                assert isinstance(mt, MachineType)
                sr.seekset(nt + 4)
                sr.write(mt.value.to_bytes(2, &#39;little&#39;))

        pe = self._pefile.PE(data=data, fast_load=True)

        if (alignment := pe.OPTIONAL_HEADER.FileAlignment) not in {1 &lt;&lt; k for k in range(9, 16)}:
            for k in range(9, 16):
                alignment = 1 &lt;&lt; k
                size_of_headers = 0x28 * len(pe.sections) + oh + 0xF0
                soh = align(alignment, size_of_headers)
                if any(data[size_of_headers:soh]):
                    raise ValueError(&#39;nonzero bytes in what must be header padding&#39;)
                if any(data[soh:soh + 8]):
                    pe.OPTIONAL_HEADER.SizeOfHeaders = soh
                    break
            else:
                raise ValueError(&#39;unable to find a valid file alignment&#39;)

        pe.OPTIONAL_HEADER.FileAlignment = alignment
        pe.OPTIONAL_HEADER.SectionAlignment = max(pe.OPTIONAL_HEADER.SectionAlignment, alignment)

        return pe.write()</code></pre>
</details>
</dd>
<dt id="refinery.shell.pemeta"><code class="flex name class">
<span>class <span class="ident">pemeta</span></span>
<span>(</span><span>custom=False, debug=False, dotnet=False, signatures=False, timestamps=0, version=False, header=False, exports=0, imports=0, tabular=False, timeraw=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract metadata from PE files. By default, all information except for imports and exports are
extracted.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pe/pemeta.py#L149-L748" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pemeta(Unit):
    &#34;&#34;&#34;
    Extract metadata from PE files. By default, all information except for imports and exports are
    extracted.
    &#34;&#34;&#34;
    def __init__(
        self,
        custom: Param[bool, Arg(&#39;-c&#39;, &#39;--custom&#39;,
            help=&#39;Unless enabled, all default categories will be extracted.&#39;)] = False,
        debug: Param[bool, Arg.Switch(&#39;-D&#39;,
            help=&#39;Parse the PDB path from the debug directory.&#39;)] = False,
        dotnet: Param[bool, Arg.Switch(&#39;-N&#39;,
            help=&#39;Parse the .NET header.&#39;)] = False,
        signatures: Param[bool, Arg.Switch(&#39;-S&#39;,
            help=&#39;Parse digital signatures.&#39;)] = False,
        timestamps: Param[int, Arg.Counts(&#39;-T&#39;,
            help=&#39;Extract time stamps. Specify twice for more detail.&#39;)] = 0,
        version: Param[bool, Arg.Switch(&#39;-V&#39;,
            help=&#39;Parse the VERSION resource.&#39;)] = False,
        header: Param[bool, Arg.Switch(&#39;-H&#39;,
            help=&#39;Parse base data from the PE header.&#39;)] = False,
        exports: Param[int, Arg.Counts(&#39;-E&#39;,
            help=&#39;List all exported functions. Specify twice to include addresses.&#39;)] = 0,
        imports: Param[int, Arg.Counts(&#39;-I&#39;,
            help=&#39;List all imported functions. Specify twice to include addresses.&#39;)] = 0,
        tabular: Param[bool, Arg.Switch(&#39;-t&#39;,
            help=&#39;Print information in a table rather than as JSON&#39;)] = False,
        timeraw: Param[bool, Arg.Switch(&#39;-r&#39;,
            help=&#39;Extract time stamps as numbers instead of human-readable format.&#39;)] = False,
    ):
        if not custom and not any((debug, dotnet, signatures, timestamps, version, header)):
            debug = dotnet = signatures = timestamps = version = header = True
        super().__init__(
            debug=debug,
            dotnet=dotnet,
            signatures=signatures,
            timestamps=timestamps,
            version=version,
            header=header,
            imports=imports,
            exports=exports,
            timeraw=timeraw,
            tabular=tabular,
        )

    @classmethod
    def handles(cls, data):
        return is_likely_pe(data)

    @classmethod
    def _ensure_string(cls, x):
        if not isinstance(x, str):
            x = repr(x) if not isinstance(x, bytes) else x.decode(cls.codec, &#39;backslashreplace&#39;)
        return x

    @classmethod
    def _parse_pedict(cls, bin: dict):
        return {
            cls._ensure_string(key).replace(&#34; &#34;, &#34;&#34;): cls._ensure_string(val)
            for key, val in bin.items() if val}

    @classmethod
    def parse_signature(cls, data: bytearray) -&gt; dict:
        &#34;&#34;&#34;
        Extracts a JSON-serializable and human-readable dictionary with information about
        time stamp and code signing certificates that are attached to the input PE file.
        &#34;&#34;&#34;
        from refinery.units.formats.pkcs7 import pkcs7

        try:
            signature = data | pkcs7 | json.loads
        except Exception as E:
            raise ValueError(F&#39;PKCS7 parser failed with error: {E!s}&#39;)

        info = {}

        def _value(doc: dict, require_type=None):
            if require_type is not None:
                if doc.get(&#39;type&#39;, None) != require_type:
                    raise LookupError
            value = doc.get(&#39;value&#39;, None)
            value = [value] if value else doc.get(&#39;values&#39;, [])
            if not value:
                raise LookupError
            return value[0]

        def find_timestamps(entry) -&gt; dict:
            if isinstance(entry, dict):
                try:
                    return {&#39;Timestamp&#39;: _value(entry, &#39;signing_time&#39;)}
                except LookupError:
                    pass
                for value in entry.values():
                    result = find_timestamps(value)
                    if result is None:
                        continue
                    with suppress(KeyError):
                        result.setdefault(&#39;TimestampIssuer&#39;, entry[&#39;sid&#39;][&#39;issuer&#39;][&#39;common_name&#39;])
                    return result
            elif isinstance(entry, list):
                for value in entry:
                    result = find_timestamps(value)
                    if result is None:
                        continue
                    return result

        timestamp_info = find_timestamps(signature)
        if timestamp_info is not None:
            info.update(timestamp_info)

        try:
            certificates = signature[&#39;content&#39;][&#39;certificates&#39;]
            signer_infos = signature[&#39;content&#39;][&#39;signer_infos&#39;]
        except KeyError:
            return info

        try:
            signer_serials = {info[&#39;sid&#39;][&#39;serial_number&#39;]: info for info in signer_infos}
        except KeyError:
            return info

        signer_certificates = []

        for certificate in certificates:
            with suppress(Exception):
                crt = certificate[&#39;tbs_certificate&#39;]
                serial = crt[&#39;serial_number&#39;]
                signer = signer_serials[serial]
                if isinstance(serial, int):
                    serial = F&#39;{serial:x}&#39;
                if len(serial) % 2 != 0:
                    serial = F&#39;0{serial}&#39;
                assert bytes.fromhex(serial) in data
                subject = crt[&#39;subject&#39;]
                location = [subject.get(t, &#39;&#39;) for t in (
                    &#39;locality_name&#39;, &#39;state_or_province_name&#39;, &#39;country_name&#39;)]
                cert_info = {}
                cert_info.update(Subject=subject[&#39;common_name&#39;])
                if any(location):
                    cert_info.update(SubjectLocation=&#39;, &#39;.join(filter(None, location)))
                for attr in signer[&#39;signed_attrs&#39;]:
                    if attr[&#39;type&#39;] == &#39;authenticode_info&#39;:
                        auth = _value(attr)
                        cert_info.update(ProgramName=auth[&#39;programName&#39;])
                        cert_info.update(MoreInfo=auth[&#39;moreInfo&#39;])
                try:
                    valid_since = crt[&#39;validity&#39;][&#39;not_before&#39;]
                    valid_until = crt[&#39;validity&#39;][&#39;not_after&#39;]
                except KeyError:
                    pass
                else:
                    cert_info.update(ValidSince=valid_since, ValidUntil=valid_until)
                cert_info.update(
                    Issuer=crt[&#39;issuer&#39;][&#39;common_name&#39;], Fingerprint=certificate[&#39;fingerprint&#39;], Serial=serial)
                signer_certificates.append(cert_info)

        if len(signer_certificates) == 1:
            info.update(signer_certificates[0])
        if len(signer_certificates) &gt;= 2:
            info[&#39;Signer&#39;] = signer_certificates
        return info

    def _pe_characteristics(self, pe: lief.PE.Binary):
        characteristics = {F&#39;IMAGE_FILE_{flag.name}&#39; for flag in lief.PE.Header.CHARACTERISTICS
            if pe.header.characteristics &amp; flag.value}
        if pe.header.characteristics &amp; 0x40:
            # TODO: Missing from LIEF
            characteristics.add(&#39;IMAGE_FILE_16BIT_MACHINE&#39;)
        return characteristics

    def _pe_address_width(self, pe: lief.PE.Binary, default=16) -&gt; int:
        # TODO: missing from LIEF
        IMAGE_FILE_16BIT_MACHINE = 0x40
        if pe.header.characteristics &amp; IMAGE_FILE_16BIT_MACHINE:
            return 4
        elif pe.header.machine == lief.PE.Header.MACHINE_TYPES.I386:
            return 8
        elif pe.header.machine in (
            lief.PE.Header.MACHINE_TYPES.AMD64,
            lief.PE.Header.MACHINE_TYPES.IA64,
        ):
            return 16
        else:
            return default

    def _vint(self, pe: lief.PE.Binary, value: int):
        if not self.args.tabular:
            return value
        aw = self._pe_address_width(pe)
        return F&#39;0x{value:0{aw}X}&#39;

    def parse_version(self, pe: lief.PE.Binary, data=None) -&gt; dict | None:
        &#34;&#34;&#34;
        Extracts a JSON-serializable and human-readable dictionary with information about
        the version resource of an input PE file, if available.
        &#34;&#34;&#34;
        version_info = {}
        rsrc = unwrap(pe.resources_manager)
        if isinstance(rsrc, lief.lib.lief_errors) or not rsrc.has_version:
            return None
        version = rsrc.version[0]

        if info := version.string_file_info:
            for child in info.children:
                entries = {e.key: e.value for e in child.entries}
                version_info.update({
                    k.replace(&#39; &#39;, &#39;&#39;): _STRING(v) for k, v in entries.items()
                })

        if rsrc.has_icons:
            icon = next(iter(rsrc.icons))
            version_info.update(
                LangID=self._vint(pe, icon.lang &lt;&lt; 0x10 | icon.sublang),
                Language=LCID.get(icon.lang, &#39;Language Neutral&#39;),
                Charset=self._CHARSET.get(icon.sublang, &#39;Unknown Charset&#39;),
            )

        def _code_pages(d: lief.PE.ResourceDirectory | lief.PE.ResourceData):
            if isinstance(d, lief.PE.ResourceData):
                yield d.code_page
                return
            for child in d.childs:
                yield from _code_pages(child)

        code_pages: set[int] = set()

        for t in rsrc.types:
            code_pages.update(_code_pages(rsrc.get_node_type(t)))

        if len(code_pages) == 1:
            cp = next(iter(code_pages))
            version_info.update(CodePage=cp)

        def _to_version_string(hi: int, lo: int):
            a = hi &gt;&gt; 0x10
            b = hi &amp; 0xFFFF
            c = lo &gt;&gt; 0x10
            d = lo &amp; 0xFFFF
            return F&#39;{a}.{b}.{c}.{d}&#39;

        # TODO: Missing: Version.CompanyName
        # TODO: Missing: Version.FileDescription
        # TODO: Missing: Version.LegalCopyright
        # TODO: Missing: Version.ProductName

        if info := version.file_info:
            for name, val, T in (
                (&#39;FileType&#39;, info.file_type, info.FILE_TYPE),
                (&#39;OSName&#39;, info.file_os, info.VERSION_OS),
                (&#39;FileSubType&#39;, info.file_subtype, info.FILE_TYPE_DETAILS),
            ):
                if not val:
                    continue
                try:
                    version_info[name] = T(val).name
                except Exception:
                    continue
            if t := info.file_date_ms &lt;&lt; 32 | info.file_date_ls:
                version_info.update(Timestamp=_FILETIME(t))
            version_info.update(
                ProductVersion=_to_version_string(info.product_version_ms, info.product_version_ls),
                FileVersion=_to_version_string(info.file_version_ms, info.file_version_ls),
            )

        if info := version.var_file_info:
            ...

        return version_info or None

    def parse_exports(self, pe: lief.PE.Binary, data=None, include_addresses=False) -&gt; list:
        base = pe.optional_header.imagebase
        info = []
        if not pe.has_exports:
            return None
        for k, exp in enumerate(pe.get_export().entries):
            name = exp.demangled_name
            if not name:
                name = exp.name
            if not name:
                name = F&#39;@{k}&#39;
            if not isinstance(name, str):
                name = name.decode(&#39;latin1&#39;)
            item = {
                &#39;Name&#39;: name, &#39;Address&#39;: self._vint(pe, exp.address + base)
            } if include_addresses else name
            info.append(item)
        return info

    def parse_imports(self, pe: lief.PE.Binary, data=None, include_addresses=False) -&gt; list:
        info = {}
        for idd in itertools.chain(pe.imports, pe.delay_imports):
            dll = _STRING(idd.name)
            if dll.lower().endswith(&#39;.dll&#39;):
                dll = dll[:~3]
            imports: list[str] = info.setdefault(dll, [])
            for imp in idd.entries:
                name = _STRING(imp.name) or F&#39;@{imp.ordinal}&#39;
                imports.append(dict(
                    Name=name, Address=self._vint(pe, imp.value)
                ) if include_addresses else name)
        return info

    def parse_header(self, pe: lief.PE.Binary, data=None) -&gt; dict:
        major = pe.optional_header.major_operating_system_version
        minor = pe.optional_header.minor_operating_system_version
        version = self._WINVER.get(major, {0: &#39;Unknown&#39;})

        try:
            MinimumOS = version[minor]
        except LookupError:
            MinimumOS = version[0]
        header_information = {
            &#39;Machine&#39;: pe.header.machine.name,
            &#39;Subsystem&#39;: pe.optional_header.subsystem.name,
            &#39;MinimumOS&#39;: MinimumOS,
        }
        if pe.has_exports:
            export_name = _STRING(pe.get_export().name)
            if export_name.isprintable():
                header_information[&#39;ExportName&#39;] = export_name

        if pe.has_rich_header:
            rich = []
            if self.args.tabular:
                cw = max(len(F&#39;{entry.count:d}&#39;) for entry in pe.rich_header.entries)
            for entry in pe.rich_header.entries:
                idv = entry.build_id | (entry.id &lt;&lt; 0x10)
                count = entry.count
                info = get_rich_info(idv)
                if not info:
                    continue
                pid = info.pid.upper()
                if self.args.tabular:
                    short_pid = get_rich_short_pid(pid)
                    rich.append(F&#39;[{idv:08x}] {count:&gt;0{cw}d} {short_pid!s} {info.ver}&#39;)
                else:
                    rich.append({
                        &#39;Counter&#39;: count,
                        &#39;Encoded&#39;: F&#39;{idv:08x}&#39;,
                        &#39;Library&#39;: pid,
                        &#39;Product&#39;: info.ver,
                    })
            header_information[&#39;RICH&#39;] = rich

        characteristics = self._pe_characteristics(pe)
        for typespec, flag in {
            &#39;EXE&#39;: &#39;IMAGE_FILE_EXECUTABLE_IMAGE&#39;,
            &#39;DLL&#39;: &#39;IMAGE_FILE_DLL&#39;,
            &#39;SYS&#39;: &#39;IMAGE_FILE_SYSTEM&#39;
        }.items():
            if flag in characteristics:
                header_information[&#39;Type&#39;] = typespec

        base = pe.optional_header.imagebase
        header_information[&#39;ImageBase&#39;] = self._vint(pe, base)
        header_information[&#39;ImageSize&#39;] = self._vint(pe, pe.optional_header.sizeof_image)
        header_information[&#39;ComputedSize&#39;] = get_pe_size(pe)
        header_information[&#39;Bits&#39;] = 4 * self._pe_address_width(pe, 16)
        header_information[&#39;EntryPoint&#39;] = self._vint(pe, pe.optional_header.addressof_entrypoint + base)
        return header_information

    def parse_time_stamps(self, pe: lief.PE.Binary, raw_time_stamps: bool, more_detail: bool) -&gt; dict:
        &#34;&#34;&#34;
        Extracts time stamps from the PE header (link time), as well as from the imports,
        exports, debug, and resource directory. The resource time stamp is also parsed as
        a DOS time stamp and returned as the &#34;Delphi&#34; time stamp.
        &#34;&#34;&#34;
        def _id(x): return x
        dt = _id if raw_time_stamps else date_from_timestamp
        info = {}

        with suppress(AttributeError):
            info.update(Linker=dt(pe.header.time_date_stamps))

        import_timestamps = {}
        for entry in pe.imports:
            ts = entry.timedatestamp
            if ts == 0 or ts == 0xFFFFFFFF:
                continue
            import_timestamps[_STRING(entry.name, True)] = dt(ts)

        symbol_timestamps = {}
        for entry in pe.delay_imports:
            ts = entry.timestamp
            if ts == 0 or ts == 0xFFFFFFFF:
                continue
            symbol_timestamps[_STRING(entry.name, True)] = dt(ts)

        for key, impts in [
            (&#39;Import&#39;, import_timestamps),
            (&#39;Symbol&#39;, symbol_timestamps),
        ]:
            if not impts:
                continue
            if not more_detail:
                dmin = min(impts.values())
                dmax = max(impts.values())
                small_delta = 2 * 60 * 60
                if not raw_time_stamps:
                    small_delta = timedelta(seconds=small_delta)
                if dmax - dmin &lt; small_delta:
                    impts = dmin
            info[key] = impts

        if pe.has_exports and (ts := pe.get_export().timestamp):
            info.update(Export=dt(ts))

        if pe.has_resources and pe.resources.is_directory:
            rsrc: lief.PE.ResourceDirectory = pe.resources
            if res_timestamp := rsrc.time_date_stamp:
                with suppress(ValueError):
                    from refinery.units.misc.datefix import datefix
                    dos = datefix.dostime(res_timestamp)
                    info.update(Delphi=dos)
                    info.update(RsrcTS=dt(res_timestamp))

        def norm(value):
            if isinstance(value, list):
                return [norm(v) for v in value]
            if isinstance(value, dict):
                return {k: norm(v) for k, v in value.items()}
            if isinstance(value, int):
                return value
            return str(value)

        return {key: norm(value) for key, value in info.items()}

    def parse_dotnet(self, pe: lief.PE.Binary, data):
        &#34;&#34;&#34;
        Extracts a JSON-serializable and human-readable dictionary with information about
        the .NET metadata of an input PE file.
        &#34;&#34;&#34;
        header = DotNetHeader(data, pe)
        tables = header.meta.Streams.Tables
        info = dict(
            RuntimeVersion=F&#39;{header.head.MajorRuntimeVersion}.{header.head.MinorRuntimeVersion}&#39;,
            Version=F&#39;{header.meta.MajorVersion}.{header.meta.MinorVersion}&#39;,
            VersionString=header.meta.VersionString
        )

        info[&#39;Flags&#39;] = [name for name, check in header.head.KnownFlags.items() if check]

        if len(tables.Assembly) == 1:
            assembly = tables.Assembly[0]
            info.update(
                AssemblyName=assembly.Name,
                Release=&#39;{}.{}.{}.{}&#39;.format(
                    assembly.MajorVersion,
                    assembly.MinorVersion,
                    assembly.BuildNumber,
                    assembly.RevisionNumber
                )
            )

        try:
            entry = self._vint(pe, header.head.EntryPointToken + pe.optional_header.imagebase)
            info.update(EntryPoint=entry)
        except AttributeError:
            pass

        if len(tables.Module) == 1:
            module = tables.Module[0]
            info.update(ModuleName=module.Name)

        return info

    def parse_debug(self, pe: lief.PE.Binary, data=None):
        result = []
        if not pe.has_debug:
            return None
        for entry in pe.debug:
            if entry.type != lief.PE.Debug.TYPES.CODEVIEW:
                continue
            try:
                entry: lief.PE.CodeViewPDB
                result.append(dict(
                    PdbPath=_STRING(entry.filename),
                    PdbGUID=entry.guid,
                    PdbAge=entry.age,
                ))
            except AttributeError:
                continue
        if len(result) == 1:
            result = result[0]
        return result

    def process(self, data):
        result = {}

        pe = lief.load_pe(
            data,
            parse_exports=True,
            parse_imports=self.args.imports,
            parse_rsrc=self.args.version,
            parse_reloc=False,
            parse_signature=self.args.timestamps or self.args.signatures,
        )

        if pe is None:
            raise ValueError(&#39;Input not recognized as a PE file.&#39;)

        pe = NoLoggingProxy(pe)

        for switch, resolver, name in [
            (self.args.debug,   self.parse_debug,    &#39;Debug&#39;),    # noqa
            (self.args.dotnet,  self.parse_dotnet,   &#39;DotNet&#39;),   # noqa
            (self.args.header,  self.parse_header,   &#39;Header&#39;),   # noqa
            (self.args.version, self.parse_version,  &#39;Version&#39;),  # noqa
            (self.args.imports, self.parse_imports,  &#39;Imports&#39;),  # noqa
            (self.args.exports, self.parse_exports,  &#39;Exports&#39;),  # noqa
        ]:
            if not switch:
                continue
            self.log_debug(F&#39;parsing: {name}&#39;)
            args = pe, data
            if switch &gt; 1:
                args = *args, True
            try:
                info = resolver(*args)
            except Exception as E:
                self.log_info(F&#39;failed to obtain {name}: {E!s}&#39;)
                continue
            if info:
                result[name] = info

        signature = {}

        if self.args.timestamps or self.args.signatures:
            with suppress(Exception):
                from refinery.units.formats.pe.pesig import pesig
                signature = self.parse_signature(next(data | pesig))

        if signature:
            try:
                verification = pe.verify_signature()
            except Exception:
                pass
            else:
                from lief.PE import Signature
                if verification == Signature.VERIFICATION_FLAGS.OK:
                    signature[&#39;IsValid&#39;] = True
                else:
                    signature[&#39;Flags&#39;] = [
                        vf.name for vf in Signature.VERIFICATION_FLAGS if vf &amp; verification]
                    signature[&#39;IsValid&#39;] = False

        if self.args.timestamps:
            ts = self.parse_time_stamps(pe, self.args.timeraw, self.args.timestamps &gt; 1)
            with suppress(KeyError):
                ts.update(Signed=signature[&#39;Timestamp&#39;])
            result.update(TimeStamp=ts)

        if signature and self.args.signatures:
            result[&#39;Signature&#39;] = signature

        if result:
            yield from ppjson(tabular=self.args.tabular)._pretty_output(result, indent=4, ensure_ascii=False)

    _CHARSET = {
        0x0000: &#39;7-bit ASCII&#39;,
        0x03A4: &#39;Japan (Shift ? JIS X-0208)&#39;,
        0x03B5: &#39;Korea (Shift ? KSC 5601)&#39;,
        0x03B6: &#39;Taiwan (Big5)&#39;,
        0x04B0: &#39;Unicode&#39;,
        0x04E2: &#39;Latin-2 (Eastern European)&#39;,
        0x04E3: &#39;Cyrillic&#39;,
        0x04E4: &#39;Multilingual&#39;,
        0x04E5: &#39;Greek&#39;,
        0x04E6: &#39;Turkish&#39;,
        0x04E7: &#39;Hebrew&#39;,
        0x04E8: &#39;Arabic&#39;,
    }

    _WINVER = {
        3: {
            0x00: &#39;Windows NT 3&#39;,
            0x0A: &#39;Windows NT 3.1&#39;,
            0x32: &#39;Windows NT 3.5&#39;,
            0x33: &#39;Windows NT 3.51&#39;,
        },
        4: {
            0x00: &#39;Windows 95&#39;,
            0x0A: &#39;Windows 98&#39;,
        },
        5: {
            0x00: &#39;Windows 2000&#39;,
            0x5A: &#39;Windows Me&#39;,
            0x01: &#39;Windows XP&#39;,
            0x02: &#39;Windows Server 2003&#39;,
        },
        6: {
            0x00: &#39;Windows Vista&#39;,
            0x01: &#39;Windows 7&#39;,
            0x02: &#39;Windows 8&#39;,
            0x03: &#39;Windows 8.1&#39;,
        },
        10: {
            0x00: &#39;Windows 10&#39;,
        }
    }</code></pre>
</details>
</dd>
<dt id="refinery.shell.peoverlay"><code class="flex name class">
<span>class <span class="ident">peoverlay</span></span>
<span>(</span><span>certificate=False, directories=False, memdump=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the overlay of a PE file, i.e. anything that may have been appended to the file.
This does not include digital signatures. Use <code><a title="refinery.pestrip" href="index.html#refinery.pestrip">pestrip</a></code> to obtain only the body
of the PE file after removing the overlay.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pe/peoverlay.py#L6-L19" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class peoverlay(OverlayUnit):
    &#34;&#34;&#34;
    Returns the overlay of a PE file, i.e. anything that may have been appended to the file.
    This does not include digital signatures. Use `refinery.pestrip` to obtain only the body
    of the PE file after removing the overlay.
    &#34;&#34;&#34;
    def process(self, data: bytearray) -&gt; bytearray:
        size = self._get_size(data)
        try:
            data[:size] = []
        except Exception:
            return data[size:]
        else:
            return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.perc"><code class="flex name class">
<span>class <span class="ident">perc</span></span>
<span>(</span><span>*paths, pretty=False, path=b'path', regex=False, exact=False, fuzzy=0, drop_path=False, join_path=False, list=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract PE file resources.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pe/perc.py#L74-L438" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class perc(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract PE file resources.
    &#34;&#34;&#34;
    def __init__(
        self, *paths,
        pretty: Param[bool, Arg.Switch(&#39;-p&#39;, help=&#39;Add missing headers to bitmap and icon resources.&#39;)] = False,
        **kwargs
    ):
        super().__init__(*paths, pretty=pretty, **kwargs)

    def _get_icon_dir(self, pe: lief.PE.Binary):
        for manifest_entry in pe.resources.childs:
            if manifest_entry.id != RSRC.ICON_GROUP.value:
                continue
            child = manifest_entry.childs[0].childs[0]
            if isinstance(child, lief.PE.ResourceData):
                return GRPICONDIR.Parse(bytearray(child.content))

    def _search(self, pe: lief.PE.Binary, directory: lief.PE.ResourceDirectory, *parts):
        if directory.depth &gt;= 3:
            self.log_warn(F&#39;unexpected resource tree level {directory.depth + 1:d}&#39;)
        for entry in directory.childs:
            if entry.has_name:
                identifier = str(entry.name)
            elif directory.depth == 0 and entry.id in _RSRC:
                identifier = RSRC(entry.id)
            elif entry.id is not None:
                identifier = entry.id
            else:
                self.log_warn(F&#39;resource entry has name {entry.name} and id {entry.id} at level {directory.depth + 1:d}&#39;)
                continue
            if isinstance(entry, lief.PE.ResourceDirectory):
                yield from self._search(pe, entry, *parts, identifier)
                continue
            if isinstance(entry, lief.PE.ResourceData):
                def _extract_raw_data(_=pe, e=entry):
                    return bytearray(e.content)
                extract: buf | Callable[[], buf] = _extract_raw_data
                path = &#39;/&#39;.join(str(p) for p in (*parts, identifier))
                if self.args.pretty:
                    if parts[0] is RSRC.BITMAP:
                        extract = self._handle_bitmap(extract)
                    elif parts[0] is RSRC.ICON:
                        extract = self._handle_icon(pe, extract, parts)
                    elif parts[0] is RSRC.ICON_GROUP:
                        def _extract_ico_grp(_=pe, e=entry):
                            data = GRPICONDIR.Parse(e.content)
                            return json.dumps({
                                entry.nid: {
                                    &#39;width&#39;         : entry.width,
                                    &#39;height&#39;        : entry.height,
                                    &#39;bytes&#39;         : entry.bytes_in_res,
                                    &#39;color&#39;         : {
                                        &#39;count&#39;     : entry.color_count,
                                        &#39;planes&#39;    : entry.planes,
                                        &#39;bits&#39;      : entry.bit_count,
                                    },
                                } for entry in data.entries},
                                indent=4
                            ).encode(self.codec)
                        extract = _extract_ico_grp

                yield UnpackResult(
                    path,
                    extract,
                    lcid=self._get_lcid(entry),
                    offset=entry.offset,
                )

    def _get_lcid(self, node_data) -&gt; str | None:
        try:
            pid = node_data.id &amp; 0x3FF
            sid = node_data.id &gt;&gt; 0x0A
        except AttributeError:
            return None
        try:
            pid = self._LANG_ID_TO_LCID[pid]
        except KeyError:
            return None
        lcid = pid.get(sid, 0)
        return LCID.get(lcid)

    def _handle_bitmap(self, extract_raw_data: Callable[[], buf]) -&gt; Callable[[], buf]:
        def extract():
            bitmap = extract_raw_data()
            total = (len(bitmap) + 14).to_bytes(4, &#39;little&#39;)
            return B&#39;BM&#39; + total + B&#39;\0\0\0\0\x36\0\0\0&#39; + bitmap
        return extract

    def _handle_icon(
        self,
        pe: lief.PE.Binary,
        extract_raw_data: Callable[[], buf],
        parts: tuple[RSRC, int, int]
    ) -&gt; buf | Callable[[], buf]:
        try:
            icondir = self._get_icon_dir(pe)
            if icondir is None:
                raise RuntimeError
            index = int(parts[1]) - 1
            info = icondir.entries[index]
        except IndexError:
            return extract_raw_data
        except Exception as E:
            self.log_warn(F&#39;unable to generate icon header: {E!s}&#39;)
            return extract_raw_data

        def extract(info=info):
            icon = extract_raw_data()
            if icon[:4] == B&#39;\x28\0\0\0&#39;:
                header = struct.pack(&#39;&lt;HHHBBBBHHII&#39;,
                    0,
                    1,
                    1,
                    info.width,
                    info.height,
                    info.color_count,
                    0,
                    info.planes,
                    info.bit_count,
                    len(icon),
                    0x16
                )
                icon = header + icon
            return icon

        return extract

    def unpack(self, data):
        pe = lief.load_pe_fast(data, parse_rsrc=True)
        if not pe.has_resources:
            return
        if not isinstance(rsrc := pe.resources, lief.PE.ResourceDirectory):
            raise TypeError(&#39;resource root entry was unexpectedly not a directory&#39;)
        yield from self._search(pe, rsrc)

    _LANG_ID_TO_LCID = _mktbl([
        (0x00, 0x03, 0x0C00),
        (0x00, 0x05, 0x1400),
        (0x7F, 0x00, 0x007F),
        (0x00, 0x00, 0x0000),
        (0x02, 0x02, 0x0800),
        (0x00, 0x04, 0x1000),
        (0x00, 0x01, 0x0400),
        (0x36, 0x01, 0x0436),
        (0x1c, 0x01, 0x041C),
        (0x84, 0x01, 0x0484),
        (0x5E, 0x01, 0x045E),
        (0x01, 0x05, 0x1401),
        (0x01, 0x0f, 0x3C01),
        (0x01, 0x03, 0x0C01),
        (0x01, 0x02, 0x0801),
        (0x01, 0x0B, 0x2C01),
        (0x01, 0x0D, 0x3401),
        (0x01, 0x0C, 0x3001),
        (0x01, 0x04, 0x1001),
        (0x01, 0x06, 0x1801),
        (0x01, 0x08, 0x2001),
        (0x01, 0x10, 0x4001),
        (0x01, 0x01, 0x0401),
        (0x01, 0x0A, 0x2801),
        (0x01, 0x07, 0x1C01),
        (0x01, 0x0E, 0x3801),
        (0x01, 0x09, 0x2401),
        (0x2B, 0x01, 0x042B),
        (0x4D, 0x01, 0x044D),
        (0x2C, 0x02, 0x082C),
        (0x2C, 0x01, 0x042C),
        (0x45, 0x02, 0x0445),
        (0x6D, 0x01, 0x046D),
        (0x2d, 0x01, 0x042D),
        (0x23, 0x01, 0x0423),
        (0x1A, 0x08, 0x201A),
        (0x1A, 0x05, 0x141A),
        (0x7E, 0x01, 0x047E),
        (0x02, 0x01, 0x0402),
        (0x92, 0x01, 0x0492),
        (0x5C, 0x01, 0x045C),
        (0x03, 0x01, 0x0403),
        (0x04, 0x03, 0x0C04),
        (0x04, 0x05, 0x1404),
        (0x04, 0x04, 0x1004),
        (0x04, 0x02, 0x0004),
        (0x04, 0x01, 0x7C04),
        (0x83, 0x01, 0x0483),
        (0x1A, None, 0x001A),
        (0x1a, 0x04, 0x101A),
        (0x1a, 0x01, 0x041A),
        (0x05, 0x01, 0x0405),
        (0x06, 0x01, 0x0406),
        (0x8C, 0x01, 0x048C),
        (0x65, 0x01, 0x0465),
        (0x13, 0x02, 0x0813),
        (0x13, 0x01, 0x0413),
        (0x09, 0x03, 0x0C09),
        (0x09, 0x0A, 0x2809),
        (0x09, 0x04, 0x1009),
        (0x09, 0x09, 0x2409),
        (0x09, 0x10, 0x4009),
        (0x09, 0x06, 0x1809),
        (0x09, 0x08, 0x2009),
        (0x09, 0x11, 0x4409),
        (0x09, 0x05, 0x1409),
        (0x09, 0x0D, 0x3409),
        (0x09, 0x12, 0x4809),
        (0x09, 0x07, 0x1c09),
        (0x09, 0x0B, 0x2C09),
        (0x09, 0x02, 0x0809),
        (0x09, 0x01, 0x0409),
        (0x09, 0x0C, 0x3009),
        (0x25, 0x01, 0x0425),
        (0x38, 0x01, 0x0438),
        (0x64, 0x01, 0x0464),
        (0x0B, 0x01, 0x040B),
        (0x0C, 0x02, 0x080c),
        (0x0C, 0x03, 0x0C0C),
        (0x0C, 0x01, 0x040c),
        (0x0C, 0x05, 0x140C),
        (0x0C, 0x06, 0x180C),
        (0x0C, 0x04, 0x100C),
        (0x62, 0x01, 0x0462),
        (0x56, 0x01, 0x0456),
        (0x37, 0x01, 0x0437),
        (0x07, 0x03, 0x0C07),
        (0x07, 0x01, 0x0407),
        (0x07, 0x05, 0x1407),
        (0x07, 0x04, 0x1007),
        (0x07, 0x02, 0x0807),
        (0x08, 0x01, 0x0408),
        (0x6F, 0x01, 0x046F),
        (0x47, 0x01, 0x0447),
        (0x68, 0x01, 0x0468),
        (0x75, 0x01, 0x0475),
        (0x0D, 0x01, 0x040D),
        (0x39, 0x01, 0x0439),
        (0x0E, 0x01, 0x040E),
        (0x0F, 0x01, 0x040F),
        (0x70, 0x01, 0x0470),
        (0x21, 0x01, 0x0421),
        (0x5D, 0x02, 0x085D),
        (0x5D, 0x01, 0x045D),
        (0x3C, 0x02, 0x083C),
        (0x34, 0x01, 0x0434),
        (0x35, 0x01, 0x0435),
        (0x10, 0x01, 0x0410),
        (0x10, 0x02, 0x0810),
        (0x11, 0x01, 0x0411),
        (0x4B, 0x01, 0x044B),
        (0x3F, 0x01, 0x043F),
        (0x53, 0x01, 0x0453),
        (0x86, 0x01, 0x0486),
        (0x87, 0x01, 0x0487),
        (0x57, 0x01, 0x0457),
        (0x12, 0x01, 0x0412),
        (0x40, 0x01, 0x0440),
        (0x54, 0x01, 0x0454),
        (0x26, 0x01, 0x0426),
        (0x27, 0x01, 0x0427),
        (0x2E, 0x02, 0x082E),
        (0x6E, 0x01, 0x046E),
        (0x2F, 0x01, 0x042F),
        (0x3E, 0x02, 0x083E),
        (0x3E, 0x01, 0x043e),
        (0x4C, 0x01, 0x044C),
        (0x3A, 0x01, 0x043A),
        (0x81, 0x01, 0x0481),
        (0x7A, 0x01, 0x047A),
        (0x4E, 0x01, 0x044E),
        (0x7C, 0x01, 0x047C),
        (0x50, 0x01, 0x0450),
        (0x50, 0x02, 0x0850),
        (0x61, 0x01, 0x0461),
        (0x14, 0x01, 0x0414),
        (0x14, 0x02, 0x0814),
        (0x82, 0x01, 0x0482),
        (0x48, 0x01, 0x0448),
        (0x63, 0x01, 0x0463),
        (0x29, 0x01, 0x0429),
        (0x15, 0x01, 0x0415),
        (0x16, 0x01, 0x0416),
        (0x16, 0x02, 0x0816),
        (0x67, 0x02, 0x0867),
        (0x46, 0x01, 0x0446),
        (0x46, 0x02, 0x0846),
        (0x6B, 0x01, 0x046B),
        (0x6B, 0x02, 0x086B),
        (0x6B, 0x03, 0x0C6B),
        (0x18, 0x01, 0x0418),
        (0x17, 0x01, 0x0417),
        (0x19, 0x01, 0x0419),
        (0x85, 0x01, 0x0485),
        (0x3B, 0x09, 0x243B),
        (0x3B, 0x04, 0x103B),
        (0x3B, 0x05, 0x143B),
        (0x3B, 0x03, 0x0C3B),
        (0x3B, 0x01, 0x043B),
        (0x3B, 0x02, 0x083B),
        (0x3B, 0x08, 0x203B),
        (0x3B, 0x06, 0x183B),
        (0x3B, 0x07, 0x1C3B),
        (0x4F, 0x01, 0x044F),
        (0x1a, 0x07, 0x1C1A),
        (0x1a, 0x06, 0x181A),
        (0x1a, 0x03, 0x0C1A),
        (0x1a, 0x02, 0x081A),
        (0x6C, 0x01, 0x046C),
        (0x32, 0x02, 0x0832),
        (0x32, 0x01, 0x0432),
        (0x32, 0x01, 0x0459),
        (0x32, 0x02, 0x0859),
        (0x5B, 0x01, 0x045B),
        (0x1b, 0x01, 0x041B),
        (0x24, 0x01, 0x0424),
        (0x0A, 0x0b, 0x2C0A),
        (0x0A, 0x10, 0x400A),
        (0x0A, 0x0D, 0x340A),
        (0x0A, 0x09, 0x240A),
        (0x0A, 0x05, 0x140A),
        (0x0A, 0x07, 0x1C0A),
        (0x0A, 0x0C, 0x300A),
        (0x0A, 0x11, 0x440A),
        (0x0A, 0x04, 0x100A),
        (0x0A, 0x12, 0x480A),
        (0x0A, 0x02, 0x080A),
        (0x0A, 0x13, 0x4C0A),
        (0x0A, 0x06, 0x180A),
        (0x0A, 0x0F, 0x3C0A),
        (0x0A, 0x0A, 0x280A),
        (0x0A, 0x14, 0x500A),
        (0x0A, 0x03, 0x0C0A),
        (0x0A, 0x01, 0x040A),
        (0x0A, 0x15, 0x540A),
        (0x0A, 0x0E, 0x380A),
        (0x0A, 0x08, 0x200A),
        (0x41, 0x01, 0x0441),
        (0x1D, 0x02, 0x081D),
        (0x1D, 0x01, 0x041D),
        (0x5A, 0x01, 0x045A),
        (0x28, 0x01, 0x0428),
        (0x5F, 0x02, 0x085F),
        (0x49, 0x01, 0x0449),
        (0x49, 0x02, 0x0849),
        (0x44, 0x01, 0x0444),
        (0x4A, 0x01, 0x044A),
        (0x1E, 0x01, 0x041E),
        (0x51, 0x01, 0x0451),
        (0x73, 0x02, 0x0873),
        (0x73, 0x01, 0x0473),
        (0x1F, 0x01, 0x041F),
        (0x42, 0x01, 0x0442),
        (0x22, 0x01, 0x0422),
        (0x2E, 0x01, 0x042E),
        (0x20, 0x02, 0x0820),
        (0x20, 0x01, 0x0420),
        (0x80, 0x01, 0x0480),
        (0x43, 0x02, 0x0843),
        (0x43, 0x01, 0x0443),
        (0x03, 0x02, 0x0803),
        (0x2A, 0x01, 0x042A),
        (0x52, 0x01, 0x0452),
        (0x88, 0x01, 0x0488),
        (0x78, 0x01, 0x0478),
        (0x6A, 0x01, 0x046A),
    ])</code></pre>
</details>
</dd>
<dt id="refinery.shell.pesig"><code class="flex name class">
<span>class <span class="ident">pesig</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts the contents of the IMAGE_DIRECTORY_ENTRY_SECURITY entry of a PE file,
i.e. the digital signatures in DER format.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pe/pesig.py#L9-L30" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pesig(Unit):
    &#34;&#34;&#34;
    Extracts the contents of the IMAGE_DIRECTORY_ENTRY_SECURITY entry of a PE file,
    i.e. the digital signatures in DER format.
    &#34;&#34;&#34;
    def process(self, data: bytearray):
        view = memoryview(data)
        pe = lief.load_pe_fast(view)
        security = pe.data_directory(lief.PE.DataDirectory.TYPES.CERTIFICATE_TABLE)
        self.log_info(F&#39;signature offset: 0x{security.rva:08X}&#39;)
        self.log_info(F&#39;signature length: 0x{security.size:08X}&#39;)
        if security.rva == 0 or security.size == 0:
            raise ValueError(&#39;IMAGE_DIRECTORY_ENTRY_SECURITY is corrupt.&#39;)
        sgnoff = security.rva + 8
        sgnend = sgnoff + security.size
        length, _, _ = struct.unpack(&#39;&lt;IHH&#39;, view[sgnoff - 8:sgnoff])
        signature = view[sgnoff:sgnend]
        if len(signature) + 8 != length:
            raise RefineryPartialResult(
                F&#39;Found {len(signature) + 8} bytes of signature, but length should be {length}.&#39;,
                partial=signature)
        return signature</code></pre>
</details>
</dd>
<dt id="refinery.shell.pestrip"><code class="flex name class">
<span>class <span class="ident">pestrip</span></span>
<span>(</span><span>certificate=False, directories=False, memdump=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Removes the overlay of a PE file and returns the main executable. Use <code><a title="refinery.peoverlay" href="index.html#refinery.peoverlay">peoverlay</a></code> to
extract the overlay.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pe/pestrip.py#L6-L19" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pestrip(OverlayUnit):
    &#34;&#34;&#34;
    Removes the overlay of a PE file and returns the main executable. Use `refinery.peoverlay` to
    extract the overlay.
    &#34;&#34;&#34;

    def process(self, data: bytearray) -&gt; bytearray:
        size = self._get_size(data)
        try:
            data[size:] = []
        except Exception:
            data = data[:size]
        else:
            return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.pf"><code class="flex name class">
<span>class <span class="ident">pf</span></span>
<span>(</span><span>*formats, variable=None, separator=' ', multiplex=False, binary=False, unescape=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Stands for "Print Format": Transform a given chunk by applying a format string operation. The
positional format string placeholder <code>{}</code> will be replaced by the incoming data, named
placeholders have to exist as meta variables in the current chunk. For example, the following
pipeline can be used to print all files in a given directory with their corresponding SHA-256
hash:</p>
<pre><code>ef ** [| sha256 -t | pf {} {path} ]]
</code></pre>
<p>By default, format string arguments are simply joined along a space character to form a single
format string.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/strings/pf.py#L10-L62" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pf(Unit):
    &#34;&#34;&#34;
    Stands for &#34;Print Format&#34;: Transform a given chunk by applying a format string operation. The
    positional format string placeholder `{}` will be replaced by the incoming data, named
    placeholders have to exist as meta variables in the current chunk. For example, the following
    pipeline can be used to print all files in a given directory with their corresponding SHA-256
    hash:

        ef ** [| sha256 -t | pf {} {path} ]]

    By default, format string arguments are simply joined along a space character to form a single
    format string.
    &#34;&#34;&#34;

    def __init__(
        self,
        *formats: Param[buf, Arg.Binary(help=&#39;Format strings.&#39;, metavar=&#39;format&#39;)],
        variable: Param[str, Arg.String(&#39;-n&#39;, metavar=&#39;N&#39;, help=&#39;Store the formatted string in a meta variable.&#39;)] = None,
        separator: Param[str, Arg.String(&#39;-s&#39;, group=&#39;SEP&#39;, metavar=&#39;S&#39;,
            help=&#39;Separator to insert between format strings. The default is a space character.&#39;)] = &#39; &#39;,
        multiplex: Param[bool, Arg.Switch(&#39;-m&#39;, group=&#39;SEP&#39;,
            help=&#39;Do not join the format strings along the separator, generate one output for each.&#39;)] = False,
        binary: Param[bool, Arg.Switch(&#39;-b&#39;, help=&#39;Use the binary formatter instead of the string formatter.&#39;)] = False,
        unescape: Param[bool, Arg.Switch(&#39;-e&#39;, help=&#39;Interpret escape sequences in format strings.&#39;)] = False,
    ):
        def fixfmt(fmt: bytes | str):
            if unescape:
                if isinstance(fmt, str):
                    fmt = fmt.encode(&#39;latin1&#39;)
                return bytes(fmt).decode(&#39;unicode-escape&#39;)
            elif not isinstance(fmt, str):
                fmt = bytes(fmt).decode(self.codec)
            return fmt
        _formats = [fixfmt(f) for f in formats]
        if not multiplex:
            _formats = [fixfmt(separator).join(_formats)]
        super().__init__(formats=_formats, variable=variable, binary=binary)

    def process(self, data):
        meta = metavars(data)
        meta.ghost = True
        args = [data]
        variable = self.args.variable
        if self.args.binary:
            formatter = partial(meta.format_bin, codec=self.codec, args=args)
        else:
            def formatter(spec):
                return meta.format_str(spec, self.codec, args).encode(self.codec)
        for spec in self.args.formats:
            result = formatter(spec)
            if variable is not None:
                result = self.labelled(data, **{variable: result})
            yield result</code></pre>
</details>
</dd>
<dt id="refinery.shell.pick"><code class="flex name class">
<span>class <span class="ident">pick</span></span>
<span>(</span><span>*bounds)</span>
</code></dt>
<dd>
<section class="desc"><p>Picks sequences from the array of multiple inputs. For example, <code>pick 0 2:</code>
will return all but the second ingested input (which has index <code>1</code>).</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/pick.py#L44-L116" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pick(Unit):
    &#34;&#34;&#34;
    Picks sequences from the array of multiple inputs. For example, `pick 0 2:`
    will return all but the second ingested input (which has index `1`).
    &#34;&#34;&#34;
    def __init__(self, *bounds: Param[slice, Arg.Bounds(nargs=&#39;*&#39;, default=[0])]):
        super().__init__(bounds=[sliceobj(s) for s in bounds])

    def process(self, data: Chunk):
        if not data.visible:
            yield data
            return

        state: _PickState = data.temp
        a = state.accessor
        lower = a.start
        upper = a.stop

        if lower is not None:
            lower -= state.discarded
        if upper is not None:
            upper -= state.discarded
        if state.consumed:
            yield from state.remaining[slice(lower, upper, a.step)]
            return

        while lower:
            try:
                chunk = next(state.chunks)
            except StopIteration:
                upper = None
                break
            if chunk.visible:
                lower -= 1
                upper -= 1
                state.discarded += 1
            else:
                yield chunk
        if upper is None:
            yield from state.chunks
            return
        while upper:
            try:
                chunk = next(state.chunks)
            except StopIteration:
                break
            if chunk.visible:
                upper -= 1
                state.discarded += 1
            yield chunk

    def filter(self, chunks: Iterable[Chunk]):
        chunks = begin(chunks)
        if chunks is None:
            return
        container, chunks = chunks
        if container.scope &lt; 1:
            raise RuntimeError(F&#39;{self.__class__.__name__} cannot be used outside a frame; maybe you meant to use snip?&#39;)
        container = container.copy()
        container.visible = True
        state = _PickState(deque(self.args.bounds), chunks)
        while state.next():
            if not state.consumed:
                if not state.discardable():
                    self.log_debug(F&#39;consumed input into buffer after {state.discarded} skips&#39;)
                    for chunk in state.chunks:
                        if not chunk.visible:
                            yield chunk
                            continue
                        state.remaining.append(chunk)
                    state.consumed = True
            container.temp = state
            yield container</code></pre>
</details>
</dd>
<dt id="refinery.shell.pkcs7"><code class="flex name class">
<span>class <span class="ident">pkcs7</span></span>
</code></dt>
<dd>
<section class="desc"><p>Converts PKCS7 encoded data to a JSON representation.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pkcs7.py#L13-L148" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pkcs7(Unit):
    &#34;&#34;&#34;
    Converts PKCS7 encoded data to a JSON representation.
    &#34;&#34;&#34;
    @Unit.Requires(&#39;asn1crypto&#39;, [&#39;default&#39;, &#39;extended&#39;])
    def _asn1crypto():
        import asn1crypto
        import asn1crypto.cms
        import asn1crypto.core
        import asn1crypto.x509
        return asn1crypto

    def process(self, data):
        asn1 = self._asn1crypto.core
        cms = self._asn1crypto.cms
        signature = cms.ContentInfo.load(convert(data, bytes))

        def unsign(data):
            if isinstance(data, int):
                size = data.bit_length()
                if data &lt; 0:
                    data = (1 &lt;&lt; (size + 1)) - ~data - 1
                if data &gt; 0xFFFFFFFF_FFFFFFFF:
                    size, r = divmod(size, 8)
                    size += bool(r)
                    data = data.to_bytes(size, &#39;big&#39;).hex()
                return data
            elif isinstance(data, dict):
                return {key: unsign(value) for key, value in data.items()}
            elif isinstance(data, list):
                return [unsign(x) for x in data]
            else:
                return data

        class SpcString(asn1.Choice):
            _alternatives = [
                (&#39;unicode&#39;, asn1.BMPString, {&#39;implicit&#39;: 0}),
                (&#39;ascii&#39;, asn1.IA5String, {&#39;implicit&#39;: 1})
            ]

        SpcUuid = asn1.OctetString

        class SpcSerializedObject(asn1.Sequence):
            _fields = [
                (&#39;classId&#39;, SpcUuid),
                (&#39;serializedData&#39;, asn1.OctetString),
            ]

        class SpcLink(asn1.Choice):
            _alternatives = [
                (&#39;url&#39;, asn1.IA5String, {&#39;implicit&#39;: 0}),
                (&#39;monikier&#39;, SpcSerializedObject, {&#39;implicit&#39;: 1}),
                (&#39;file&#39;, SpcString, {&#39;explicit&#39;: 2})
            ]

        class SpcSpOpusInfo(asn1.Sequence):
            _fields = [
                (&#39;programName&#39;, SpcString, {&#39;optional&#39;: True, &#39;explicit&#39;: 0}),
                (&#39;moreInfo&#39;, SpcLink, {&#39;optional&#39;: True, &#39;explicit&#39;: 1}),
            ]

        class SetOfInfos(asn1.SetOf):
            _child_spec = SpcSpOpusInfo

        cms.CMSAttributeType._map[&#39;1.3.6.1.4.1.311.2.1.12&#39;] = &#39;authenticode_info&#39;
        cms.CMSAttribute._oid_specs[&#39;authenticode_info&#39;] = SetOfInfos

        class ParsedASN1ToJSON(BytesAsStringEncoder):
            unit = self

            @classmethod
            def _is_keyval(cls, obj):
                return (
                    isinstance(obj, dict)
                    and set(obj.keys()) == {&#39;type&#39;, &#39;values&#39;}
                    and len(obj[&#39;values&#39;]) == 1
                )

            @classmethod
            def handled(cls, obj) -&gt; bool:
                return BytesAsStringEncoder.handled(obj) or cls._is_keyval(obj)

            def encode_bytes(self, obj: bytes):
                with suppress(Exception):
                    string = obj.decode(&#39;latin1&#39;)
                    if string.isprintable():
                        return string
                return super().encode_bytes(obj)

            def default(self, obj):
                if self._is_keyval(obj):
                    return dict(type=obj[&#39;type&#39;], value=obj[&#39;values&#39;][0])
                with suppress(TypeError):
                    return super().default(obj)
                if isinstance(obj, (set, tuple)):
                    return list(obj)
                if isinstance(obj, datetime):
                    return str(obj)
                dict_result = {}
                list_result = None
                if isinstance(obj, self.unit._asn1crypto.x509.Certificate):
                    dict_result.update(fingerprint=obj.sha1.hex())
                if isinstance(obj, asn1.BitString):
                    return {&#39;bit_string&#39;: obj.native}
                with suppress(Exception):
                    list_result = list(obj)
                    if all(isinstance(k, str) for k in list_result):
                        dict_result.update((key, obj[key]) for key in list_result)
                if dict_result:
                    return dict_result
                if list_result is not None:
                    return list_result
                if isinstance(obj, self.unit._asn1crypto.cms.CertificateChoices):
                    return obj.chosen
                if isinstance(obj, asn1.Sequence):
                    children = obj.children
                    if children:
                        return children
                    return obj.dump()
                with suppress(Exception):
                    return obj.native
                if isinstance(obj, asn1.Any):
                    parsed = None
                    with suppress(Exception):
                        parsed = obj.parse()
                    if parsed:
                        return parsed
                    return obj.dump()
                if isinstance(obj, asn1.Asn1Value):
                    return obj.dump()
                raise ValueError(F&#39;Unable to determine JSON encoding of {obj.__class__.__name__} object.&#39;)

        with ParsedASN1ToJSON as encoder:
            encoded = encoder.dumps(signature)
            converted = unsign(json.loads(encoded))
            return json.dumps(converted, indent=4).encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.pkcs7sig"><code class="flex name class">
<span>class <span class="ident">pkcs7sig</span></span>
<span>(</span><span>tabular=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Converts PKCS7 encoded signatures into a human-readable JSON representation. This can be used
to parse authenticode signatures appended to files that are not PE files to get the same output
that is produced by the pemeta unit.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pkcs7sig.py#L9-L20" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pkcs7sig(Unit):
    &#34;&#34;&#34;
    Converts PKCS7 encoded signatures into a human-readable JSON representation. This can be used
    to parse authenticode signatures appended to files that are not PE files to get the same output
    that is produced by the pemeta unit.
    &#34;&#34;&#34;
    def __init__(self, tabular: Param[bool, Arg(&#39;-t&#39;, help=&#39;Print information in a table rather than as JSON&#39;)] = False):
        super().__init__(tabular=tabular)

    def process(self, data):
        json = pemeta.parse_signature(data)
        yield from ppjson(tabular=self.args.tabular)._pretty_output(json, indent=4, ensure_ascii=False)</code></pre>
</details>
</dd>
<dt id="refinery.shell.pkw"><code class="flex name class">
<span>class <span class="ident">pkw</span></span>
</code></dt>
<dd>
<section class="desc"><p>This unit implements PKWare decompression.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/pkw.py#L7-L69" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pkw(Unit):
    &#34;&#34;&#34;
    This unit implements PKWare decompression.
    &#34;&#34;&#34;
    def process(self, data):

        def read_from_table(table: dict[tuple[int, int], int], start: int, stop: int):
            value = length = 0
            while length &lt; start:
                value &lt;&lt;= 1
                value |= getint(1)
                length += 1
            while length &lt; stop:
                try:
                    return table[length, value]
                except KeyError:
                    value &lt;&lt;= 1
                    value |= getint(1)
                    length += 1
            raise ValueError(
                &#39;Failed to decode a symbol in the compressed data stream.&#39;)

        reader = StructReader(data)
        codelit = reader.u8()  # First byte is 0 if literals are uncoded, otherwise 1
        maxdict = reader.u8()  # Second byte is 4, 5, or 6 (max size of dictionary)

        if not 0 &lt;= codelit &lt;= 1:
            raise ValueError(F&#39;Invalid literal encoding value {codelit}.&#39;)

        if not 4 &lt;= maxdict &lt;= 6:
            raise ValueError(F&#39;Invalid dictionary size {maxdict}.&#39;)

        output = MemoryFile()
        getint = reader.read_integer

        while not reader.eof:
            try:
                if not getint(1):
                    if codelit:
                        code = read_from_table(_LITERALS, 4, 14)
                    else:
                        code = getint(8)
                    output.write_byte(code)
                else:
                    length = read_from_table(_COPY_LENGTHS, 2, 0x10)
                    if length == 519:
                        break
                    offset = read_from_table(_COPY_OFFSETS, 2, 0x09)
                    more = (2 if length == 2 else maxdict)
                    offset &lt;&lt;= more
                    offset += getint(more)
                    offset += 1
                    output.replay(offset, length)
            except Exception as E:
                if not (out := output.getvalue()):
                    raise
                raise RefineryPartialResult(str(E), out) from E

        return output.getvalue()

    @classmethod
    def handles(cls, data) -&gt; bool:
        return (len(data) &gt; 2) and (0 &lt;= data[0] &lt;= 1) and (4 &lt;= data[1] &lt;= 6)</code></pre>
</details>
</dd>
<dt id="refinery.shell.pop"><code class="flex name class">
<span>class <span class="ident">pop</span></span>
<span>(</span><span>*names)</span>
</code></dt>
<dd>
<section class="desc"><p>In processing order, remove visible chunks from the current frame and store their contents in
the given meta variables on all chunks that remain. All chunks in the input stream are
consequently made visible again. If pop is used at the end of a frame, then variables will be
local to the parent frame. A pop instruction has the following format:</p>
<pre><code>count | @ | name[=source][:conversion]
</code></pre>
<p>If the instruction is an integer, it is interpreted as <code>count</code>, specifying a number of chunks
to be skipped from the frame without storing them. The letter "@" can be used to
remove a single chunk from the input and merge all of its meta data into the ones that follow.
Otherwise, the pop instruction consists of the name of the variable to be created, an optional
source variable name, and an optional conversion sequence. If no source variable is specified,
the chunk contents are used as the source. The conversion is a sequence of multibin handlers
that are applied to the source data from right to left before storing it.
For example, the argument <code>k:le:b64</code> first decodes the chunk data using base64, then converts
it to an integer in little endian format, and store the integer result in the variable <code>k</code>. The
visual aid is that the content is passed from right to left through all conversions, into the
variable <code>k</code>. Similarly, the argument k=size will store the current chunk's size in <code>k</code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/pop.py#L68-L143" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pop(Unit):
    &#34;&#34;&#34;
    In processing order, remove visible chunks from the current frame and store their contents in
    the given meta variables on all chunks that remain. All chunks in the input stream are
    consequently made visible again. If pop is used at the end of a frame, then variables will be
    local to the parent frame. A pop instruction has the following format:

        count | {_MERGE_META} | name[{_CHERRYPICK}source][{_CONVERSION}conversion]

    If the instruction is an integer, it is interpreted as `count`, specifying a number of chunks
    to be skipped from the frame without storing them. The letter &#34;{_MERGE_META}&#34; can be used to
    remove a single chunk from the input and merge all of its meta data into the ones that follow.
    Otherwise, the pop instruction consists of the name of the variable to be created, an optional
    source variable name, and an optional conversion sequence. If no source variable is specified,
    the chunk contents are used as the source. The conversion is a sequence of multibin handlers
    that are applied to the source data from right to left before storing it.
    For example, the argument `k:le:b64` first decodes the chunk data using base64, then converts
    it to an integer in little endian format, and store the integer result in the variable `k`. The
    visual aid is that the content is passed from right to left through all conversions, into the
    variable `k`. Similarly, the argument k=size will store the current chunk&#39;s size in `k`.
    &#34;&#34;&#34;
    def __init__(
        self,
        *names: Param[str, Arg.String(metavar=&#39;instruction&#39;, help=&#39;A sequence of instructions, see above.&#39;)]
    ):
        if not names:
            names = _MERGE_META,
        super().__init__(names=[_popcount(n) for n in names])

    def process(self, data):
        return data

    def filter(self, chunks: Iterable[Chunk]):
        invisible = []
        variables = {}
        remaining: Iterator[_popcount] = iter(self.args.names)

        it = iter(chunks)
        pop = next(remaining).reset()
        done = False

        for chunk in it:
            if not chunk.visible:
                self.log_debug(&#39;buffering invisible chunk&#39;)
                invisible.append(chunk)
                continue
            try:
                while not pop.into(variables, chunk):
                    pop = next(remaining).reset()
            except StopIteration:
                done = True
                invisible.append(chunk)
                break

        if not done and pop.done:
            try:
                next(remaining)
            except StopIteration:
                done = True

        if not done:
            msg = &#39;Not all variables could be assigned.&#39;
            if not self.leniency:
                raise ValueError(F&#39;{msg} Increase leniency to downgrade this failure to a warning.&#39;)
            self.log_warn(msg)

        nesting = self.args.nesting

        for chunk in chain(invisible, it):
            meta = chunk.meta
            meta.update(variables)
            if nesting &lt; 0:
                for name in variables:
                    meta.set_scope(name, chunk.scope + nesting)
            chunk.visible = True
            yield chunk</code></pre>
</details>
</dd>
<dt id="refinery.shell.ppjscript"><code class="flex name class">
<span>class <span class="ident">ppjscript</span></span>
<span>(</span><span>indent=4, strip_comments=False, strip_lines=False, keep_lines=False, keep_escapes=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Pretty-prints JavaScript without any reflection or evaluation.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/sinks/ppjscript.py#L7-L58" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ppjscript(Unit):
    &#34;&#34;&#34;
    Pretty-prints JavaScript without any reflection or evaluation.
    &#34;&#34;&#34;
    def __init__(
        self,
        indent: Param[int, Arg.Number(&#39;-i&#39;, help=(
            &#39;Number of space characters used for indentation in the output. Default is {default}.&#39;))] = 4,
        strip_comments: Param[bool, Arg.Switch(&#39;-c&#39;, help=(
            &#39;Remove all comments from the input before pretty-printing.&#39;))] = False,
        strip_lines: Param[bool, Arg.Switch(&#39;-b&#39;, group=&#39;LINES&#39;, help=(
            &#39;Remove all line breaks after potentially stripping comments, before beautifying.&#39;))] = False,
        keep_lines: Param[bool, Arg.Switch(&#39;-B&#39;, group=&#39;LINES&#39;, help=(
            &#39;Preserve line breaks as they occur in the input.&#39;))] = False,
        keep_escapes: Param[bool, Arg.Switch(&#39;-E&#39;, help=(
            &#39;Preserve unnecessary escape sequences in string literals.&#39;))] = False,
    ):
        return super().__init__(
            indent=indent,
            strip_comments=strip_comments,
            strip_lines=strip_lines,
            keep_lines=keep_lines,
            keep_escapes=keep_escapes,
        )

    @Unit.Requires(&#39;jsbeautifier&#39;, [&#39;display&#39;, &#39;extended&#39;])
    def _jsb():
        import jsbeautifier
        import jsbeautifier.unpackers.javascriptobfuscator

        # TODO: This is a workaround for the following bug:
        # https://github.com/beautify-web/js-beautify/issues/1350
        jsbeautifier.unpackers.javascriptobfuscator.detect = lambda *_: False
        return jsbeautifier

    def process(self, data: bytearray):
        if self.args.strip_comments:
            from refinery.units.obfuscation.js.comments import deob_js_comments
            code = data | deob_js_comments | str
        else:
            code = data.decode(self.codec)
        if self.args.strip_lines:
            code = &#39; &#39;.join(code.splitlines(False))
        options = self._jsb.default_options()
        options.eval_code = False
        options.indent_size = self.args.indent
        options.unescape_strings = not self.args.keep_escapes
        options.preserve_newlines = self.args.keep_lines
        options.indent_level = 0
        options.keep_array_indentation = False
        return self._jsb.beautify(
            code.strip(), options).encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.ppjson"><code class="flex name class">
<span>class <span class="ident">ppjson</span></span>
<span>(</span><span>tabular=False, indent=4)</span>
</code></dt>
<dd>
<section class="desc"><p>Expects JSON input data and outputs it in a neatly formatted manner.
If the indentation is set to zero, the output is minified.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/sinks/ppjson.py#L18-L65" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ppjson(Unit):
    &#34;&#34;&#34;
    Expects JSON input data and outputs it in a neatly formatted manner.
    If the indentation is set to zero, the output is minified.
    &#34;&#34;&#34;
    _TRAILING_COMMA = re.compile(BR&#39;,\s*(}|])&#39;)

    def __init__(
        self,
        tabular: Param[bool, Arg.Switch(&#39;-t&#39;, group=&#39;OUT&#39;, help=&#39;Convert JSON input into a flattened table.&#39;)] = False,
        indent: Param[int, Arg.Number(&#39;-i&#39;, group=&#39;OUT&#39;, help=&#39;Number of spaces used for indentation. Default is {default}.&#39;)] = 4
    ):
        return super().__init__(indent=indent, tabular=tabular)

    def _pretty_output(self, parsed, **kwargs):
        encoded = json.dumps(parsed, **kwargs)
        if self.args.tabular:
            table = list(flattened(json.loads(encoded)))
            width = max(len(key) for key, _ in table)
            tsize = get_terminal_size(80) - width - 4
            for key, value in table:
                if isinstance(value, str):
                    value = value.strip()
                    if not is_printable(value) and all(ord(c) &lt; 0x100 for c in value):
                        value = value.encode(&#39;latin1&#39;).hex(&#39;:&#39;)
                value = str(value).rstrip()
                value = textwrap.wrap(value, tsize)
                it = iter(value)
                try:
                    item = next(it)
                except StopIteration:
                    continue
                yield F&#39;{key:&lt;{width}} : {item}&#39;.encode(self.codec)
                for wrap in it:
                    yield F&#39;{&#34;&#34;:&lt;{width + 3}}{wrap}&#39;.encode(self.codec)
        else:
            yield encoded.encode(self.codec)

    def process(self, data):
        if self._TRAILING_COMMA.search(data):
            def smartfix(match):
                k = match.start()
                return match.group(0 if any(k in s for s in strings) else 1)
            from refinery.lib.patterns import formats
            strings = {range(*m.span()) for m in formats.string.finditer(data)}
            data = self._TRAILING_COMMA.sub(smartfix, data)
        kwargs = {&#39;indent&#39;: self.args.indent} if self.args.indent else {&#39;separators&#39;: (&#39;,&#39;, &#39;:&#39;)}
        yield from self._pretty_output(json.loads(data), **kwargs)</code></pre>
</details>
</dd>
<dt id="refinery.shell.ppxml"><code class="flex name class">
<span>class <span class="ident">ppxml</span></span>
<span>(</span><span>indent=4, header=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Expects XML input data and outputs it in a neatly formatted manner.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/sinks/ppxml.py#L11-L78" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ppxml(Unit):
    &#34;&#34;&#34;
    Expects XML input data and outputs it in a neatly formatted manner.
    &#34;&#34;&#34;

    def __init__(self,
        indent: Param[int, Arg.Number(&#39;-i&#39;, help=(
            &#39;Controls the amount of space characters used for indentation in the output. Default is 4.&#39;))] = 4,
        header: Param[bool, Arg.Switch(&#39;-x&#39;, help=&#39;Add an XML header to the formatted output.&#39;)] = False
    ):
        super().__init__(indent=indent, header=header)

    def process(self, data):

        pad = self.args.indent * &#39; &#39;
        etm = {}

        try:
            dom = ForgivingParse(data, etm)
        except Exception:
            from refinery.lib.meta import metavars
            msg = &#39;error parsing as XML, returning original content&#39;
            path = metavars(data).get(&#39;path&#39;)
            if path:
                msg = F&#39;{msg}: {path}&#39;
            self.log_warn(msg)
            return data

        def indent(element, level=0, more_sibs=False):
            &#34;&#34;&#34;
            The credit for this one goes to:
            https://stackoverflow.com/a/12940014
            &#34;&#34;&#34;
            indentation = &#39;\n&#39;
            if level:
                indentation += (level - 1) * pad
            childcount = len(element)
            if childcount:
                if not element.text or not element.text.strip():
                    element.text = indentation + pad
                    if level:
                        element.text += pad
                for count, child in enumerate(element):
                    indent(child, level + 1, count &lt; childcount - 1)
                if level and (not element.tail or element.tail.isspace()):
                    element.tail = indentation
                    if more_sibs:
                        element.tail += pad
            elif level and (not element.tail or element.tail.isspace()):
                element.tail = indentation
                if more_sibs: element.tail += pad

        indent(dom.getroot())

        with io.BytesIO() as output:
            dom.write(output, encoding=self.codec, xml_declaration=self.args.header)
            result = output.getvalue()

        for uid, key in etm.items():
            entity = F&#39;&amp;{key};&#39;.encode(self.codec)
            needle = uid.encode(self.codec)
            result = result.replace(needle, entity)

        return result

    @classmethod
    def handles(cls, data):
        return is_likely_xml(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.push"><code class="flex name class">
<span>class <span class="ident">push</span></span>
<span>(</span><span>data=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>The unit inserts an additional chunk before each input chunk and moves the original
data out of scope. This chunk is considered the "original" data, while the one inserted
in front of it is used as an intermediate result. By default, this intermediate data is
a copy of the input data. For example:</p>
<pre><code>emit key=value | push [[| rex =(.*)$ {1} | pop v ]| repl var:v censored ]
</code></pre>
<p>will output <code>key=censored</code>. The application of <code><a title="refinery.rex" href="index.html#refinery.rex">rex</a></code> turns the (duplicated)
data into just the value, which is then stored in the variable <code>v</code>. The application
of <code><a title="refinery.repl" href="index.html#refinery.repl">repl</a></code> replaces this value with the hard-coded string <code>censored</code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/push.py#L7-L35" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class push(Unit):
    &#34;&#34;&#34;
    The unit inserts an additional chunk before each input chunk and moves the original
    data out of scope. This chunk is considered the &#34;original&#34; data, while the one inserted
    in front of it is used as an intermediate result. By default, this intermediate data is
    a copy of the input data. For example:

        emit key=value | push [[| rex =(.*)$ {1} | pop v ]| repl var:v censored ]

    will output `key=censored`. The application of `refinery.rex` turns the (duplicated)
    data into just the value, which is then stored in the variable `v`. The application
    of `refinery.repl` replaces this value with the hard-coded string `censored`.
    &#34;&#34;&#34;
    def __init__(self, data: Param[buf, Arg(help=&#39;The data to be pushed, by default a copy of the input.&#39;)] = B&#39;&#39;):
        super().__init__(data=data)

    def process(self, data: Chunk):
        src = self.args.data
        tos = data.copy(meta=True, data=False)
        tos[:] = src or data
        if self.args.nesting &gt; 0:
            data.set_next_scope(False)
        else:
            try:
                data.visible = False
            except AttributeError:
                self.log_warn(&#39;application has no effect outside frame.&#39;)
        yield data
        yield tos</code></pre>
</details>
</dd>
<dt id="refinery.shell.put"><code class="flex name class">
<span>class <span class="ident">put</span></span>
<span>(</span><span>name, value=&lt;object object&gt;)</span>
</code></dt>
<dd>
<section class="desc"><p>Can be used to add a meta variable to the processed chunk. Note that meta variables
cease to exist outside a frame.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/put.py#L13-L44" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class put(Unit):
    &#34;&#34;&#34;
    Can be used to add a meta variable to the processed chunk. Note that meta variables
    cease to exist outside a frame.
    &#34;&#34;&#34;
    def __init__(
        self,
        name: Param[str, Arg.String(help=&#39;The name of the variable to be used.&#39;)],
        value: Param[isq, Arg.NumSeq(check=False, help=(
            &#39;The value for the variable. If no value is given, the entire current chunk is stored.&#39;
        ))] = _EMPTY
    ):
        super().__init__(name=check_variable_name(name), value=value)

    def process(self, data: Chunk):
        value = self.args.value
        if value is _EMPTY:
            value = data
        if not isinstance(value, (int, float)) and not isbuffer(value):
            try:
                len(value)
            except TypeError:
                if isinstance(value, itertools.repeat):
                    value = next(value)
                if not isinstance(value, (int, float)):
                    raise NotImplementedError(F&#39;put does not support {value.__class__.__name__} values.&#39;)
            else:
                if not isinstance(value, (dict, list)):
                    value = list(value)
        self.log_debug(F&#39;storing {typename(value)}:&#39;, value, clip=True)
        data.meta[self.args.name] = value
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.pyc"><code class="flex name class">
<span>class <span class="ident">pyc</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Decompiles Python bytecode (PYC) files back to source code. A known limitation is that it does
not work on recent Python versions, but anything below 3.9 should work.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pyc.py#L10-L23" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pyc(ArchiveUnit):
    &#34;&#34;&#34;
    Decompiles Python bytecode (PYC) files back to source code. A known limitation is that it does
    not work on recent Python versions, but anything below 3.9 should work.
    &#34;&#34;&#34;
    def unpack(self, data):
        input_path = metavars(data).get(self.args.path.decode(self.codec))
        for k, code in enumerate(extract_code_from_buffer(bytes(data), input_path)):
            if (co := code.container) is None:
                raise ValueError(&#39;could not find code in buffer&#39;)
            path = co.co_filename or F&#39;__unknown_name_{k:02d}.py&#39;
            date = datetime.fromtimestamp(code.timestamp)
            data = decompile_buffer(code)
            yield self._pack(path, date, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.pym"><code class="flex name class">
<span>class <span class="ident">pym</span></span>
<span>(</span><span>version=None, system=False, redump=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Converts Python-Marshaled code objects to the PYC (Python Bytecode) format. If it is an
older Python version, you can use the <code><a title="refinery.pyc" href="index.html#refinery.pyc">pyc</a></code> unit to then decompile the code, but
for more recent versions a separate Python decompiler will be required.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pym.py#L14-L88" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pym(Unit):
    &#34;&#34;&#34;
    Converts Python-Marshaled code objects to the PYC (Python Bytecode) format. If it is an
    older Python version, you can use the `refinery.pyc` unit to then decompile the code, but
    for more recent versions a separate Python decompiler will be required.
    &#34;&#34;&#34;
    def __init__(
        self,
        version: Param[str | None, Arg.String(&#39;-V&#39;, metavar=&#39;V&#39;,
            help=&#39;Optionally select the (known) Python version.&#39;)] = None,
        system: Param[bool, Arg.Switch(&#39;-s&#39;,
            help=&#39;Try to use the built-in marshal.loads before using the parser.&#39;)] = False,
        redump: Param[bool, Arg.Switch(&#39;-r&#39;,
            help=&#39;Load marshaled code objects before re-dumping them.&#39;)] = False,
    ):
        super().__init__(
            version=version,
            system=system,
            redump=redump,
        )

    def reverse(self, data):
        return marshal.dumps(data)

    def process(self, data):
        def toblob(data):
            if isinstance(data, (bytes, bytearray)):
                self.log_info(&#39;unmarshalled a byte string, returning as is&#39;)
                return data
            if isinstance(data, str):
                self.log_info(F&#39;unmarshalled a string object, encoding as {self.codec}&#39;)
                return data.encode(self.codec)
            if isinstance(data, CodeType):
                self.log_info(&#39;unmarshalled a code object, converting to pyc&#39;)
                pyc = code_header()
                pyc.extend(marshal.dumps(data))
                return pyc
            if isinstance(data, int):
                self.log_info(&#39;unmarshalled an integer, returning big endian encoding&#39;)
                q, r = divmod(data.bit_length(), 8)
                q += int(bool(r))
                return data.to_bytes(q, &#39;big&#39;)
            if isinstance(data, dict):
                with BytesAsStringEncoder as encoder:
                    return encoder.dumps(data).encode(self.codec)
            raise NotImplementedError(
                F&#39;No serialization implemented for object of type {data.__class__.__name__}&#39;)

        if version := self.args.version:
            version = version2tuple(version)

        if version and version != SYS_PYTHON or not self.args.system:
            out = None
        else:
            try:
                out = marshal.loads(data)
            except Exception as error:
                self.log_info(F&#39;the marshal.loads method failed: {error!s}&#39;)
                out = None
            else:
                v = sys.version_info
                self.log_info(F&#39;unmarshaled using the {v.major}.{v.minor}.{v.micro} built-in marshal.loads&#39;)

        if out is None:
            dumpcode = not self.args.redump
            memory = memoryview(data)
            unpacker = Marshal(memory, version=version, dumpcode=dumpcode)
            out = unpacker.object()

        if isinstance(out, (list, tuple, set, frozenset)):
            self.log_info(&#39;object is a collection, converting each item individually&#39;)
            for item in out:
                yield toblob(item)
        else:
            yield toblob(out)</code></pre>
</details>
</dd>
<dt id="refinery.shell.pymstr"><code class="flex name class">
<span>class <span class="ident">pymstr</span></span>
<span>(</span><span>buffers=False, strings=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract string constants from Python-Marshaled objects.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pymstr.py#L8-L29" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pymstr(Unit):
    &#34;&#34;&#34;
    Extract string constants from Python-Marshaled objects.
    &#34;&#34;&#34;
    def __init__(
        self,
        buffers: Param[bool, Arg.Switch(&#39;-b&#39;, help=&#39;Dump byte strings.&#39;)] = False,
        strings: Param[bool, Arg.Switch(&#39;-s&#39;, help=&#39;Dump strings.&#39;)] = False,
    ):
        if not buffers and not strings:
            buffers = strings = True
        super().__init__(buffers=buffers, strings=strings)

    def process(self, data):
        marshaled = Marshal(memoryview(data))
        marshaled.object()
        if self.args.buffers:
            for bs in marshaled.buffers:
                yield bs
        if self.args.strings:
            for us in marshaled.strings:
                yield us.encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.qb"><code class="flex name class">
<span>class <span class="ident">qb</span></span>
<span>(</span><span>*data)</span>
</code></dt>
<dd>
<section class="desc"><p>Short for "queue back": Insert new chunks at the end of the current frame.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/queue.py#L57-L62" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class qb(QueueUnit):
    &#34;&#34;&#34;
    Short for &#34;queue back&#34;: Insert new chunks at the end of the current frame.
    &#34;&#34;&#34;
    def filter(self, chunks: Iterable[Chunk]):
        yield from self._queue(chunks, False)</code></pre>
</details>
</dd>
<dt id="refinery.shell.qf"><code class="flex name class">
<span>class <span class="ident">qf</span></span>
<span>(</span><span>*data)</span>
</code></dt>
<dd>
<section class="desc"><p>Short for "queue front": Insert new chunks at the beginning of the current frame.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/queue.py#L49-L54" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class qf(QueueUnit):
    &#34;&#34;&#34;
    Short for &#34;queue front&#34;: Insert new chunks at the beginning of the current frame.
    &#34;&#34;&#34;
    def filter(self, chunks: Iterable[Chunk]):
        yield from self._queue(chunks, True)</code></pre>
</details>
</dd>
<dt id="refinery.shell.qlz"><code class="flex name class">
<span>class <span class="ident">qlz</span></span>
</code></dt>
<dd>
<section class="desc"><p>This unit implements QuickLZ decompression levels 1 and 3.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/qlz.py#L12-L136" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class qlz(Unit):
    &#34;&#34;&#34;
    This unit implements QuickLZ decompression levels 1 and 3.
    &#34;&#34;&#34;

    def process(self, data):
        source = memoryview(data)
        head = source[0]
        clvl = (head &gt;&gt; 2) &amp; 0x3

        if head &amp; 2:
            self.log_info(&#39;long header detected&#39;)
            size = int.from_bytes(source[5:9], &#39;little&#39;)
            source = source[9:]
        else:
            self.log_info(&#39;short header detected&#39;)
            size = source[3]
            source = source[3:]
        if head &amp; 1 != 1:
            self.log_warn(&#39;header indicates that data is uncompressed, returning remaining data&#39;)
            return source
        else:
            self.log_info(F&#39;compression level {clvl}, decompressed size {SizeInt(size)!r}&#39;)

        def fetchhash():
            return int.from_bytes(destination[hashvalue + 1:hashvalue + 4], byteorder=&#39;little&#39;)

        codeword = 1
        destination = bytearray()
        hashtable = [0] * _HASH_VALUES
        hashvalue = -1
        last_matchstart = size - _UNCONDITIONAL_MATCHLEN - _UNCOMPRESSED_END - 1
        fetch = 0

        if clvl == 2:
            raise ValueError(&#34;This version only supports level 1 and 3&#34;)
        while source:
            if codeword == 1:
                codeword = int.from_bytes(source[:4], byteorder=&#39;little&#39;)
                source = source[4:]
                if len(destination) &lt;= last_matchstart:
                    c = 3 if clvl == 1 else 4
                    fetch = int.from_bytes(source[:c], byteorder=&#39;little&#39;)
            if codeword &amp; 1:
                codeword = codeword &gt;&gt; 1
                if clvl == 1:
                    hash = (fetch &gt;&gt; 4) &amp; 0xFFF
                    offset = hashtable[hash]
                    if fetch &amp; 0xF:
                        matchlen = (fetch &amp; 0xF) + 2
                        source = source[2:]
                    else:
                        matchlen = source[2]
                        source = source[3:]
                else:
                    if (fetch &amp; 3) == 0:
                        delta = (fetch &amp; 0xFF) &gt;&gt; 2
                        matchlen = 3
                        source = source[1:]
                    elif (fetch &amp; 2) == 0:
                        delta = (fetch &amp; 0xFFFF) &gt;&gt; 2
                        matchlen = 3
                        source = source[2:]
                    elif (fetch &amp; 1) == 0:
                        delta = (fetch &amp; 0xFFFF) &gt;&gt; 6
                        matchlen = ((fetch &gt;&gt; 2) &amp; 15) + 3
                        source = source[2:]
                    elif (fetch &amp; 127) != 3:
                        delta = (fetch &gt;&gt; 7) &amp; 0x1FFFF
                        matchlen = ((fetch &gt;&gt; 2) &amp; 0x1F) + 2
                        source = source[3:]
                    else:
                        delta = fetch &gt;&gt; 15
                        matchlen = ((fetch &gt;&gt; 7) &amp; 255) + 3
                        source = source[4:]
                    offset = (len(destination) - delta) &amp; 0xFFFFFFFF

                for i in range(offset, offset + matchlen):
                    destination.append(destination[i])

                if clvl == 1:
                    fetch = fetchhash()
                    while hashvalue &lt; len(destination) - matchlen:
                        hashvalue += 1
                        hash = ((fetch &gt;&gt; 12) ^ fetch) &amp; _HASH_MASK
                        hashtable[hash] = hashvalue
                        fetch = fetch &gt;&gt; 8 &amp; 0xFFFF
                        try:
                            fetch |= destination[hashvalue + 3] &lt;&lt; 16
                        except IndexError:
                            pass
                    fetch = int.from_bytes(source[:3], byteorder=&#39;little&#39;)
                else:
                    fetch = int.from_bytes(source[:4], byteorder=&#39;little&#39;)
                hashvalue = len(destination) - 1
            else:
                if len(destination) &lt;= last_matchstart:
                    destination.append(source[0])
                    source = source[1:]
                    codeword = codeword &gt;&gt; 1
                    if clvl == 1:
                        while hashvalue &lt; len(destination) - 3:
                            fetch2 = fetchhash()
                            hashvalue += 1
                            hash = ((fetch2 &gt;&gt; 12) ^ fetch2) &amp; _HASH_MASK
                            hashtable[hash] = hashvalue
                        fetch = fetch &gt;&gt; 8 &amp; 0xFFFF | source[2] &lt;&lt; 16
                    else:
                        fetch = fetch &gt;&gt; 8 &amp; 0xFFFF
                        fetch |= source[2] &lt;&lt; 16
                        fetch |= source[3] &lt;&lt; 24
                else:
                    while len(destination) &lt;= size - 1:
                        if codeword == 1:
                            source = source[4:]
                            codeword = 0x80000000
                        destination.append(source[0])
                        source = source[1:]
                        codeword = codeword &gt;&gt; 1
                    break
        if len(destination) != size:
            raise RefineryPartialResult(
                F&#39;Header indicates decompressed size 0x{size:X}, but 0x{len(destination):X} bytes &#39;
                F&#39;were decompressed.&#39;, destination)
        return destination</code></pre>
</details>
</dd>
<dt id="refinery.shell.qr"><code class="flex name class">
<span>class <span class="ident">qr</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extract information from bar codes, especially QR codes. This unit is a thin proxy around the
pyzbar library, which itself only provides Python bindings for the ZBar library.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/qr.py#L43-L87" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class qr(Unit):
    &#34;&#34;&#34;
    Extract information from bar codes, especially QR codes. This unit is a thin proxy around the
    pyzbar library, which itself only provides Python bindings for the ZBar library.
    &#34;&#34;&#34;
    @Unit.Requires(&#39;pyzbar&#39;, [&#39;formats&#39;, &#39;extended&#39;, &#39;all&#39;], info=_ZBAR_FOOTNOTE)
    def _pyzbar():
        try:
            import pyzbar
            import pyzbar.pyzbar
        except ModuleNotFoundError:
            raise
        except ImportError as ie:
            msg = str(ie).split()
            if &#39;zbar&#39; in msg and &#39;shared&#39; in msg:
                if info := _ZBAR_ON_ERROR:
                    raise RefineryImportError(info)
            raise RefineryImportError(F&#39;there was an unexpected error importing pyzbar: {ie!s}&#39;)
        return pyzbar

    @Unit.Requires(&#39;Pillow&#39;, [&#39;formats&#39;, &#39;extended&#39;, &#39;all&#39;])
    def _image():
        from PIL import Image
        return Image

    def process(self, data):
        try:
            img = self._image.open(MemoryFile(data, output=bytes))
        except ImportError:
            raise
        except Exception:
            raise ValueError(&#39;the input data is not recognized as an image&#39;)
        else:
            bar = self._pyzbar.pyzbar.decode(img)
        for data in bar:
            self.log_debug(data)
            if not (data := getattr(data, &#39;data&#39;, None)):
                continue
            if isinstance(data, str):
                data = data.encode(self.codec)
            if isinstance(data, (bytes, bytearray)):
                yield data
                continue
            self.log_warn(
                F&#39;skipping unknown data generated by zbar: {data!r}&#39;, clip=True)</code></pre>
</details>
</dd>
<dt id="refinery.shell.rabbit"><code class="flex name class">
<span>class <span class="ident">rabbit</span></span>
<span>(</span><span>key, discard=0, stateful=False, iv=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>RABBIT encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/rabbit.py#L90-L102" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rabbit(StreamCipherUnit):
    &#34;&#34;&#34;
    RABBIT encryption and decryption.
    &#34;&#34;&#34;
    key_size = {16}

    def __init__(self, key, discard=0, stateful=False, iv: Param[buf, Arg(&#39;-i&#39;, &#39;--iv&#39;, help=&#39;Optional initialization vector.&#39;)] = B&#39;&#39;):
        super().__init__(key=key, iv=iv, stateful=stateful, discard=discard)

    def keystream(self) -&gt; Iterable[int]:
        if len(self.args.iv) not in (0, 8):
            raise ValueError(&#39;The IV length must be exactly 8 bytes.&#39;)
        return RabbitCipher(self.args.key, self.args.iv)</code></pre>
</details>
</dd>
<dt id="refinery.shell.rc2"><code class="flex name class">
<span>class <span class="ident">rc2</span></span>
<span>(</span><span>key, *, iv=b'', eks=1024, derive_eks=False, padding=None, mode=None, raw=False, little_endian=False, segment_size=0, tag=None, aad=None)</span>
</code></dt>
<dd>
<section class="desc"><p>RC2 encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/rc2.py#L11-L50" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rc2(StandardBlockCipherUnit, cipher=PyCryptoFactoryWrapper(ARC2)):
    &#34;&#34;&#34;
    RC2 encryption and decryption.
    &#34;&#34;&#34;

    def __init__(
        self, key, *,
        iv=b&#39;&#39;,
        eks: Param[int, Arg.Number(&#39;-k&#39;, &#39;--eks&#39;, group=&#39;EKS&#39;,
            help=&#39;Set the effective key size. Default is {default}.&#39;)] = 1024,
        derive_eks: Param[bool, Arg.Switch(&#39;-d&#39;, &#39;--dks&#39;, group=&#39;EKS&#39;,
            help=&#39;Act as .NET and derive the effective key size from the key length.&#39;)] = False,
        padding=None,
        mode=None,
        raw=False,
        little_endian=False,
        segment_size=0,
        tag=None,
        aad=None,
        **keywords
    ):
        super().__init__(
            key,
            iv=iv,
            eks=eks,
            derive_eks=derive_eks,
            padding=padding,
            mode=mode,
            raw=raw,
            little_endian=little_endian,
            segment_size=segment_size,
            tag=tag,
            aad=aad,
            **keywords
        )

    def _new_cipher(self, **optionals) -&gt; CipherInterface:
        eks = len(self.args.key) * 8 if self.args.derive_eks else self.args.eks
        optionals.update(effective_keylen=eks)
        return super()._new_cipher(**optionals)</code></pre>
</details>
</dd>
<dt id="refinery.shell.rc4"><code class="flex name class">
<span>class <span class="ident">rc4</span></span>
<span>(</span><span>key, discard=0)</span>
</code></dt>
<dd>
<section class="desc"><p>RC4 encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/rc4.py#L12-L23" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rc4(StandardCipherUnit, cipher=PyCryptoFactoryWrapper(ARC4)):
    &#34;&#34;&#34;
    RC4 encryption and decryption.
    &#34;&#34;&#34;
    def __init__(
        self, key,
        discard: Param[int, Arg.Number(&#39;-d&#39;, help=&#39;Discard the first {varname} bytes of the keystream, {default} by default.&#39;)] = 0,
    ):
        super().__init__(key, discard=discard)

    def _new_cipher(self, **optionals):
        return super()._new_cipher(drop=self.args.discard, **optionals)</code></pre>
</details>
</dd>
<dt id="refinery.shell.rc4mod"><code class="flex name class">
<span>class <span class="ident">rc4mod</span></span>
<span>(</span><span>key, stateful=False, discard=0, *, size=256)</span>
</code></dt>
<dd>
<section class="desc"><p>Implements a modified version of the RC4 stream cipher where the size of the RC4 SBox can be altered.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/rc4mod.py#L9-L37" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rc4mod(StreamCipherUnit):
    &#34;&#34;&#34;
    Implements a modified version of the RC4 stream cipher where the size of the RC4 SBox can be altered.
    &#34;&#34;&#34;

    def __init__(
        self, key, stateful=False, discard=0, *,
        size: Param[int, Arg.Number(&#39;-t&#39;, help=&#39;Table size, {default} by default.&#39;, bound=(1, None))] = 0x100
    ):
        super().__init__(key=key, stateful=stateful, discard=discard, size=size)

    def keystream(self):
        size = self.args.size
        tablerange = range(max(size, 0x100))
        b, table = 0, bytearray(k &amp; 0xFF for k in tablerange)
        for a, keybyte in zip(tablerange, cycle(self.args.key)):
            t = table[a]
            b = (b + keybyte + t) % size
            table[a] = table[b]
            table[b] = t
        self.log_debug(lambda: F&#39;SBOX = {table.hex(&#34; &#34;).upper()}&#39;, clip=True)
        b, a = 0, 0
        while True:
            a = (a + 1) % size
            t = table[a]
            b = (b + t) % size
            table[a] = table[b]
            table[b] = t
            yield table[(table[a] + t) % size]</code></pre>
</details>
</dd>
<dt id="refinery.shell.rc5"><code class="flex name class">
<span>class <span class="ident">rc5</span></span>
<span>(</span><span>key, *, iv=b'', padding=None, mode=None, raw=False, little_endian=False, segment_size=0, rounds=12, word_size=32, aad=b'', tag=())</span>
</code></dt>
<dd>
<section class="desc"><p>RC5 encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/rc5.py#L117-L149" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rc5(StandardBlockCipherUnit, cipher=BlockCipherFactory(RC5)):
    &#34;&#34;&#34;
    RC5 encryption and decryption.
    &#34;&#34;&#34;
    def __init__(
        self, key, *, iv=b&#39;&#39;, padding=None, mode=None, raw=False, little_endian=False, segment_size=0,
        rounds: Param[int, Arg.Number(&#39;-k&#39;, help=&#39;Number of rounds to use, the default is {default}&#39;)] = _R,
        word_size: Param[int, Arg.Number(&#39;-w&#39;, help=&#39;The word size in bits, {default} by default.&#39;)] = _W,
        **more
    ):
        super().__init__(
            key,
            iv=iv,
            padding=padding,
            mode=mode,
            raw=raw,
            little_endian=little_endian,
            segment_size=segment_size,
            rounds=rounds,
            word_size=word_size,
            **more
        )

    @property
    def block_size(self):
        return self.args.word_size // 4

    def _new_cipher(self, **optionals) -&gt; CipherInterface:
        return super()._new_cipher(
            rounds=self.args.rounds,
            word_size=self.args.word_size,
            **optionals
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.rc6"><code class="flex name class">
<span>class <span class="ident">rc6</span></span>
<span>(</span><span>key, *, iv=b'', padding=None, mode=None, raw=False, little_endian=False, segment_size=0, rounds=20, word_size=32)</span>
</code></dt>
<dd>
<section class="desc"><p>RC6 encryption and decryption. The parameter defaults are the RC6 parameters that were chosen
for the AES candidacy. Only key sizes of 128, 192, and 256 bits are used for AES candidates, but
the unit will allow any key size up to 256 bits.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/rc6.py#L117-L149" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rc6(StandardBlockCipherUnit, cipher=BlockCipherFactory(RC6)):
    &#34;&#34;&#34;
    RC6 encryption and decryption. The parameter defaults are the RC6 parameters that were chosen
    for the AES candidacy. Only key sizes of 128, 192, and 256 bits are used for AES candidates, but
    the unit will allow any key size up to 256 bits.
    &#34;&#34;&#34;
    def __init__(
        self, key, *, iv=b&#39;&#39;, padding=None, mode=None, raw=False, little_endian=False, segment_size=0,
        rounds: Param[int, Arg.Number(&#39;-k&#39;, help=&#39;Number of rounds to use, the default is {default}&#39;)] = _R,
        word_size: Param[int, Arg.Number(&#39;-w&#39;, help=&#39;The word size in bits, {default} by default.&#39;)] = _W,
    ):
        super().__init__(
            key,
            iv=iv,
            padding=padding,
            mode=mode,
            raw=raw,
            little_endian=little_endian,
            segment_size=segment_size,
            rounds=rounds,
            word_size=word_size
        )

    @property
    def block_size(self):
        return self.args.word_size // 2

    def _new_cipher(self, **optionals) -&gt; CipherInterface:
        return super()._new_cipher(
            rounds=self.args.rounds,
            word_size=self.args.word_size,
            **optionals
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.recode"><code class="flex name class">
<span>class <span class="ident">recode</span></span>
<span>(</span><span>decode=None, encode='UTF8', decerr=None, encerr=None, errors=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Expects input string data encoded in the <code>from</code> encoding and encodes it in
the <code>to</code> encoding, then outputs the result.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/recode.py#L19-L66" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class recode(Unit):
    &#34;&#34;&#34;
    Expects input string data encoded in the `from` encoding and encodes it in
    the `to` encoding, then outputs the result.
    &#34;&#34;&#34;

    def __init__(
        self,
        decode: Param[str, Arg.String(metavar=&#39;decode-as&#39;, help=&#39;Input encoding; Guess encoding by default.&#39;)] = None,
        encode: Param[str, Arg.String(metavar=&#39;encode-as&#39;, help=F&#39;Output encoding; The default is {Unit.codec}.&#39;)] = Unit.codec,
        decerr: Param[str, Arg.Option(&#39;-d&#39;, choices=Handler,
            help=&#39;Specify an error handler for decoding.&#39;)] = None,
        encerr: Param[str, Arg.Option(&#39;-e&#39;, choices=Handler,
            help=&#39;Specify an error handler for encoding.&#39;)] = None,
        errors: Param[str, Arg.Option(&#39;-E&#39;, choices=Handler, help=(
            &#39;Specify an error handler for both encoding and decoding. &#39;
            &#39;The possible choices are the following: {choices}&#39;))] = None,
    ):
        super().__init__(
            decode=decode,
            encode=encode,
            decerr=Arg.AsOption(decerr or errors or &#39;STRICT&#39;, Handler).value,
            encerr=Arg.AsOption(encerr or errors or &#39;STRICT&#39;, Handler).value
        )

    @Unit.Requires(&#39;chardet&#39;, [&#39;default&#39;, &#39;extended&#39;])
    def _chardet():
        import chardet
        return chardet

    def _detect(self, data):
        mv = memoryview(data)
        if not any(mv[1::2]): return &#39;utf-16le&#39;
        if not any(mv[0::2]): return &#39;utf-16be&#39;
        detection = self._chardet.detect(data)
        codec = detection[&#39;encoding&#39;]
        self.log_info(lambda: F&#39;Using input encoding: {codec}, detected with {int(detection[&#34;confidence&#34;] * 100)}% confidence.&#39;)
        return codec

    def _recode(self, enc, dec, encerr, decerr, data):
        dec = dec or self._detect(data)
        return codecs.encode(codecs.decode(data, dec, errors=decerr), enc, errors=encerr)

    def reverse(self, data):
        return self._recode(self.args.decode, self.args.encode, self.args.decerr, self.args.encerr, data)

    def process(self, data):
        return self._recode(self.args.encode, self.args.decode, self.args.encerr, self.args.decerr, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.reduce"><code class="flex name class">
<span>class <span class="ident">reduce</span></span>
<span>(</span><span>suffix, just=0, temp='t')</span>
</code></dt>
<dd>
<section class="desc"><p>The reduce unit applies an arbitrary multibin suffix repeatedly to reduce a complete frame to a
single chunk. The first chunk in the frame serves as initialization.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/reduce.py#L10-L46" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class reduce(Unit):
    &#34;&#34;&#34;
    The reduce unit applies an arbitrary multibin suffix repeatedly to reduce a complete frame to a
    single chunk. The first chunk in the frame serves as initialization.
    &#34;&#34;&#34;

    def __init__(self,
        suffix: Param[str, Arg.String(help=(
            &#39;The remaining command line is a multibin suffix. The reduction accumulator is initialized &#39;
            &#39;with the first chunk in the frame. Then, each remaining chunk is processed with the given &#39;
            &#39;suffix and the result is used to overwrite the accumulator.&#39;
        ))],
        just: Param[int, Arg.Number(&#39;-j&#39;,
            help=&#39;Optionally specify a maximum number of chunks to process beyond the first.&#39;)] = 0,
        temp: Param[str, Arg.String(&#39;-t&#39;, metavar=&#39;name&#39;,
            help=&#39;The name of the accumulator variable. The default is &#34;{default}&#34;.&#39;)] = &#39;t&#39;,
    ):
        super().__init__(suffix=suffix, temp=temp, just=just)

    def filter(self, chunks: Iterable[Chunk]):
        it = iter(chunks)
        just = self.args.just
        name = self.args.temp
        accu = next(it)
        if not just:
            scope = it
        else:
            import itertools
            self.log_info(F&#39;reducing only the next {just} chunks&#39;)
            scope = itertools.islice(it, 0, just)
        for chunk in scope:
            chunk.meta[name] = accu
            accu[:] = DelayedBinaryArgument(self.args.suffix, reverse=True, seed=chunk)(chunk)
            self.log_debug(&#39;reduced:&#39;, accu, clip=True)
        accu.meta.discard(name)
        yield accu
        yield from it</code></pre>
</details>
</dd>
<dt id="refinery.shell.rep"><code class="flex name class">
<span>class <span class="ident">rep</span></span>
<span>(</span><span>count=2, label='')</span>
</code></dt>
<dd>
<section class="desc"><p>Duplicates the given input a given number of times. It is also possible to specify
an iterable instead of a number, in which case the input will be replicated once for
each item in this iterable.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/strings/rep.py#L9-L49" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rep(Unit):
    &#34;&#34;&#34;
    Duplicates the given input a given number of times. It is also possible to specify
    an iterable instead of a number, in which case the input will be replicated once for
    each item in this iterable.
    &#34;&#34;&#34;

    def __init__(
        self,
        count: Param[isq, Arg.NumSeq(help=(
            &#39;Defines the number of outputs to generate for each input. The default is {default}. &#39;
            &#39;You can specify any multibin expression that defines an integer iterable here: Each &#39;
            &#39;input chunk will be replicated once for each element of that sequence.&#39;))] = 2,
        label: Param[str, Arg.String(help=(
            &#39;If specified, the meta variable with this name will be populated with the index of &#39;
            &#39;the replicated chunk. When the count parameter is an integer, this label will be &#39;
            &#39;equivalent to the index meta variable.&#39;))] = &#39;&#39;
    ):
        super().__init__(count=count, label=label)

    def process(self, data: bytearray):
        def count():
            count = self.args.count
            if isinstance(count, int):
                return count
            return sum(1 for _ in count)

        if self.args.squeeze or not self._framed:
            self.log_debug(&#39;compressing all repeated items into a single chunk&#39;)
            yield data * count()
            return

        self.log_debug(&#39;emitting each repeated item as an individual chunk&#39;)

        if label := self.args.label:
            meta = {}
            for counter in self.args.count:
                meta[label] = counter
                yield self.labelled(data, **meta)
        else:
            yield from repeat(data, count())</code></pre>
</details>
</dd>
<dt id="refinery.shell.repl"><code class="flex name class">
<span>class <span class="ident">repl</span></span>
<span>(</span><span>search, replace=b'', count=-1)</span>
</code></dt>
<dd>
<section class="desc"><p>Performs a simple binary string replacement on the input data.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/strings/repl.py#L7-L25" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class repl(Unit):
    &#34;&#34;&#34;
    Performs a simple binary string replacement on the input data.
    &#34;&#34;&#34;

    def __init__(
        self,
        search: Param[buf, Arg(help=&#39;This is the search term.&#39;)],
        replace: Param[buf, Arg(help=&#39;The substitution string. Leave this empty to remove all occurrences of the search term.&#39;)] = B&#39;&#39;,
        count: Param[int, Arg.Number(&#39;-n&#39;, help=&#39;Only replace the given number of occurrences&#39;)] = -1
    ):
        super().__init__(search=search, replace=replace, count=count)

    def process(self, data: bytearray):
        return data.replace(
            self.args.search,
            self.args.replace,
            self.args.count
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.resplit"><code class="flex name class">
<span>class <span class="ident">resplit</span></span>
<span>(</span><span>regex=b'\\r?\\n', multiline=False, ignorecase=False, count=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Splits the data at the given regular expression and returns the sequence of
chunks between the separators. By default, the input is split along line breaks.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/pattern/resplit.py#L6-L27" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class resplit(SingleRegexUnit):
    &#34;&#34;&#34;
    Splits the data at the given regular expression and returns the sequence of
    chunks between the separators. By default, the input is split along line breaks.
    &#34;&#34;&#34;

    def __init__(
        self, regex=RB&#39;\r?\n&#39;, multiline=False, ignorecase=False, count=0
    ):
        super().__init__(regex=regex, multiline=multiline, ignorecase=ignorecase, count=count)

    def process(self, data):
        view = memoryview(data)
        cursor = 0
        count = self.args.count
        for k, match in enumerate(self.regex.finditer(view), 2):
            yield view[cursor:match.start()]
            cursor = match.end()
            yield from match.groups()
            if k &gt; count &gt; 0:
                break
        yield view[cursor:]</code></pre>
</details>
</dd>
<dt id="refinery.shell.resub"><code class="flex name class">
<span>class <span class="ident">resub</span></span>
<span>(</span><span>regex='\\s+', subst=b'', multiline=False, ignorecase=False, count=0)</span>
</code></dt>
<dd>
<section class="desc"><p>A unit for performing substitutions based on a binary regular expression pattern. Besides the
syntax <code>{k}</code> to insert the <code>k</code>-th match group, the unit supports processing the contents of
match groups with arbitrary refinery units. To do so, use the following F-string-like syntax:</p>
<pre><code>{match-group:handlers}
</code></pre>
<p>where <code>:handlers</code> is an optional reverse multibin expression that is used to post-process the
binary data from the match. For example, <code>{2:hex:b64}</code> represents the base64-decoding of the
hex-decoding of the second match group.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/pattern/resub.py#L10-L50" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class resub(SingleRegexUnit):
    &#34;&#34;&#34;
    A unit for performing substitutions based on a binary regular expression pattern. Besides the
    syntax `{k}` to insert the `k`-th match group, the unit supports processing the contents of
    match groups with arbitrary refinery units. To do so, use the following F-string-like syntax:

        {match-group:handlers}

    where `:handlers` is an optional reverse multibin expression that is used to post-process the
    binary data from the match. For example, `{2:hex:b64}` represents the base64-decoding of the
    hex-decoding of the second match group.
    &#34;&#34;&#34;
    def __init__(
        self,
        regex: Param[str, Arg(
            help=&#39;Regular expression to be searched and replaced. The default is &#34;{default}&#34;.&#39;)
        ] = &#39;\\s+&#39;,
        subst: Param[buf, Arg(&#39;subst&#39;, help=(
            &#39;Substitution value: use {1} for group 1, {0} for entire match. Matches are removed &#39;
            &#39;(replaced by an empty string) by default.&#39;
        ))] = B&#39;&#39;,
        multiline=False,
        ignorecase=False,
        count=0
    ):
        super().__init__(regex=regex, subst=subst, multiline=multiline, ignorecase=ignorecase, count=count)

    def process(self, data):
        def repl(match: Match):
            r = meta.format_bin(spec, self.codec, [match[0], *match.groups()], match.groupdict())
            self.log_debug(&#39;substitution:&#39;, repr(r), clip=True)
            return r
        self.log_info(&#39;pattern:&#39;, getattr(self.regex, &#39;pattern&#39;, self.regex))
        self.log_info(&#39;replace:&#39;, self.args.subst)
        meta = metavars(data)
        spec = self.args.subst.decode(&#39;ascii&#39;, &#39;backslashreplace&#39;)
        substitute = self.regex.sub
        if self.args.count:
            from functools import partial
            substitute = partial(substitute, count=self.args.count)
        return substitute(repl, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.rev"><code class="flex name class">
<span>class <span class="ident">rev</span></span>
<span>(</span><span>blocksize=1)</span>
</code></dt>
<dd>
<section class="desc"><p>The blocks of the input data are output in reverse order. If the length of
the input data is not a multiple of the block size, the data is truncated.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/blockwise/rev.py#L11-L48" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rev(UnaryOperation):
    &#34;&#34;&#34;
    The blocks of the input data are output in reverse order. If the length of
    the input data is not a multiple of the block size, the data is truncated.
    &#34;&#34;&#34;

    def __init__(self, blocksize=1):
        super().__init__(blocksize=blocksize, _truncate=2)

    def inplace(self, block: ndarray):
        return self._numpy.flip(block)

    operate = NotImplemented

    def process(self, data: bytearray):
        if self.bytestream:
            data.reverse()
            return data
        try:
            return self._fastblock(data)
        except FastBlockError:
            b = self.blocksize
            n = len(data)
            q = n // b
            m = q * b
            view = memoryview(data)
            temp = bytearray(b)
            for k in range(0, (q // 2) * b, b):
                lhs = slice(k, k + b)
                rhs = slice(m - k - b, m - k)
                temp[:] = view[rhs]
                data[rhs] = view[lhs]
                data[lhs] = temp
            if m &lt; n:
                del view
                del temp
                del data[m:]
            return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.rex"><code class="flex name class">
<span>class <span class="ident">rex</span></span>
<span>(</span><span>regex, /, *transformation, unicode=False, unique=False, multiline=False, ignorecase=False, min=1, max=None, len=None, stripspace=False, longest=False, take=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Short for Regular Expression eXtractor: A binary grep which can apply a transformation to each
match. Each match is an individual output. Besides the syntax <code>{k}</code> to insert the <code>k</code>-th match
group, the unit supports processing the contents of match groups with arbitrary refinery units.
To do so, use the following F-string-like syntax:</p>
<pre><code>{match-group:pipeline}
</code></pre>
<p>where <code>:pipeline</code> is an optional pipeline of refinery commands as it would be specified on
the command line. The value of the corresponding match is post-processed with this command. The
unit also supports the special output format <code>{.}</code> which represents the input data.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/pattern/rex.py#L13-L95" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rex(SingleRegexUnit, PatternExtractor):
    &#34;&#34;&#34;
    Short for Regular Expression eXtractor: A binary grep which can apply a transformation to each
    match. Each match is an individual output. Besides the syntax `{k}` to insert the `k`-th match
    group, the unit supports processing the contents of match groups with arbitrary refinery units.
    To do so, use the following F-string-like syntax:

        {match-group:pipeline}

    where `:pipeline` is an optional pipeline of refinery commands as it would be specified on
    the command line. The value of the corresponding match is post-processed with this command. The
    unit also supports the special output format `{%s}` which represents the input data.
    &#34;&#34;&#34;
    def __init__(
        self, regex,
        /,
        *transformation: Param[str, Arg.String(help=(
            &#39;An optional sequence of transformations to be applied to each match. &#39;
            &#39;Each transformation produces one output in the order in which they &#39;
            &#39;are given. The default transformation is {0}, i.e. the entire match.&#39;
        ))],
        unicode: Param[bool, Arg.Switch(&#39;-u&#39;, help=&#39;Also find unicode strings.&#39;)] = False,
        unique: Param[bool, Arg.Switch(&#39;-q&#39;, help=&#39;Yield every (transformed) match only once.&#39;)] = False,
        multiline=False, ignorecase=False, min=1, max=None, len=None, stripspace=False,
        longest=False, take=None
    ):
        super().__init__(
            regex=regex,
            transformation=transformation,
            unicode=unicode,
            unique=unique,
            multiline=multiline,
            ignorecase=ignorecase,
            min=min,
            max=max,
            len=len,
            stripspace=stripspace,
            longest=longest,
            take=take,
            utf16=unicode,
            ascii=True,
            duplicates=not unique
        )

    def process(self, data):
        meta = metavars(data)
        wrap = ByteStringWrapper.Wrap(data)
        self.log_debug(&#39;regular expression:&#39;, getattr(self.regex, &#39;pattern&#39;, self.regex))
        transformations = []
        specs: list[str] = list(self.args.transformation)
        if not specs:
            specs.append(&#39;{0}&#39;)
        for spec in specs:
            if spec.startswith(&#39;{&#39;) and spec.endswith(&#39;}&#39;) and (group := spec[1:-1]).isdigit():
                transformations.append(int(group))
            else:
                def transformation(match: Match, s=spec):
                    symb: dict = {
                        key: (value or b&#39;&#39;) for key, value in match.groupdict().items()
                        if not key.startswith(&#39;__&#39;)}
                    args: list = [match.group(0), *match.groups()]
                    used = set()
                    for key, value in symb.items():
                        if value is None:
                            symb[key] = B&#39;&#39;
                    symb[_FORWARD_VAR] = wrap
                    item = meta.format(s, self.codec, args, symb, True, True, used)
                    used.update(key for key, value in symb.items() if not value)
                    used.add(_FORWARD_VAR)
                    for variable in used:
                        symb.pop(variable, None)
                    symb.update(offset=match.start())
                    chunk = Chunk(item)
                    chunk.meta.update(meta)
                    chunk.meta.update(symb)
                    return chunk
                transformations.append(transformation)
        yield from self.matches_filtered(
            memoryview(data),
            self.regex,
            *transformations,
            expose_named_groups=True
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.rijndael"><code class="flex name class">
<span>class <span class="ident">rijndael</span></span>
<span>(</span><span>key, iv=b'', block_size=16, *, aad=b'', tag=(), segment_size=0, little_endian=False, raw=False, mode=None, padding=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Rijndael encryption and decryption. Note that there is also a <code><a title="refinery.aes" href="index.html#refinery.aes">aes</a></code> unit which has
much better performance because it calls into the PyCryptodome library. You would have to
use this specific Rijndael unit only if Rijndael is used with a block size that is different
from 16 bytes, in which case it is equivalent to AES.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/rijndael.py#L601-L620" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rijndael(StandardBlockCipherUnit, cipher=BlockCipherFactory(Rijndael)):
    &#34;&#34;&#34;
    Rijndael encryption and decryption. Note that there is also a `refinery.aes` unit which has
    much better performance because it calls into the PyCryptodome library. You would have to
    use this specific Rijndael unit only if Rijndael is used with a block size that is different
    from 16 bytes, in which case it is equivalent to AES.
    &#34;&#34;&#34;
    def __init__(
        self, key, iv=b&#39;&#39;,
        block_size: Param[int, Arg.Number(&#39;-b&#39;, help=&#39;Cipher block size, default is {default}. Valid choices are 16, 24, and 32.&#39;)] = 16,
        **more
    ):
        return super().__init__(key, iv=iv, block_size=block_size, **more)

    @property
    def block_size(self):
        return self.args.block_size

    def _new_cipher(self, **optionals) -&gt; CipherInterface:
        return super()._new_cipher(block_size=self.args.block_size, **optionals)</code></pre>
</details>
</dd>
<dt id="refinery.shell.ripemd128"><code class="flex name class">
<span>class <span class="ident">ripemd128</span></span>
<span>(</span><span>reps=1, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the RIPEMD-128 hash of the input data.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/hash/cryptographic.py#L101-L107" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ripemd128(HashUnit):
    &#34;&#34;&#34;
    Returns the RIPEMD-128 hash of the input data.
    &#34;&#34;&#34;
    def _algorithm(self, data):
        from refinery.lib.ripemd128 import ripemd128
        return ripemd128(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.ripemd160"><code class="flex name class">
<span>class <span class="ident">ripemd160</span></span>
<span>(</span><span>reps=1, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the RIPEMD160 hash of the input data.</p></section>
</dd>
<dt id="refinery.shell.rmv"><code class="flex name class">
<span>class <span class="ident">rmv</span></span>
<span>(</span><span>*names)</span>
</code></dt>
<dd>
<section class="desc"><p>Short for "ReMove Variable": Removes meta variables that were created in the current frame. If no
variable names are given, the unit removes all of them. Note that this can recover variables from
outer frames that were previously shadowed.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/rmv.py#L8-L22" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rmv(Unit):
    &#34;&#34;&#34;
    Short for &#34;ReMove Variable&#34;: Removes meta variables that were created in the current frame. If no
    variable names are given, the unit removes all of them. Note that this can recover variables from
    outer frames that were previously shadowed.
    &#34;&#34;&#34;
    def __init__(self, *names: Param[str, Arg.String(metavar=&#39;name&#39;, help=&#39;Name of a variable to be removed.&#39;)]):
        super().__init__(names=names)

    def process(self, data: Chunk):
        meta = metavars(data)
        keys = self.args.names or list(meta.variable_names())
        for key in keys:
            meta.discard(key)
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.rncrypt"><code class="flex name class">
<span>class <span class="ident">rncrypt</span></span>
<span>(</span><span>password)</span>
</code></dt>
<dd>
<section class="desc"><p>Implements encryption and decryption using the RNCryptor specification.
See also: <a href="https://github.com/RNCryptor">https://github.com/RNCryptor</a></p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/rncrypt.py#L46-L94" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rncrypt(Unit):
    &#34;&#34;&#34;
    Implements encryption and decryption using the RNCryptor specification.
    See also: https://github.com/RNCryptor
    &#34;&#34;&#34;
    def __init__(self, password: bytearray):
        super().__init__(password=password)

    def process(self, data: bytearray) -&gt; bytes:
        encryption_salt = data[2:10]
        hmac_salt = data[10:18]
        iv = data[18:34]
        cipher_text = data[34:-32]
        hmac_signature = data[-32:]
        encryption_key = self._pbkdf2(self.args.password, encryption_salt)
        hmac_key = self._pbkdf2(self.args.password, hmac_salt)
        if not hmac.compare_digest(self._hmac(hmac_key, data[:-32]), hmac_signature):
            raise ValueError(&#34;Failed to verify signature.&#34;)
        return unpad(
            self._aes_decrypt(encryption_key, iv, cipher_text),
            block_size=AES.block_size
        )

    def reverse(self, data: bytes) -&gt; bytes:
        prng = Random.new()
        data = pad(data, block_size=AES.block_size)
        encryption_salt = prng.read(8)
        encryption_key = self._pbkdf2(self.args.password, encryption_salt)
        hmac_salt = prng.read(8)
        hmac_key = self._pbkdf2(self.args.password, hmac_salt)
        iv = prng.read(AES.block_size)
        cipher_text = self._aes_encrypt(encryption_key, iv, data)
        new_data = b&#39;\x03\x01&#39; + encryption_salt + hmac_salt + iv + cipher_text
        return new_data + self._hmac(hmac_key, new_data)

    def _aes_encrypt(self, key, iv, text):
        return AES.new(key, AES.MODE_CBC, iv).encrypt(text)

    def _aes_decrypt(self, key, iv, text):
        return AES.new(key, AES.MODE_CBC, iv).decrypt(text)

    def _hmac(self, key, data):
        return hmac.new(key, data, hashlib.sha256).digest()

    def _prf(self, secret, salt):
        return hmac.new(secret, salt, hashlib.sha1).digest()

    def _pbkdf2(self, password, salt, iterations=10000, key_length=32):
        return KDF.PBKDF2(password, salt, dkLen=key_length, count=iterations, prf=self._prf)</code></pre>
</details>
</dd>
<dt id="refinery.shell.rot"><code class="flex name class">
<span>class <span class="ident">rot</span></span>
<span>(</span><span>amount=13)</span>
</code></dt>
<dd>
<section class="desc"><p>Rotate the characters of the alphabet by the given amount. The default
amount is 13, providing the common (and weak) string obfuscation method.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/rot.py#L10-L27" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rot(Unit):
    &#34;&#34;&#34;
    Rotate the characters of the alphabet by the given amount. The default
    amount is 13, providing the common (and weak) string obfuscation method.
    &#34;&#34;&#34;

    def __init__(self, amount: Param[int, Arg.Number(help=&#39;Number of letters to rotate by; Default is 13.&#39;)] = 13):
        super().__init__(amount=amount)

    def process(self, data: bytearray):
        rot = self.args.amount % 26
        for index, byte in enumerate(data):
            for alphabet in _LCASE, _UCASE:
                if byte in alphabet:
                    zero = alphabet[0]
                    data[index] = zero + (byte - zero + rot) % 26
                    break
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.rotl"><code class="flex name class">
<span>class <span class="ident">rotl</span></span>
<span>(</span><span>*argument, bigendian=False, blocksize=1)</span>
</code></dt>
<dd>
<section class="desc"><p>Rotate the bits of each block left.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/blockwise/rotl.py#L6-L18" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rotl(BinaryOperation):
    &#34;&#34;&#34;
    Rotate the bits of each block left.
    &#34;&#34;&#34;
    def operate(self, value, shift):
        shift %= self.fbits
        return (value &lt;&lt; shift) | (value &gt;&gt; (self.fbits - shift))

    def inplace(self, value, shift):
        shift %= self.fbits
        lower = value &gt;&gt; (self.fbits - shift)
        value &lt;&lt;= shift
        value |= lower</code></pre>
</details>
</dd>
<dt id="refinery.shell.rotr"><code class="flex name class">
<span>class <span class="ident">rotr</span></span>
<span>(</span><span>*argument, bigendian=False, blocksize=1)</span>
</code></dt>
<dd>
<section class="desc"><p>Rotate the bits of each block right.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/blockwise/rotr.py#L6-L18" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rotr(BinaryOperation):
    &#34;&#34;&#34;
    Rotate the bits of each block right.
    &#34;&#34;&#34;
    def operate(self, value, shift):
        shift %= self.fbits
        return (value &gt;&gt; shift) | (value &lt;&lt; (self.fbits - shift))

    def inplace(self, value, shift):
        shift %= self.fbits
        lower = value &gt;&gt; shift
        value &lt;&lt;= self.fbits - shift
        value |= lower</code></pre>
</details>
</dd>
<dt id="refinery.shell.rsa"><code class="flex name class">
<span>class <span class="ident">rsa</span></span>
<span>(</span><span>key, swapkeys=False, textbook=False, padding=0, rsautl=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Implements single block RSA encryption and decryption. This unit can be used to encrypt
and decrypt blocks generated by openssl's <code>rsautl</code> tool when using the mode <code>-verify</code>.
When it is executed with a public key for decryption or with a private key for encryption,
it will perform a raw RSA operation. The result of these operations are (un)padded using
EMSA-PKCS1-v1_5.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/rsa.py#L83-L274" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rsa(Unit):
    &#34;&#34;&#34;
    Implements single block RSA encryption and decryption. This unit can be used to encrypt
    and decrypt blocks generated by openssl&#39;s `rsautl` tool when using the mode `-verify`.
    When it is executed with a public key for decryption or with a private key for encryption,
    it will perform a raw RSA operation. The result of these operations are (un)padded using
    EMSA-PKCS1-v1_5.
    &#34;&#34;&#34;
    def __init__(
        self,
        key: Param[buf, Arg(help=&#39;RSA key in PEM, DER, or Microsoft BLOB format.&#39;)],
        swapkeys: Param[bool, Arg.Switch(&#39;-s&#39;, help=&#39;Swap public and private exponent.&#39;)] = False,
        textbook: Param[bool, Arg.Switch(&#39;-t&#39;, group=&#39;PAD&#39;, help=&#39;Equivalent to --padding=NONE.&#39;)] = False,
        padding: Param[str, Arg.Option(&#39;-p&#39;, group=&#39;PAD&#39;, metavar=&#39;P&#39;, choices=PAD,
            help=&#39;Choose one of the following padding modes: {choices}. The default is AUTO.&#39;)] = PAD.AUTO,
        rsautl: Param[bool, Arg.Switch(&#39;-r&#39;, group=&#39;PAD&#39;,
            help=&#39;Act as rsautl from OpenSSH; This is equivalent to --swapkeys --padding=PKCS10&#39;)] = False,
    ):
        padding = Arg.AsOption(padding, PAD)
        if textbook:
            if padding != PAD.AUTO:
                raise ValueError(&#39;Conflicting padding options!&#39;)
            padding = padding.NONE
        if rsautl:
            if padding and padding != PAD.PKCS10:
                raise ValueError(&#39;Conflicting padding options!&#39;)
            swapkeys = True
            padding = PAD.PKCS10

        super().__init__(key=key, textbook=textbook, padding=padding, swapkeys=swapkeys)

        self._key_hash = None
        self._key_data = None

    @property
    def blocksize(self) -&gt; int:
        return self.key.size_in_bytes()

    @property
    def _blocksize_plain(self) -&gt; int:
        # PKCS#1 v1.5 padding is at least 11 bytes.
        return self.blocksize - 11

    @property
    def pub(self):
        return self.key.d if self.args.swapkeys else self.key.e

    @property
    def prv(self):
        return self.key.e if self.args.swapkeys else self.key.d

    def _get_msg(self, data):
        msg = int.from_bytes(data, byteorder=&#39;big&#39;)
        if msg &gt; self.key.n:
            raise ValueError(F&#39;This key can only handle messages of size {self.blocksize}.&#39;)
        return msg

    def _encrypt_raw(self, data):
        return pow(
            self._get_msg(data),
            self.pub,
            self.key.n
        ).to_bytes(self.blocksize, byteorder=&#39;big&#39;)

    def _decrypt_raw(self, data):
        return pow(
            self._get_msg(data),
            self.prv,
            self.key.n
        ).to_bytes(self.blocksize, byteorder=&#39;big&#39;)

    def _unpad(self, data, head, padbyte=None):
        if len(data) &gt; self.blocksize:
            raise ValueError(F&#39;This key can only handle messages of size {self.blocksize}.&#39;)
        if data.startswith(head):
            pos = data.find(B&#39;\0&#39;, 2)
            if pos &gt; 0:
                pad = data[2:pos]
                if padbyte is None or all(b == padbyte for b in pad):
                    return data[pos + 1:]
        raise ValueError(&#39;Incorrect padding&#39;)

    def _pad(self, data, head, padbyte=None):
        if len(data) &gt; self._blocksize_plain:
            raise ValueError(F&#39;This key can only encrypt messages of size at most {self._blocksize_plain}.&#39;)
        pad = self.blocksize - len(data) - len(head) - 1
        if padbyte is not None:
            padding = pad * bytes((padbyte,))
        else:
            padding = bytearray(1)
            while not all(padding):
                padding = bytearray(filter(None, padding))
                padding.extend(get_random_bytes(pad - len(padding)))
        return head + padding + B&#39;\0&#39; + data

    def _unpad_pkcs10(self, data):
        return self._unpad(data, B&#39;\x00\x01&#39;, 0xFF)

    def _unpad_pkcs15(self, data):
        return self._unpad(data, B&#39;\x00\x02&#39;, None)

    def _pad_pkcs10(self, data):
        return self._pad(data, B&#39;\x00\x01&#39;, 0xFF)

    def _pad_pkcs15(self, data):
        return self._pad(data, B&#39;\x00\x02&#39;, None)

    def _decrypt_block_OAEP(self, data):
        self.log_debug(&#39;Attempting decryption with PyCrypto PKCS1 OAEP.&#39;)
        return PKCS1_OAEP.new(self.key).decrypt(data)

    def _encrypt_block_OAEP(self, data):
        self.log_debug(&#39;Attempting encryption with PyCrypto PKCS1 OAEP.&#39;)
        return PKCS1_OAEP.new(self.key).encrypt(data)

    def _decrypt_block(self, data):
        if self._oaep and self._pads in {PAD.AUTO, PAD.OAEP}:
            try:
                return self._decrypt_block_OAEP(data)
            except ValueError as E:
                if self._pads:
                    raise
                self.log_debug(F&#39;{E!s} No longer attempting OAEP.&#39;)
                self._oaep = False

        data = self._decrypt_raw(data)
        return self._unpad_per_argument(data)

    def _unpad_per_argument(self, data):
        if self._pads == PAD.NONE:
            return data
        elif self._pads == PAD.PKCS10:
            return self._unpad_pkcs10(data)
        elif self._pads == PAD.PKCS15:
            return self._unpad_pkcs15(data)
        elif self._pads == PAD.AUTO:
            with suppress(ValueError):
                data = self._unpad_pkcs10(data)
                self.log_info(&#39;Detected PKCS1.0 padding.&#39;)
                self._pads = PAD.PKCS10
                return data
            with suppress(ValueError):
                data = self._unpad_pkcs15(data)
                self.log_info(&#39;Detected PKCS1.5 padding.&#39;)
                self._pads = PAD.PKCS15
                return data
            raise RefineryPartialResult(&#39;No padding worked, returning raw decrypted blocks.&#39;, data)
        else:
            raise ValueError(F&#39;Invalid padding value: {self._pads!r}&#39;)

    def _encrypt_block(self, data):
        if self._pads in {PAD.AUTO, PAD.OAEP}:
            try:
                return self._encrypt_block_OAEP(data)
            except ValueError:
                if self._pads: raise
                self.log_debug(&#39;PyCrypto primitives for OAEP failed, falling back to PKCS1.5.&#39;)
                self._pads = PAD.PKCS15

        if self._pads == PAD.PKCS15:
            data = self._pad_pkcs15(data)
        elif self._pads == PAD.PKCS10:
            data = self._pad_pkcs10(data)

        return self._encrypt_raw(data)

    @property
    def key(self) -&gt; RSA.RsaKey:
        key_blob = self.args.key
        key_hash = hash(key_blob)
        if key_hash != self._key_hash:
            fmt, key_data = normalize_rsa_key(key_blob)
            self.log_info(F&#39;successfully parsed RSA key as {fmt.value}&#39;)
            self._key_hash = key_hash
            self._key_data = key_data
        return self._key_data

    def process(self, data):
        self._oaep = True
        self._pads = self.args.padding
        if not self.key.has_private():
            try:
                return self._unpad_per_argument(self._encrypt_raw(data))
            except RefineryPartialResult:
                raise
            except Exception as E:
                raise ValueError(F&#39;A public key was given for decryption and rsautl mode resulted in an error: {E}&#39;) from E
        return B&#39;&#39;.join(self._decrypt_block(block) for block in splitchunks(data, self.blocksize))

    def reverse(self, data):
        self._pads = self.args.padding
        return B&#39;&#39;.join(self._encrypt_block(block) for block in splitchunks(data, self._blocksize_plain))</code></pre>
</details>
</dd>
<dt id="refinery.shell.rsakey"><code class="flex name class">
<span>class <span class="ident">rsakey</span></span>
<span>(</span><span>output=RSAFormat.PEM, public=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Parse RSA keys in various formats; PEM, DER, Microsoft BLOB, and W3C-XKMS (XML) format are supported.
The same formats are supported for the input format, but you can also specify a key in the following
format, where both modulus and exponent have to be hex-encoded: <code>[modulus]:[exponent]</code></p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/rsakey.py#L27-L118" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rsakey(Unit):
    &#34;&#34;&#34;
    Parse RSA keys in various formats; PEM, DER, Microsoft BLOB, and W3C-XKMS (XML) format are supported.
    The same formats are supported for the input format, but you can also specify a key in the following
    format, where both modulus and exponent have to be hex-encoded: `[modulus]:[exponent]`
    &#34;&#34;&#34;
    def __init__(
        self,
        output: Param[str, Arg.Option(choices=RSAFormat,
            help=&#39;Select an output format ({choices}), default is {default}.&#39;)] = RSAFormat.PEM,
        public: Param[bool, Arg.Switch(&#39;-p&#39;,
            help=&#39;Force public key output even if the input is private.&#39;)] = False,
    ):
        super().__init__(output=Arg.AsOption(output, RSAFormat), public=public)

    def _xkms_wrap(self, number: int):
        size, r = divmod(number.bit_length(), 8)
        size += int(bool(r))
        return base64.b64encode(number.to_bytes(size, &#39;big&#39;))

    def process(self, data):
        from refinery.lib.mscrypto import ALGORITHMS, TYPES
        fmt, key = normalize_rsa_key(data, force_public=self.args.public)
        self.log_info(F&#39;parsing input as {fmt.value} format&#39;)
        out = self.args.output
        if out is RSAFormat.PEM:
            yield key.export_key(&#39;PEM&#39;)
            return
        if out is RSAFormat.DER:
            yield key.export_key(&#39;DER&#39;)
            return
        if out is RSAFormat.BLOB:
            def le(v: int, s: int):
                return v.to_bytes(s, &#39;little&#39;)
            buffer = bytearray()
            buffer.append(TYPES.PRIVATEKEYBLOB if key.has_private() else TYPES.PUBLICKEYBLOB)
            buffer.extend(le(2, 3))
            buffer.extend(le(ALGORITHMS.CALG_RSA_KEYX, 4))
            buffer.extend(B&#39;RSA2&#39; if key.has_private() else B&#39;RSA1&#39;)
            size = 2
            while size &lt; key.n.bit_length():
                size &lt;&lt;= 1
            self.log_info(F&#39;using bit size {size}&#39;)
            buffer.extend(le(size, 4))
            size //= 8
            buffer.extend(le(key.e, 4))
            buffer.extend(le(key.n, size))
            if key.has_private():
                exp_1 = key.d % (key.p - 1)
                exp_2 = key.d % (key.q - 1)
                coeff = pow(key.q, -1, key.p)
                half = size // 2
                buffer.extend(le(key.p, half))
                buffer.extend(le(key.q, half))
                buffer.extend(le(exp_1, half))
                buffer.extend(le(exp_2, half))
                buffer.extend(le(coeff, half))
                buffer.extend(le(key.d, size))
            yield buffer
            return
        components = {
            &#39;Modulus&#39; : key.n,
            &#39;Exponent&#39;: key.e,
        }
        if key.has_private():
            decoded = DerSequence()
            decoded.decode(key.export_key(&#39;DER&#39;))
            it = itertools.islice(decoded, 3, None)
            for v in (&#39;D&#39;, &#39;P&#39;, &#39;Q&#39;, &#39;DP&#39;, &#39;DQ&#39;, &#39;InverseQ&#39;):
                try:
                    components[v] = next(it)
                except StopIteration:
                    break
        if out is RSAFormat.XKMS:
            for tag in components:
                components[tag] = base64.b64encode(number.long_to_bytes(components[tag])).decode(&#39;ascii&#39;)
            tags = &#39;\n&#39;.join(F&#39;\t&lt;{tag}&gt;{value}&lt;/{tag}&gt;&#39; for tag, value in components.items())
            yield F&#39;&lt;RSAKeyPair&gt;\n{tags}\n&lt;/RSAKeyPair&gt;&#39;.encode(self.codec)
            return
        components[&#39;BitSize&#39;] = key.n.bit_length()
        for tag, value in components.items():
            if value.bit_length() &gt; 32:
                components[tag] = F&#39;{value:X}&#39;
        if out is RSAFormat.JSON:
            yield json.dumps(components, indent=4).encode(self.codec)
            return
        if out is RSAFormat.TEXT:
            table = list(flattened(components))
            for key, value in table:
                value = F&#39;0x{value}&#39; if isinstance(value, str) else str(value)
                value = &#39;\n&#39;.join(F&#39;{L}&#39; for L in textwrap.wrap(value, 80))
                yield F&#39;-- {key + &#34; &#34;:-&lt;77}\n{value!s}&#39;.encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.rtfc"><code class="flex name class">
<span>class <span class="ident">rtfc</span></span>
</code></dt>
<dd>
<section class="desc"><p>Implements the RTF compression format. This compression algorithm is used, for example, to
compress RTF data in Outlook messages.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/office/rtfc.py#L6-L20" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rtfc(Unit):
    &#34;&#34;&#34;
    Implements the RTF compression format. This compression algorithm is used, for example, to
    compress RTF data in Outlook messages.
    &#34;&#34;&#34;
    @Unit.Requires(&#39;compressed_rtf&#39;, [&#39;formats&#39;, &#39;office&#39;, &#39;default&#39;, &#39;extended&#39;])
    def _rtfc():
        import compressed_rtf
        return compressed_rtf

    def process(self, data):
        return self._rtfc.decompress(data)

    def reverse(self, data):
        return self._rtfc.compress(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.run"><code class="flex name class">
<span>class <span class="ident">run</span></span>
<span>(</span><span>*commandline, stream=False, noinput=False, errors=False, timeout=0.0)</span>
</code></dt>
<dd>
<section class="desc"><p>Turns any command into a refinery unit. Data is processed by feeding it to the standard input
of a process spawned from the given command line, and then reading the standard output of that
process as the result of the operation. The main purpose of this unit is to allow using the
syntax from <code><a title="refinery.lib.frame" href="lib/frame.html">refinery.lib.frame</a></code> with other command line tools. By default, the unit streams
the output from the executed command as individual outputs, but the <code>buffer</code> option can be set
to buffer all output of a single execution. The format string expression <code>{}</code> or <code>{0}</code> can be
used as one of the arguments passed to the external command to represent the incoming data. In
this case, the data will not be sent to the standard input device of the new process.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/misc/run.py#L13-L176" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class run(Unit):
    &#34;&#34;&#34;
    Turns any command into a refinery unit. Data is processed by feeding it to the standard input
    of a process spawned from the given command line, and then reading the standard output of that
    process as the result of the operation. The main purpose of this unit is to allow using the
    syntax from `refinery.lib.frame` with other command line tools. By default, the unit streams
    the output from the executed command as individual outputs, but the `buffer` option can be set
    to buffer all output of a single execution. The format string expression `{}` or `{0}` can be
    used as one of the arguments passed to the external command to represent the incoming data. In
    this case, the data will not be sent to the standard input device of the new process.
    &#34;&#34;&#34;

    _JOIN_TIME = 0.1

    def __init__(
        self, *commandline: Param[str, Arg.String(nargs=&#39;...&#39;, metavar=&#39;(all remaining)&#39;, help=(
            &#39;All remaining command line tokens form an arbitrary command line to be executed. Use&#39;
            &#39; format string syntax to insert meta variables and incoming data chunks.&#39;))],
        stream: Param[bool, Arg.Switch(&#39;-s&#39;,
            help=&#39;Stream the command output rather than buffering it.&#39;)] = False,
        noinput: Param[bool, Arg.Switch(&#39;-x&#39;, help=&#39;Do not send any input to the new process.&#39;)] = False,
        errors: Param[bool, Arg.Switch(&#39;-m&#39;, help=(
            &#39;Merge stdout and stderr. By default, the standard error stream of the coupled command&#39;
            &#39; is forwarded to the logger, i.e. it is only visible if -v is also specified.&#39;
        ))] = False,
        timeout: Param[float, Arg.Double(&#39;-t&#39;, metavar=&#39;T&#39;, help=(
            &#39;Optionally set an execution timeout as a floating point number in seconds.&#39;
        ))] = 0.0
    ):
        if not commandline:
            raise ValueError(&#39;you need to provide a command line.&#39;)
        super().__init__(
            commandline=commandline, errors=errors, noinput=noinput, stream=stream, timeout=timeout)

    def process(self, data):
        def shlexjoin():
            import shlex
            return shlex.join(commandline)

        meta = metavars(data)
        meta.ghost = True
        used = set()
        commandline = [
            meta.format(cmd, self.codec, [data], None, False, used=used)
            for cmd in self.args.commandline
        ]

        if self.args.noinput:
            self.log_info(&#39;sending no input to process stdin&#39;)
            data = None

        if not self.log_debug(commandline):
            self.log_info(shlexjoin)

        posix = &#39;posix&#39; in sys.builtin_module_names
        process = Popen(commandline, shell=True,
            stdin=PIPE, stdout=PIPE, stderr=PIPE, close_fds=posix)

        if not self.args.stream and not self.args.timeout:
            out, err = process.communicate(data)
            for line in err.splitlines():
                self.log_info(line)
            yield out
            return

        from queue import Empty, Queue
        from threading import Event, Thread
        from time import process_time, sleep

        start = 0
        result = None

        qerr = Queue()
        qout = Queue()
        done = Event()

        def adapter(stream, queue: Queue, event: Event):
            while not event.is_set():
                out = stream.read1()
                if out: queue.put(out)
                else: break
            stream.close()

        recvout = Thread(target=adapter, args=(process.stdout, qout, done), daemon=True)
        recverr = Thread(target=adapter, args=(process.stderr, qerr, done), daemon=True)

        recvout.start()
        recverr.start()

        if data:
            process.stdin.write(data)
        process.stdin.close()
        start = process_time()

        if not self.args.stream or self.args.timeout:
            result = MemoryFile()

        def queue_read(q: Queue):
            try: return q.get_nowait()
            except Empty: return None

        errbuf = MemoryFile()

        while True:
            out = queue_read(qout)
            err = None

            if self.args.errors:
                out = out or queue_read(qerr)
            else:
                err = queue_read(qerr)

            if err and self.log_info():
                errbuf.write(err)
                errbuf.seek(0)
                lines = errbuf.readlines()
                errbuf.seek(0)
                errbuf.truncate()
                if lines:
                    if not (done.is_set() or lines[~0].endswith(B&#39;\n&#39;)):
                        errbuf.write(lines.pop())
                    for line in lines:
                        msg = line.rstrip(B&#39;\n&#39;)
                        if msg: self.log_info(msg)
            if out:
                if not self.args.stream or self.args.timeout:
                    result.write(out)
                if self.args.stream:
                    yield out

            if done.is_set():
                if recverr.is_alive():
                    self.log_warn(&#39;stderr receiver thread zombied&#39;)
                if recvout.is_alive():
                    self.log_warn(&#39;stdout receiver thread zombied&#39;)
                break
            elif not err and not out and process.poll() is not None:
                recverr.join(self._JOIN_TIME)
                recvout.join(self._JOIN_TIME)
                done.set()
            elif self.args.timeout:
                if process_time() - start &gt; self.args.timeout:
                    self.log_info(&#39;terminating process after timeout expired&#39;)
                    done.set()
                    process.terminate()
                    for wait in range(4):
                        if process.poll() is not None:
                            break
                        sleep(self._JOIN_TIME)
                    else:
                        self.log_warn(&#39;process termination may have failed&#39;)
                    recverr.join(self._JOIN_TIME)
                    recvout.join(self._JOIN_TIME)
                    if not len(result):
                        result = RuntimeError(&#39;timeout reached, process had no output&#39;)
                    else:
                        result = RefineryPartialResult(
                            &#39;timeout reached, returning all collected output&#39;,
                            partial=result.getvalue())

        if isinstance(result, Exception):
            raise result
        elif not self.args.stream:
            yield result.getvalue()</code></pre>
</details>
</dd>
<dt id="refinery.shell.salsa"><code class="flex name class">
<span>class <span class="ident">salsa</span></span>
<span>(</span><span>key, stateful=False, discard=0, nonce=b'REFINERY', magic=b'', offset=0, rounds=20)</span>
</code></dt>
<dd>
<section class="desc"><p>Salsa encryption and decryption. The nonce must be 8 bytes long. When 64 bytes are provided
as the key, this data is interpreted as the initial state box and all other parameters are
ignored.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/salsa.py#L157-L175" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class salsa(LatinCipherUnit):
    &#34;&#34;&#34;
    Salsa encryption and decryption. The nonce must be 8 bytes long. When 64 bytes are provided
    as the key, this data is interpreted as the initial state box and all other parameters are
    ignored.
    &#34;&#34;&#34;
    def keystream(self) -&gt; Iterable[int]:
        key = self.args.key
        if len(key) == 64:
            it = SalsaCipher.FromState(key)
        else:
            it = SalsaCipher(
                key,
                self.args.nonce,
                self.args.magic,
                self.args.rounds,
                self.args.offset,
            )
        yield from it</code></pre>
</details>
</dd>
<dt id="refinery.shell.salsa20"><code class="flex name class">
<span>class <span class="ident">salsa20</span></span>
<span>(</span><span>key, nonce=b'REFINERY')</span>
</code></dt>
<dd>
<section class="desc"><p>Salsa20 encryption and decryption. This unit is functionally equivalent to <code><a title="refinery.salsa" href="index.html#refinery.salsa">salsa</a></code>
with 20 rounds, but it uses the PyCryptodome library C implementation rather than the pure
Python implementation used by <code><a title="refinery.salsa" href="index.html#refinery.salsa">salsa</a></code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/salsa.py#L197-L202" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class salsa20(LatinCipherStandardUnit, cipher=PyCryptoFactoryWrapper(Salsa20)):
    &#34;&#34;&#34;
    Salsa20 encryption and decryption. This unit is functionally equivalent to `refinery.salsa`
    with 20 rounds, but it uses the PyCryptodome library C implementation rather than the pure
    Python implementation used by `refinery.salsa`.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="refinery.shell.scope"><code class="flex name class">
<span>class <span class="ident">scope</span></span>
<span>(</span><span>*slice, visible=True)</span>
</code></dt>
<dd>
<section class="desc"><p>After using <code><a title="refinery.scope" href="index.html#refinery.scope">scope</a></code> within in a <code><a title="refinery.lib.frame" href="lib/frame.html">refinery.lib.frame</a></code>, all the
following operations will be applied only to the selected indices. All
remaining chunks still exist, they are just not operated on. When the
frame closes or the frame is being rescoped by a second application of
this unit, they become visible again.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/scope.py#L14-L63" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class scope(FrameSlicer):
    &#34;&#34;&#34;
    After using `refinery.scope` within in a `refinery.lib.frame`, all the
    following operations will be applied only to the selected indices. All
    remaining chunks still exist, they are just not operated on. When the
    frame closes or the frame is being rescoped by a second application of
    this unit, they become visible again.
    &#34;&#34;&#34;
    def __init__(self, *slice, visible: Param[bool, Arg.Switch(&#39;-n&#39;, &#39;--not&#39;, off=True, help=(
        &#39;Hide the given chunks instead of making them the only ones visible.&#39;))] = True
    ):
        super().__init__(*slice, visible=visible)
        # Sort any slices with negative arguments to the back so we check
        # them last. This delays potential consumption of the chunks iterator
        # as much as possible.
        self.args.slice.sort(
            key=lambda s: (s.start or 0, s.stop or 0), reverse=True)

    def filter(self, chunks):
        it = iter(chunks)
        consumed = None
        size = None

        def buffered() -&gt; Generator[Chunk]:
            yield from it
            while consumed:
                yield consumed.popleft()

        def shift(offset, default):
            nonlocal consumed, size
            if offset is None:
                return default
            if offset &gt;= 0:
                return offset
            if consumed is None:
                from collections import deque
                self.log_info(F&#39;consuming iterator to compute negative offset {offset}.&#39;)
                consumed = deque(it)
                size = len(consumed) + k + 1
            return max(0, offset + size)

        for k, chunk in enumerate(buffered()):
            for s in self.args.slice:
                if k in range(shift(s.start, 0), shift(s.stop, k + 1), s.step or 1):
                    chunk.visible = self.args.visible
                    break
            else:
                chunk.visible = not self.args.visible
            self.log_debug(chunk)
            yield chunk</code></pre>
</details>
</dd>
<dt id="refinery.shell.seal"><code class="flex name class">
<span>class <span class="ident">seal</span></span>
<span>(</span><span>key, discard=0, stateful=False)</span>
</code></dt>
<dd>
<section class="desc"><p>SEAL encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/seal.py#L189-L196" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class seal(StreamCipherUnit):
    &#34;&#34;&#34;
    SEAL encryption and decryption.
    &#34;&#34;&#34;
    key_size = {20}

    def keystream(self) -&gt; Iterable[bytes]:
        return SEAL_Cipher(self.args.key)</code></pre>
</details>
</dd>
<dt id="refinery.shell.secstr"><code class="flex name class">
<span>class <span class="ident">secstr</span></span>
<span>(</span><span>key=b'\x01\x02\x03\x04\x05\x06\x07\x08\t\n\x0b\x0c\r\x0e\x0f\x10', iv=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Implements the AES-based encryption scheme used by the PowerShell commands
<code>ConvertFrom-SecureString</code> and <code>ConvertTo-SecureString</code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/secstr.py#L13-L85" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class secstr(Unit):
    &#34;&#34;&#34;
    Implements the AES-based encryption scheme used by the PowerShell commands
    `ConvertFrom-SecureString` and `ConvertTo-SecureString`.
    &#34;&#34;&#34;

    # This is a magic header value used for PowerShell secure strings.
    _MAGIC = bytes((
        0xEF, 0xAE, 0x3D, 0xD9, 0xDD, 0x75, 0xD7, 0xAE, 0xF8, 0xDD, 0xFD, 0x38,
        0xDB, 0x7E, 0x35, 0xDD, 0xBD, 0x7A, 0xD3, 0x9D, 0x1A, 0xE7, 0x7E, 0x39))

    # Secure strings include a decimal number formatted as a string directly
    # following the header. Presumably, this is the PowerShell version.
    _PSVER = 2

    def __init__(
        self, key: Param[buf, Arg(
            help=&#39;Secure string encryption 16-byte AES key; the default are the bytes from 1 to 16.&#39;
        )] = bytes(range(1, 17)),
        iv: Param[buf, Arg(&#39;-i&#39;, help=&#39;Optionally specify an IV to use for encryption.&#39;)] = B&#39;&#39;
    ):
        super().__init__(key=key, iv=iv)

    @property
    def key(self):
        key = self.args.key
        if len(key) not in (0x10, 0x18, 0x20):
            raise ValueError(&#39;The encryption key has to be 16 bytes long.&#39;)
        return key

    @property
    def iv(self):
        iv = self.args.iv
        if iv and len(iv) != 0x10:
            raise ValueError(&#39;The IV has to be 16 bytes long.&#39;)
        return iv

    def reverse(self, data):
        ivec = self.iv or urandom(0x10)
        if len(ivec) != 0x10:
            raise ValueError(self._IVERR)
        cipher = AES.new(self.key, AES.MODE_CBC, ivec)
        data = data.decode(&#39;latin-1&#39;).encode(&#39;utf-16LE&#39;)
        data = cipher.encrypt(pad(data, block_size=0x10))
        data = base64.b16encode(data).lower().decode(&#39;ascii&#39;)
        ivec = base64.b64encode(ivec).decode(&#39;ascii&#39;)
        data = &#39;|&#39;.join((&#39;%d&#39; % self._PSVER, ivec, data)).encode(&#39;utf-16LE&#39;)
        return base64.b64encode(self._MAGIC + data)

    def process(self, data):
        head, ivec, data = base64.b64decode(data).split(b&#39;|\0&#39;)
        self.log_info(&#39;head:&#39;, head.hex())
        ivec = base64.b64decode(ivec.decode(&#39;utf-16LE&#39;))
        self.log_info(&#39;ivec:&#39;, ivec.hex())
        data = base64.b16decode(data.decode(&#39;utf-16LE&#39;), casefold=True)
        if len(data) % 0x10 != 0:
            self.log_info(&#39;data not block-aligned, padding with zeros&#39;)
            data += B&#39;\0&#39; * (0x10 - len(data) % 0x10)
        cipher = AES.new(self.key, AES.MODE_CBC, ivec)
        data = cipher.decrypt(data)
        try:
            data = unpad(data, block_size=0x10)
        except Exception:
            self.log_warn(&#39;decrypted data does not have PKCS7 padding&#39;)
        for p in range(0x10):
            try:
                return data[-p:].decode(&#39;utf-16LE&#39;).encode(&#39;latin-1&#39;)
            except UnicodeDecodeError:
                pass
            except UnicodeEncodeError:
                pass
        self.log_warn(&#39;result is not a padded unicode string, key is likely wrong&#39;)
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.sep"><code class="flex name class">
<span>class <span class="ident">sep</span></span>
<span>(</span><span>separator=b'\n', scoped=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Multiple inputs are joined along a specified separator. If any of the input
<code><a title="refinery.lib.frame.Chunk" href="lib/frame.html#refinery.lib.frame.Chunk">Chunk</a></code>s is currently out of scope, <code><a title="refinery.sep" href="index.html#refinery.sep">sep</a></code> turns
makes them visible by default. This can be prevented by using the <code>-s</code> flag.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/sep.py#L7-L40" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class sep(Unit):
    &#34;&#34;&#34;
    Multiple inputs are joined along a specified separator. If any of the input
    `refinery.lib.frame.Chunk`s is currently out of scope, `refinery.sep` turns
    makes them visible by default. This can be prevented by using the `-s` flag.
    &#34;&#34;&#34;

    def __init__(
        self, separator: Param[buf, Arg(help=&#39;Separator; the default is a line break.&#39;)] = B&#39;\n&#39;,
        scoped: Param[bool, Arg.Switch(&#39;-s&#39;, help=(
            &#39;Maintain chunk scope; i.e. do not turn all input chunks visible.&#39;))] = False
    ):
        super().__init__(separator=separator, scoped=scoped)
        self.separate = False

    def filter(self, chunks):
        it = iter(chunks)
        try:
            chunk = next(it)
        except StopIteration:
            return
        self.separate = True
        for upcoming in it:
            if not self.args.scoped:
                chunk.visible = True
            yield chunk
            chunk = upcoming
        self.separate = False
        yield chunk

    def process(self, data):
        yield data
        if self.separate:
            yield self.args.separator</code></pre>
</details>
</dd>
<dt id="refinery.shell.serpent"><code class="flex name class">
<span>class <span class="ident">serpent</span></span>
<span>(</span><span>key, iv=b'', padding=None, mode=None, raw=False, swap=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Serpent encryption and decryption. Some Serpent implementations read the bytes of each block
in one direction, some in the other. When decryption results with this unit do not yield the
expected result, try using the <code>--swap</code> (or <code>-s</code>) option to swap the bytes in each block.
Furthermore, it is sometimes necessary to swap the bytes of the input key, which can be done
by prefixing the input key by the multibin handler <code>snip[::-1]</code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/serpent.py#L54-L71" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class serpent(StandardBlockCipherUnit, cipher=BlockCipherFactory(Serpent)):
    &#34;&#34;&#34;
    Serpent encryption and decryption. Some Serpent implementations read the bytes of each block
    in one direction, some in the other. When decryption results with this unit do not yield the
    expected result, try using the `--swap` (or `-s`) option to swap the bytes in each block.
    Furthermore, it is sometimes necessary to swap the bytes of the input key, which can be done
    by prefixing the input key by the multibin handler `snip[::-1]`.
    &#34;&#34;&#34;
    def __init__(
        self, key, iv=b&#39;&#39;, padding=None, mode=None, raw=False,
        swap: Param[bool, Arg.Switch(&#39;-s&#39;, help=&#39;Read the bytes in each block in reverse order.&#39;)] = False
    ):
        super().__init__(key, iv=iv, padding=padding, mode=mode, raw=raw, swap=swap)

    def _new_cipher(self, **optionals) -&gt; CipherInterface:
        instance: Serpent = super()._new_cipher()
        instance.swap = self.args.swap
        return instance</code></pre>
</details>
</dd>
<dt id="refinery.shell.shl"><code class="flex name class">
<span>class <span class="ident">shl</span></span>
<span>(</span><span>*argument, bigendian=False, blocksize=1)</span>
</code></dt>
<dd>
<section class="desc"><p>Shift the bits of each block left, filling with zero bits.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/blockwise/shl.py#L6-L13" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class shl(BinaryOperation):
    &#34;&#34;&#34;
    Shift the bits of each block left, filling with zero bits.
    &#34;&#34;&#34;
    @staticmethod
    def operate(a, b): return a &lt;&lt; b
    @staticmethod
    def inplace(a, b): a &lt;&lt;= b</code></pre>
</details>
</dd>
<dt id="refinery.shell.shr"><code class="flex name class">
<span>class <span class="ident">shr</span></span>
<span>(</span><span>*argument, bigendian=False, blocksize=1)</span>
</code></dt>
<dd>
<section class="desc"><p>Shift the bits of each block right, filling with zero bits.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/blockwise/shr.py#L6-L13" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class shr(BinaryOperation):
    &#34;&#34;&#34;
    Shift the bits of each block right, filling with zero bits.
    &#34;&#34;&#34;
    @staticmethod
    def operate(a, b): return a &gt;&gt; b
    @staticmethod
    def inplace(a, b): a &gt;&gt;= b</code></pre>
</details>
</dd>
<dt id="refinery.shell.sm4"><code class="flex name class">
<span>class <span class="ident">sm4</span></span>
<span>(</span><span>key, *, iv=b'', padding=None, mode=None, raw=False, little_endian=False, segment_size=0, tag=(), aad=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>The SM4 symmetric blockcipher algorithm published as GB/T 32907-2016 by the State Cryptography
Administration of China (SCA).</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/sm4.py#L107-L111" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class sm4(StandardBlockCipherUnit, cipher=BlockCipherFactory(SM4)):
    &#34;&#34;&#34;
    The SM4 symmetric blockcipher algorithm published as GB/T 32907-2016 by the State Cryptography
    Administration of China (SCA).
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="refinery.shell.snip"><code class="flex name class">
<span>class <span class="ident">snip</span></span>
<span>(</span><span>slices=[slice(None, None, None)], length=False, stream=False, remove=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Snips the input data based on a Python slice expression. For example, the
initialization <code>slice 0::1 1::1</code> would yield a unit that first extracts
every byte at an even position and then, every byte at an odd position. In
this case, multiple outputs are produced. The unit can be used in reverse
mode, in which case the specified ranges are deleted sequentially from the
input.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/strings/snip.py#L7-L67" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class snip(Unit):
    &#34;&#34;&#34;
    Snips the input data based on a Python slice expression. For example, the
    initialization `slice 0::1 1::1` would yield a unit that first extracts
    every byte at an even position and then, every byte at an odd position. In
    this case, multiple outputs are produced. The unit can be used in reverse
    mode, in which case the specified ranges are deleted sequentially from the
    input.
    &#34;&#34;&#34;
    def __init__(
        self,
        slices: Param[list[slice], Arg(help=&#39;Specify start:stop:step in Python slice syntax.&#39;)] = [slice(None, None)],
        length: Param[bool, Arg.Switch(&#39;-l&#39;, help=(
            &#39;Interpret the end of a slice as a length rather than as an offset.&#39;))] = False,
        stream: Param[bool, Arg.Switch(&#39;-s&#39;, help=(
            &#39;After each slice, consider only the data that follows after it for subsequent &#39;
            &#39;slicing. This mode is incompatible with negative step sizes.&#39;))] = False,
        remove: Param[bool, Arg.Switch(&#39;-r&#39;, help=(
            &#39;Remove the slices from the input rather than selecting them.&#39;))] = False,
    ):
        super().__init__(slices=slices, length=length, stream=stream, remove=remove)

    def process(self, data: bytearray):
        slices: list[slice] = list(self.args.slices)
        opt_stream = self.args.stream
        opt_remove = self.args.remove
        opt_length = self.args.length
        view = memoryview(data)
        offset = 0

        if opt_stream and any(b.step or 1 &lt; 0 for b in slices):
            raise RuntimeError(&#39;Streaming is incompatible with negative step slices.&#39;)

        for k, bounds in enumerate(slices):
            step = bounds.step or 1
            stop = bounds.stop
            start = bounds.start
            if opt_length:
                if stop is not None:
                    if start is None:
                        if step &lt; 0:
                            stop += len(data)
                    else:
                        stop += start
            if opt_stream:
                start = start or 0
                stop = stop or len(data)
                start += offset
                stop += offset

            bounds = slice(start, stop, bounds.step)

            if not opt_remove:
                temp = view[bounds]
            else:
                temp = bytearray(data)
                del temp[bounds]
            yield temp

            if opt_stream:
                offset = stop</code></pre>
</details>
</dd>
<dt id="refinery.shell.sorted"><code class="flex name class">
<span>class <span class="ident">sorted</span></span>
<span>(</span><span>key=None, ascending=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Sorts all elements of the input <code><a title="refinery.lib.frame" href="lib/frame.html">refinery.lib.frame</a></code> lexicographically.
This unit is a <code><a title="refinery.nop" href="index.html#refinery.nop">nop</a></code> on single inputs.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/sorted.py#L9-L51" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class sorted(Unit):
    &#34;&#34;&#34;
    Sorts all elements of the input `refinery.lib.frame` lexicographically.
    This unit is a `refinery.nop` on single inputs.
    &#34;&#34;&#34;

    def __init__(
        self,
        key: Param[str, Arg.String(&#39;key&#39;, help=&#39;A meta variable expression to sort by instead of sorting the content.&#39;)] = None,
        ascending: Param[bool, Arg.Switch(&#39;-a&#39;, help=&#39;Sort in ascending order, the default is descending.&#39;)] = False
    ):
        super().__init__(key=key, ascending=ascending)

    def filter(self, chunks):
        sortbuffer = []
        invisibles = []
        key = self.args.key
        rev = not self.args.ascending

        if key is not None:
            def _key(chunk):
                return expression(metavars(chunk)), chunk
            expression = PythonExpression(key, all_variables_allowed=True)
            key = _key

        def sorted():
            if not sortbuffer:
                return
            sortbuffer.sort(key=key, reverse=rev)
            yield from sortbuffer
            sortbuffer.clear()

        for chunk in chunks:
            if chunk.visible:
                yield from invisibles
                invisibles.clear()
                sortbuffer.append(chunk)
            else:
                yield from sorted()
                invisibles.append(chunk)

        yield from invisibles
        yield from sorted()</code></pre>
</details>
</dd>
<dt id="refinery.shell.sosemanuk"><code class="flex name class">
<span>class <span class="ident">sosemanuk</span></span>
<span>(</span><span>key, stateful=False, discard=0, nonce=b'')</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/sosemanuk.py#L2220-L2229" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class sosemanuk(StreamCipherUnit):

    def __init__(
        self, key, stateful=False, discard=0,
        nonce: Param[buf, Arg(help=&#39;The nonce. Default is empty, which is equivalent to 16 null bytes.&#39;)] = B&#39;&#39;,
    ):
        super().__init__(key=key, nonce=nonce, stateful=stateful, discard=discard)

    def keystream(self):
        yield from Sosemanuk(self.args.key, self.args.nonce)</code></pre>
</details>
</dd>
<dt id="refinery.shell.speck"><code class="flex name class">
<span>class <span class="ident">speck</span></span>
<span>(</span><span>key, iv=b'', padding=None, mode=None, raw=False, block_size=16, *, aad=b'', tag=(), segment_size=0, little_endian=False)</span>
</code></dt>
<dd>
<section class="desc"><p>SPECK encryption and decryption. It supports block sizes of 8 and 16 bytes.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/speck.py#L86-L102" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class speck(StandardBlockCipherUnit, cipher=BlockCipherFactory(Speck)):
    &#34;&#34;&#34;
    SPECK encryption and decryption. It supports block sizes of 8 and 16 bytes.
    &#34;&#34;&#34;
    def __init__(
        self, key, iv=b&#39;&#39;, padding=None, mode=None, raw=False,
        block_size: Param[int, Arg.Number(&#39;-b&#39;, help=&#39;Cipher block size, default is {default}. Valid choices are 8 and 16.&#39;)] = 16,
        **more
    ):
        return super().__init__(key, iv=iv, padding=padding, mode=mode, raw=raw, block_size=block_size, **more)

    @property
    def block_size(self):
        return self.args.block_size

    def _new_cipher(self, **optionals) -&gt; CipherInterface:
        return super()._new_cipher(block_size=self.args.block_size, **optionals)</code></pre>
</details>
</dd>
<dt id="refinery.shell.sqlite"><code class="flex name class">
<span>class <span class="ident">sqlite</span></span>
<span>(</span><span>query="SELECT * FROM sqlite_master WHERE type='table';")</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts data from SQLite3 databases. Each row is returned as a single output chunk in JSON
format. If no query is provided, the unit will extract all table metadata from the database.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/sqlite.py#L10-L46" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class sqlite(Unit):
    &#34;&#34;&#34;
    Extracts data from SQLite3 databases. Each row is returned as a single output chunk in JSON
    format. If no query is provided, the unit will extract all table metadata from the database.
    &#34;&#34;&#34;
    def __init__(
        self,
        query: Param[
            str, Arg.String(&#39;query&#39;, help=&#39;The SQL query to execute.&#39;)
        ] = &#34;SELECT * FROM sqlite_master WHERE type=&#39;table&#39;;&#34;,
    ):
        super().__init__(query=query)

    def process(self, data):
        try:
            with sqlite3.connect(&#39;:memory:&#39;) as database:
                try:
                    database.deserialize(data)
                except AttributeError:
                    raise NotImplementedError(F&#39;Python &gt;= 3.11 is required to use {self.name}.&#39;)
                cursor = database.cursor().execute(self.args.query)
                fields = (
                    [i[0] for i in cursor.description] if cursor.description else []
                )
                for row in cursor:
                    if fields:
                        cleaned_row = {}
                        for i in range(len(fields)):
                            if isinstance(row[i], bytes):
                                cleaned_row[i] = row[i].decode(self.codec)
                            else:
                                cleaned_row[fields[i]] = row[i]
                        yield json.dumps(cleaned_row).encode(self.codec)
                    else:
                        yield json.dumps(list(row)).encode(self.codec)
        except sqlite3.Error as e:
            raise ValueError(F&#39;Failed to process SQLite database: {e}&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.stego"><code class="flex name class">
<span>class <span class="ident">stego</span></span>
<span>(</span><span>split=False, parts='RGB')</span>
</code></dt>
<dd>
<section class="desc"><p>Decodes the RGBA (red/green/blue/alpha) values of the pixels of a given image file and outputs
these values as bytes. By default, the pixels are converted left to right, top to bottom. When
the input image is grayscale, the color channels are ignored. Colored images are converted to
RGBA mode.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/stego.py#L17-L76" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class stego(Unit):
    &#34;&#34;&#34;
    Decodes the RGBA (red/green/blue/alpha) values of the pixels of a given image file and outputs
    these values as bytes. By default, the pixels are converted left to right, top to bottom. When
    the input image is grayscale, the color channels are ignored. Colored images are converted to
    RGBA mode.
    &#34;&#34;&#34;
    def __init__(
        self,
        split: Param[bool, Arg.Switch(&#39;-m&#39;, help=&#39;Emit the individual rows or columns as separate outputs.&#39;)] = False,
        parts: Param[str, Arg.String(&#39;parts&#39;, nargs=&#39;?&#39;, help=(
            &#39;A string containing any ordering of the letters R, G, B, and A (case-insensitive). &#39;
            &#39;These pixel components will be extracted from every pixel in the given order. The &#39;
            &#39;default value is {default}.&#39;
        ))] = &#39;RGB&#39;
    ):
        super().__init__(
            split=split,
            parts=tuple(Arg.AsOption(p, PIXEL_PART) for p in parts)
        )

    @Unit.Requires(&#39;Pillow&#39;, [&#39;formats&#39;])
    def _image():
        from PIL import Image
        return Image

    def process(self, data):
        split = self.args.split
        parts = self.args.parts
        image = self._image.open(MemoryFile(data, output=bytes))

        grayscale = image.mode.startswith(&#39;L&#39;)
        bw_bitmap = image.mode.startswith(&#39;1&#39;)
        no_colors = grayscale or bw_bitmap

        if not no_colors:
            image = image.convert(&#39;RGBA&#39;)

        width, height = image.size
        chunk_size = 1 if no_colors else len(parts)
        output = MemoryFile()
        buffer = bytearray(chunk_size * width)
        pixels = iter(image.getdata())

        for _ in range(height):
            offset = 0
            for _ in range(width):
                pixel = next(pixels)
                next_offset = offset + chunk_size
                if no_colors:
                    buffer[offset] = pixel
                else:
                    buffer[offset:next_offset] = (pixel[p] for p in parts)
                offset = next_offset
            if split:
                yield buffer
            else:
                output.write(buffer)
        if not split:
            yield output.getvalue()</code></pre>
</details>
</dd>
<dt id="refinery.shell.stretch"><code class="flex name class">
<span>class <span class="ident">stretch</span></span>
<span>(</span><span>*count)</span>
</code></dt>
<dd>
<section class="desc"><p>Stretch the input data by repeating every byte a number of times.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/strings/stretch.py#L9-L40" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class stretch(Unit):
    &#34;&#34;&#34;
    Stretch the input data by repeating every byte a number of times.
    &#34;&#34;&#34;
    def __init__(self, *count: Param[int, Arg.Number(metavar=&#39;count&#39;, help=(
        &#39;The number of times every byte should be repeated. By default,  &#39;
        &#39;every byte is repeated once.&#39;
    ))]):
        count = count or (2,)
        if any(k &lt;= 0 for k in count):
            raise ValueError(&#39;You can not use a stretching factor of less than 1.&#39;)
        super().__init__(count=count or (2,))

    def process(self, data):
        def stretched(it):
            factor = cycle(self.args.count)
            for byte in it:
                yield from repeat(byte, next(factor))
        return bytearray(stretched(iter(data)))

    def reverse(self, data):
        # one-sided inverse
        def clinched(it):
            factor = cycle(self.args.count)
            while True:
                try:
                    take = islice(it, next(factor))
                    yield next(take)
                    for _ in take: pass
                except StopIteration:
                    break
        return bytearray(clinched(iter(data)))</code></pre>
</details>
</dd>
<dt id="refinery.shell.struct"><code class="flex name class">
<span>class <span class="ident">struct</span></span>
<span>(</span><span>spec, *outputs, multi=False, count=∞, until=None, format=None, name=None, more=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Read structured data from the beginning of a chunk and store the extracted fields in chunk meta
variables. The structure format is specified in extended Python struct format, and all
remaining arguments to this unit are the names of the variables that receive the values from
this struct. The extended struct format supports all field types supported by Python, as well
as the following:</p>
<ul>
<li><code>a</code> for null-terminated ASCII strings,</li>
<li><code>u</code> to read encoded, null-terminated UTF16 strings,</li>
<li><code>w</code> to read decoded, null-terminated UTF16 strings,</li>
<li><code>g</code> to read Microsoft GUID values,</li>
<li><code>E</code> to read 7-bit encoded integers.</li>
</ul>
<p>For example, the string <code>LLxxHaa</code> will read two unsigned 32bit integers, then skip two bytes,
then read one unsigned 16bit integer, then two null-terminated ASCII strings. The unit defaults
to using native byte order with no alignment. The <code>spec</code> parameter may additionally contain
format expressions of the following form:</p>
<pre><code>{name[!alignment]:format}
</code></pre>
<p>The <code>alignment</code> parameter is optional. It must be an expression that evaluates to an integer
value. The current data pointer is aligned to a multiple of this value before reading the field.
The <code>format</code> can either be an integer expression specifying a number of bytes to read, or any
format string. If <code>name</code> is specified for an extracted field, its value is made available as a
meta variable under the given name. For example, the expression <code>LLxxH{foo:a}{bar:a}</code> would be
parsed in the same way as the previous example, but the two ASCII strings would also be stored
in meta variables under the names <code>foo</code> and <code>bar</code>, respectively. The <code>format</code> string of a named
field is itself parsed as a foramt string expression, where all the previously parsed fields
are already available. For example, <code>I{:{}}</code> reads a single 32-bit integer length prefix and
then reads as many bytes as that prefix specifies.</p>
<p>A second format string expression is used to specify the output format. For example, the format
string <code>LLxxH{foo:a}{bar:a}</code> together with the output format <code>{foo}/{bar}</code> would parse data as
before, but the output body would be the concatnation of the field <code>foo</code>, a forward slash, and
the field <code>bar</code>. Variables used in the output expression are not included as meta variables. As
format fields in the output expression, one can also use <code>{1}</code>, <code>{2}</code> or <code>{-1}</code> to access
extracted fields by index. The value <code>{0}</code> represents the entire chunk of structured data. By
default, the output format <code>{#}</code> is used, which represents either the last byte string field
that was extracted, or the entire chunk of structured data if none of the fields were extracted.</p>
<p>Reverse <code><a title="refinery.lib.argformats.multibin" href="lib/argformats.html#refinery.lib.argformats.multibin">multibin()</a></code> expressions can be used to post-process the fields
included in any output format. For example, <code>{F:b64:zl}</code> will be the base64-decoded and inflate-
decompressed contents of the data that was read as field <code>F</code>.</p>
<p>Finally, it is possible to specify a byte alignment by using the syntax <code>{field!T:a:b:c}</code> where
the letter <code>T</code> is either a single digit specifying the alignment, or a single letter variable
that holds the byte alignment value in the current metadata. It is also possible to specify the
alignment as <code>0</code> which instructs the parser to only peek the contents of this field, i.e. the
read pointer is not advanced after reading it.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/pattern/struct_parser.py#L21-L264" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class struct(Unit):
    &#34;&#34;&#34;
    Read structured data from the beginning of a chunk and store the extracted fields in chunk meta
    variables. The structure format is specified in extended Python struct format, and all
    remaining arguments to this unit are the names of the variables that receive the values from
    this struct. The extended struct format supports all field types supported by Python, as well
    as the following:

    - `a` for null-terminated ASCII strings,
    - `u` to read encoded, null-terminated UTF16 strings,
    - `w` to read decoded, null-terminated UTF16 strings,
    - `g` to read Microsoft GUID values,
    - `E` to read 7-bit encoded integers.

    For example, the string `LLxxHaa` will read two unsigned 32bit integers, then skip two bytes,
    then read one unsigned 16bit integer, then two null-terminated ASCII strings. The unit defaults
    to using native byte order with no alignment. The `spec` parameter may additionally contain
    format expressions of the following form:

        {name[!alignment]:format}

    The `alignment` parameter is optional. It must be an expression that evaluates to an integer
    value. The current data pointer is aligned to a multiple of this value before reading the field.
    The `format` can either be an integer expression specifying a number of bytes to read, or any
    format string. If `name` is specified for an extracted field, its value is made available as a
    meta variable under the given name. For example, the expression `LLxxH{foo:a}{bar:a}` would be
    parsed in the same way as the previous example, but the two ASCII strings would also be stored
    in meta variables under the names `foo` and `bar`, respectively. The `format` string of a named
    field is itself parsed as a foramt string expression, where all the previously parsed fields
    are already available. For example, `I{:{}}` reads a single 32-bit integer length prefix and
    then reads as many bytes as that prefix specifies.

    A second format string expression is used to specify the output format. For example, the format
    string `LLxxH{foo:a}{bar:a}` together with the output format `{foo}/{bar}` would parse data as
    before, but the output body would be the concatnation of the field `foo`, a forward slash, and
    the field `bar`. Variables used in the output expression are not included as meta variables. As
    format fields in the output expression, one can also use `{1}`, `{2}` or `{-1}` to access
    extracted fields by index. The value `{0}` represents the entire chunk of structured data. By
    default, the output format `{%s}` is used, which represents either the last byte string field
    that was extracted, or the entire chunk of structured data if none of the fields were extracted.

    Reverse `refinery.lib.argformats.multibin` expressions can be used to post-process the fields
    included in any output format. For example, `{F:b64:zl}` will be the base64-decoded and inflate-
    decompressed contents of the data that was read as field `F`.

    Finally, it is possible to specify a byte alignment by using the syntax `{field!T:a:b:c}` where
    the letter `T` is either a single digit specifying the alignment, or a single letter variable
    that holds the byte alignment value in the current metadata. It is also possible to specify the
    alignment as `0` which instructs the parser to only peek the contents of this field, i.e. the
    read pointer is not advanced after reading it.
    &#34;&#34;&#34;

    def __init__(
        self,
        spec: Param[str, Arg.String(help=&#39;Structure format as explained above.&#39;)],
        *outputs: Param[str, Arg.String(metavar=&#39;output&#39;, help=&#39;Output format as explained above.&#39;)],
        multi: Param[bool, Arg.Switch(&#39;-m&#39;, help=(
            &#39;Read as many pieces of structured data as possible intead of just one.&#39;))] = False,
        count: Param[int, Arg.Number(&#39;-c&#39;, help=(
            &#39;A limit on the number of chunks to read in multi mode; default is {default}.&#39;))] = INF,
        until: Param[str, Arg.String(&#39;-u&#39;, metavar=&#39;E&#39;, help=(
            &#39;An expression evaluated on each chunk in multi mode. New chunks will be parsed &#39;
            &#39;only if the result is nonzero.&#39;))] = None,
        format: Param[str, Arg.String(&#39;-f&#39;, metavar=&#39;F&#39;, help=(
            &#39;Optionally specify a format string expression to auto-name extracted fields without a &#39;
            &#39;given name. The format string accepts the field {{c}} for the type code and {{n}} for &#39;
            &#39;the variable index.&#39;))] = None,
        name: Param[str, Arg.String(&#39;-n&#39;, metavar=&#39;VAR&#39;, group=&#39;FIELDS&#39;, help=(
            &#39;Equivalent to --format=VAR{{n}}.&#39;))] = None,
        more: Param[bool, Arg.Switch(&#39;-M&#39;, help=(
            &#39;After parsing the struct, emit one chunk that contains the data that was left &#39;
            &#39;over in the buffer. If no data was left over, this chunk will be empty.&#39;))] = False
    ):
        if name:
            format = format or F&#39;{name}{{n}}&#39;
        outputs = outputs or [F&#39;{{{_REST_MARKER}}}&#39;]
        super().__init__(spec=spec, outputs=outputs, until=until, format=format, count=count, multi=multi, more=more)

    def process(self, data: Chunk):
        formatter = string.Formatter()
        field_format: str | None = self.args.format
        until = self.args.until
        until = until and PythonExpression(until, all_variables_allowed=True)
        reader = StructReader(memoryview(data))
        checkpoint = 0
        mainspec = self.args.spec
        byteorder = mainspec[:1]
        if byteorder in &#39;&lt;@=!&gt;&#39;:
            mainspec = mainspec[1:]
        else:
            byteorder = &#39;=&#39;

        def fixorder(spec):
            if spec[0] not in &#39;&lt;@=!&gt;&#39;:
                spec = byteorder + spec
            return spec

        previously_existing_variables = set(metavars(data).variable_names())

        it = itertools.count() if self.args.multi else (0,)
        for index in it:

            field_counter = 0
            checkpoint = reader.tell()

            if reader.eof:
                break
            if index &gt;= self.args.count:
                break

            meta = metavars(data)
            meta.ghost = True
            meta.index = index

            args = []
            last = None
            self.log_debug(F&#39;starting new read at: 0x{checkpoint:08X}&#39;)

            try:
                for prefix, name, spec, conversion in formatter.parse(mainspec):
                    name: str
                    spec: str = spec and spec.strip()
                    if prefix:
                        fields = reader.read_struct(fixorder(prefix))
                        if field_format is not None:
                            codes = re.findall(&#39;[?cbBhHiIlLqQnNefdspPauwgk]&#39;, prefix)
                            if len(codes) != len(fields):
                                codes = &#39;v&#39; * len(fields)
                            for code, field in zip(codes, fields):
                                code = &#39;b&#39; if code == &#39;?&#39; else code.lower()
                                v = field_format.format_map({&#39;c&#39;: code, &#39;n&#39;: field_counter})
                                meta[v] = field
                                field_counter += 1
                        args.extend(fields)

                    if name is None:
                        continue

                    field_counter += 1

                    if name and not name.isdecimal():
                        check_variable_name(name)

                    if not conversion:
                        peek = False
                    else:
                        alignment = PythonExpression.Evaluate(conversion, meta)
                        if alignment == 0:
                            peek = True
                        else:
                            _aa = reader.tell()
                            reader.byte_align(alignment)
                            _ab = reader.tell()
                            if _aa != _ab:
                                self.log_info(F&#39;aligned from 0x{_aa:X} to 0x{_ab:X}&#39;)

                    spec, _, pipeline = spec.partition(&#39;:&#39;)

                    if spec:
                        spec = meta.format_str(spec, self.codec, args)

                    if spec:
                        try:
                            _exp = PythonExpression.Evaluate(spec, meta)
                        except ParserError:
                            pass
                        else:
                            spec = _exp

                    if spec == &#39;&#39;:
                        last = value = reader.read(peek=peek)
                    elif isinstance(spec, int):
                        if spec &lt; 0:
                            spec += reader.remaining_bytes
                        if spec &lt; 0:
                            raise ValueError(F&#39;The specified negative read offset is {-spec} beyond the cursor.&#39;)
                        last = value = reader.read_bytes(spec, peek=peek)
                    else:
                        value = reader.read_struct(fixorder(spec), peek=peek)
                        if not value:
                            self.log_debug(F&#39;field {name} was empty, ignoring.&#39;)
                            continue
                        if len(value) &gt; 1:
                            self.log_info(F&#39;parsing field {name} produced {len(value)} items reading a tuple&#39;)
                        else:
                            value = value[0]

                    if pipeline:
                        value = numseq(pipeline, reverse=True, seed=value)
                    args.append(value)

                    if name == _REST_MARKER:
                        raise ValueError(F&#39;Extracting a field with name {_REST_MARKER} is forbidden.&#39;)
                    elif name.isdecimal():
                        index = int(name)
                        limit = len(args) - 1
                        if index &gt; limit:
                            self.log_warn(F&#39;cannot assign index field {name}, the highest index is {limit}&#39;)
                        else:
                            args[index] = value
                        continue
                    elif name:
                        meta[name] = value

                if until and until(meta):
                    self.log_info(F&#39;the expression ({until}) evaluated to true; aborting.&#39;)
                    break

                with StreamDetour(reader, checkpoint) as detour:
                    full = reader.read(detour.cursor - checkpoint)
                if last is None:
                    last = full

                outputs = []
                symbols = dict(meta)
                symbols[_REST_MARKER] = last

                for template in self.args.outputs:
                    used = set()
                    outputs.append(meta.format(template, self.codec, [full, *args], symbols, True, used=used))
                    for key in used:
                        if key in previously_existing_variables:
                            continue
                        meta.discard(key)

                for output in outputs:
                    chunk = Chunk(output)
                    chunk.meta.update(meta)
                    chunk.set_next_batch(index)
                    yield chunk

            except EOFError:
                break

        leftover = len(reader) - checkpoint

        if not leftover:
            return
        elif self.args.more:
            reader.seekset(checkpoint)
            yield reader.read()
        else:
            leftover = repr(SizeInt(leftover)).strip()
            self.log_info(F&#39;discarding {leftover} left in buffer&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.sub"><code class="flex name class">
<span>class <span class="ident">sub</span></span>
<span>(</span><span>*argument, bigendian=False, blocksize=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Subtract the given argument from each block.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/blockwise/sub.py#L6-L13" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class sub(BinaryOperationWithAutoBlockAdjustment):
    &#34;&#34;&#34;
    Subtract the given argument from each block.
    &#34;&#34;&#34;
    @staticmethod
    def operate(a, b): return a - b
    @staticmethod
    def inplace(a, b): a -= b</code></pre>
</details>
</dd>
<dt id="refinery.shell.subfiles"><code class="flex name class">
<span>class <span class="ident">subfiles</span></span>
<span>(</span><span>memdump=False, recursive=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Deploys carvers for ZIP, 7-Zip, PE-File, Windows Shortcuts (LNK files), JSON and XML documents against
the input data and generates one output chunk for each successfully carved subfile.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/pattern/subfiles.py#L14-L59" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class subfiles(Unit):
    &#34;&#34;&#34;
    Deploys carvers for ZIP, 7-Zip, PE-File, Windows Shortcuts (LNK files), JSON and XML documents against
    the input data and generates one output chunk for each successfully carved subfile.
    &#34;&#34;&#34;

    _MINLENGTH = {
        &#39;json&#39;: 300,
        &#39;xml&#39; : 300,
        &#39;rtf&#39; : 100,
    }

    def __init__(
        self,
        memdump: Param[bool, Arg.Switch(&#39;-m&#39;,
            help=&#39;Assume that the input is a memdump for PE file carving.&#39;)] = False,
        recursive: Param[bool, Arg.Switch(&#39;-r&#39;,
            help=&#39;Extract files that are subfiles of other extracted files as separate chunks.&#39;)] = False,
    ):
        super().__init__(memdump=memdump, recursive=recursive)

    def process(self, data: bytearray):
        carvers = {
            &#39;zip&#39;  : carve_zip(),
            &#39;7z&#39;   : carve_7z(),
            &#39;pe&#39;   : carve_pe(memdump=self.args.memdump, fileinfo=True, recursive=True, keep_root=True),
            &#39;lnk&#39;  : carve_lnk(),
            &#39;json&#39; : carve_json(dictonly=True),
            &#39;xml&#39;  : carve_xml(),
            &#39;rtf&#39;  : carve_rtf(),
        }

        covered = []

        for extension, unit in carvers.items():
            self.log_info(F&#39;carving {extension} files&#39;)
            for chunk in data | unit:
                if len(chunk) &lt; self._MINLENGTH.get(extension, 1):
                    continue
                start = chunk[&#39;offset&#39;]
                end = start + len(chunk)
                if any(start &gt; left and end &lt; right for left, right in covered):
                    continue
                if not self.args.recursive:
                    covered.append((start, end))
                yield chunk</code></pre>
</details>
</dd>
<dt id="refinery.shell.swap"><code class="flex name class">
<span>class <span class="ident">swap</span></span>
<span>(</span><span>src, dst=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Swap the contents of an existing variable with the contents of the chunk or with another meta variable.
When swapping with the chunk, the variable has to contain a binary string. When swapping with a variable
that does not exist, the original variable is cleared, essentially renaming the variable.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/swap.py#L12-L59" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class swap(Unit):
    &#34;&#34;&#34;
    Swap the contents of an existing variable with the contents of the chunk or with another meta variable.
    When swapping with the chunk, the variable has to contain a binary string. When swapping with a variable
    that does not exist, the original variable is cleared, essentially renaming the variable.
    &#34;&#34;&#34;
    def __init__(
        self,
        src: Param[str, Arg.String(help=&#39;The meta variable name.&#39;)],
        dst: Param[str, Arg.String(help=&#39;Optional name of the second meta variable.&#39;)] = None
    ):
        super().__init__(
            src=check_variable_name(src),
            dst=check_variable_name(dst)
        )

    def filter(self, chunks: Iterable[Chunk]):
        src = self.args.src
        dst = self.args.dst
        for chunk in chunks:
            if not chunk.visible:
                pass
            elif dst is None:
                try:
                    value = chunk.meta[src]
                except KeyError:
                    value = bytearray()
                if isinstance(value, str):
                    value = value.encode(self.codec)
                elif not isbuffer(value):
                    raise ValueError(F&#39;Unable to swap data with variable {src} because it has type {type(value).__name__}.&#39;)
                if not chunk:
                    chunk.meta.discard(src)
                else:
                    chunk.meta[src] = bytes(chunk)
                chunk[:] = value
            else:
                try:
                    value = chunk.meta.pop(src)
                except KeyError:
                    raise KeyError(F&#39;The variable {src} does not exist.&#39;)
                try:
                    swap = chunk.meta.pop(dst)
                except KeyError:
                    chunk.meta[dst] = value
                else:
                    chunk.meta[src], chunk.meta[dst] = swap, value
            yield chunk</code></pre>
</details>
</dd>
<dt id="refinery.shell.szdd"><code class="flex name class">
<span>class <span class="ident">szdd</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from SZDD archives.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/szdd.py#L7-L56" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class szdd(Unit):
    &#34;&#34;&#34;
    Extract files from SZDD archives.
    &#34;&#34;&#34;
    def process(self, data):
        with StructReader(data) as archive:
            if archive.read(8) != b&#39;SZDD\x88\xF0\x27\x33&#39;:
                if not self.args.lenient:
                    raise ValueError(&#39;signature missing&#39;)
                self.log_warn(&#39;the header signature is invalid, this is likely not an SZDD archive&#39;)
            if archive.read_byte() != 0x41:
                raise ValueError(&#39;Unsupported compression mode&#39;)
            # ignore the missing file extension letter:
            archive.seekrel(1)
            output_len = archive.u32()
            window_pos = 0x1000 - 0x10
            output_pos = 0
            output = bytearray(output_len)
            window = bytearray(0x1000)
            for k in range(len(window)):
                window[k] = 0x20
            while not archive.eof:
                control = archive.read_byte()
                for cb in (0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80):
                    if archive.eof:
                        break
                    if control &amp; cb:
                        output[output_pos] = window[window_pos] = archive.read_byte()
                        output_pos += 1
                        window_pos += 1
                        window_pos &amp;= 0xFFF
                    else:
                        match_pos = archive.read_byte()
                        match_len = archive.read_byte()
                        match_pos |= (match_len &amp; 0xF0) &lt;&lt; 4
                        match_len = (match_len &amp; 0x0F) + 3
                        match_pos &amp;= 0xFFF
                        for _ in range(match_len):
                            window[window_pos] = window[match_pos]
                            output[output_pos] = window[window_pos]
                            output_pos += 1
                            window_pos += 1
                            match_pos += 1
                            window_pos &amp;= 0xFFF
                            match_pos &amp;= 0xFFF
            return output

    @classmethod
    def handles(cls, data):
        return data[:4] == B&#39;SZDD&#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.tea"><code class="flex name class">
<span>class <span class="ident">tea</span></span>
<span>(</span><span>key, iv=b'', padding=None, mode=None, raw=False, swap=False, rounds=32)</span>
</code></dt>
<dd>
<section class="desc"><p>TEA encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/tea.py#L116-L119" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class tea(TEAUnit, cipher=BlockCipherFactory(TEA)):
    &#34;&#34;&#34;
    TEA encryption and decryption.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="refinery.shell.termfit"><code class="flex name class">
<span>class <span class="ident">termfit</span></span>
<span>(</span><span>width=0, delta=0, tight=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Reformat incoming text data to fit a certain width.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/strings/termfit.py#L9-L25" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class termfit(Unit):
    &#34;&#34;&#34;
    Reformat incoming text data to fit a certain width.
    &#34;&#34;&#34;

    def __init__(
        self,
        width: Param[int, Arg(&#39;width&#39;, help=&#39;Optionally specify the width, by default the current terminal width is used.&#39;)] = 0,
        delta: Param[int, Arg.Number(&#39;-d&#39;, help=&#39;Subtract this number from the calculated width (0 by default).&#39;)] = 0,
        tight: Param[bool, Arg.Switch(&#39;-t&#39;, help=&#39;Separate paragraphs by a single line break instead of two.&#39;)] = False,
    ):
        super().__init__(width=width, delta=delta, tight=tight)

    @unicoded
    def process(self, data: str) -&gt; str:
        parsep = &#39;\n&#39; if self.args.tight else &#39;\n\n&#39;
        return terminalfit(data, self.args.delta, self.args.width, parsep)</code></pre>
</details>
</dd>
<dt id="refinery.shell.terminate"><code class="flex name class">
<span>class <span class="ident">terminate</span></span>
<span>(</span><span>sentinel=b'\x00', blocksize=1, bigendian=False)</span>
</code></dt>
<dd>
<section class="desc"><p>The unit reads data from the incoming chunk in blocks of any given size until the
sentinel value is encountered. The output of the unit is all data that was read,
excluding the sentinel. The default block size is one and the default sentinel value
is zero, which corresponds to reading a null-terminated string from the input.
If the sentinel value is not found anywhere in the incoming data, the complete input
is returned as output.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/blockwise/terminate.py#L7-L58" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class terminate(BlockTransformationBase):
    &#34;&#34;&#34;
    The unit reads data from the incoming chunk in blocks of any given size until the
    sentinel value is encountered. The output of the unit is all data that was read,
    excluding the sentinel. The default block size is one and the default sentinel value
    is zero, which corresponds to reading a null-terminated string from the input.
    If the sentinel value is not found anywhere in the incoming data, the complete input
    is returned as output.
    &#34;&#34;&#34;
    def __init__(
        self,
        sentinel: Param[buf, Arg(help=&#39;sentinel value to look for; default is {default}&#39;)] = B&#39;\0&#39;,
        blocksize=1, bigendian=False
    ):
        super().__init__(blocksize=blocksize, bigendian=bigendian, sentinel=sentinel)

    def process(self, data: bytearray):
        sentinel = self.args.sentinel
        position = 0
        blocksize = self.blocksize

        self.log_info(&#39;blocksize:&#39;, blocksize)
        self.log_debug(&#39;separator:&#39;, sentinel)

        while position &gt;= 0:
            position = data.find(sentinel, position)
            if position &lt; 0:
                self.log_info(F&#39;The sentinel value {sentinel} was not found.&#39;)
                break
            q, r = divmod(position, blocksize)
            if r:
                position = (q + 1) * blocksize
                continue
            else:
                data[position:] = []
                break

        return data

    def reverse(self, data: bytearray):
        sentinel = self.args.sentinel
        position = 0
        while True:
            position = data.find(sentinel, position)
            if position &lt; 0:
                data.extend(sentinel)
                break
            if position % self.blocksize == 0:
                self.log_warn(&#39;input string already contains the termination character; returning unmodified input&#39;)
                break
            position += 1
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.transpose"><code class="flex name class">
<span>class <span class="ident">transpose</span></span>
<span>(</span><span>padding=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Interprets the chunks in the current frame as rows of a matrix and yields the columns
of that matrix. When chunks are not of even length, the matrix is considered to have
empty entries in some positions. Optionally, a padding sequence can be provided to pad
all rows to the same length.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/transpose.py#L9-L68" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class transpose(Unit):
    &#34;&#34;&#34;
    Interprets the chunks in the current frame as rows of a matrix and yields the columns
    of that matrix. When chunks are not of even length, the matrix is considered to have
    empty entries in some positions. Optionally, a padding sequence can be provided to pad
    all rows to the same length.
    &#34;&#34;&#34;
    @Unit.Requires(&#39;numpy&#39;, [&#39;speed&#39;, &#39;default&#39;, &#39;extended&#39;])
    def _numpy():
        import numpy
        return numpy

    def __init__(
        self,
        padding: Param[buf, Arg(help=&#39;Optional byte sequence to use as padding for incomplete rows.&#39;)] = B&#39;&#39;,
    ):
        super().__init__(bigendian=False, padding=padding)

    def filter(self, chunks: Iterable[Chunk]):
        rows = []
        for chunk in chunks:
            if not chunk.visible:
                yield chunk
                continue
            rows.append(chunk)
        if not rows:
            return
        matrix = rows[0]
        matrix.temp = rows
        yield matrix

    def process(self, data: Chunk):
        chunks: list[Chunk] = data.temp
        if not chunks:
            return
        length = [len(chunk) for chunk in chunks]
        n = min(length)
        m = max(length)
        pad = self.args.padding
        if pad:
            for chunk in chunks:
                while len(chunk) &lt; m:
                    chunk.extend(pad)
                del chunk[m:]
        if n &gt; 0:
            try:
                np = self._numpy
            except ImportError:
                pass
            else:
                t = [chunk[n:] for chunk in chunks if len(chunk) &gt; n]
                for chunk in chunks:
                    del chunk[n:]
                a = np.array(chunks, dtype=np.uint8).transpose()
                for row in a:
                    yield row.tobytes(&#39;C&#39;)
                m = m - n
                chunks = t
        for i in range(m):
            yield bytes(chunk[i] for chunk in chunks if len(chunk) &gt; i)</code></pre>
</details>
</dd>
<dt id="refinery.shell.trim"><code class="flex name class">
<span>class <span class="ident">trim</span></span>
<span>(</span><span>*junk, unpad=False, left=False, right=False, nocase=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Removes byte sequences at beginning and end of input data.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/strings/trim.py#L7-L88" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class trim(Unit):
    &#34;&#34;&#34;
    Removes byte sequences at beginning and end of input data.
    &#34;&#34;&#34;

    def __init__(
        self,
        *junk : Param[buf, Arg(help=&#39;Binary strings to be removed, default are all whitespace characters.&#39;)],
        unpad : Param[bool, Arg.Switch(&#39;-u&#39;, help=&#39;Also trim partial occurrences of the junk string.&#39;)] = False,
        left  : Param[bool, Arg.Switch(&#39;-l&#39;, group=&#39;SIDE&#39;, help=&#39;Trim only left.&#39;)] = False,
        right : Param[bool, Arg.Switch(&#39;-r&#39;, group=&#39;SIDE&#39;, help=&#39;Trim only right.&#39;)] = False,
        nocase: Param[bool, Arg.Switch(&#39;-i&#39;, help=&#39;Ignore capitalization for alphabetic characters.&#39;)] = False,
    ):
        if not left and not right:
            left = right = True
        super().__init__(junk=junk, left=left, right=right, unpad=unpad, nocase=nocase)

    def _trimfast(self, view: memoryview, *junks: bytes, right=False) -&gt; tuple[bool, memoryview]:
        done = False
        pos = 0
        while not done:
            done = True
            for junk in junks:
                temp = junk
                size = len(junk)
                if right and self.args.unpad:
                    for k in range(size):
                        n = size - k
                        if view[pos:pos + n] == junk[k:]:
                            pos += n
                            done = False
                            break
                if view[pos:pos + size] == temp:
                    m = len(temp)
                    while True:
                        mm = m &lt;&lt; 1
                        if view[pos + m:pos + mm] != temp:
                            break
                        temp += temp
                        m = mm
                    temp = memoryview(temp)
                    while m &gt;= size:
                        if view[pos:pos + m] == temp[:m]:
                            done = False
                            pos += m
                        m //= 2
                if right or not self.args.unpad:
                    continue
                while size &gt; 0:
                    if view[pos:pos + size] == temp[:size]:
                        done = False
                        pos += size
                        break
                    size -= 1
        return pos

    def process(self, data: bytearray):
        junk = list(self.args.junk)
        if not junk:
            import string
            space = string.whitespace.encode(&#39;ascii&#39;)
            junk = [space[k - 1:k] for k in range(1, len(space))]
        lpos = 0
        rpos = 0
        if self.args.nocase:
            work = data.lower()
            junk = [j.lower() for j in junk]
        else:
            work = data
        if self.args.left:
            lpos = self._trimfast(memoryview(work), *junk)
        if self.args.right:
            work.reverse()
            junk = [bytes(reversed(j)) for j in junk]
            rpos = self._trimfast(memoryview(work), *junk, right=True)
            work.reverse()
        view = memoryview(data)
        if lpos:
            view = view[+lpos:]
        if rpos:
            view = view[:-rpos]
        return view</code></pre>
</details>
</dd>
<dt id="refinery.shell.u16"><code class="flex name class">
<span>class <span class="ident">u16</span></span>
</code></dt>
<dd>
<section class="desc"><p>Encodes and decodes UTF-16 encoded string data.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/u16.py#L7-L21" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class u16(Unit):
    &#34;&#34;&#34;
    Encodes and decodes UTF-16 encoded string data.
    &#34;&#34;&#34;

    def reverse(self, data: bytearray):
        return data.decode(self.codec).encode(&#39;utf-16LE&#39;)

    def process(self, data: bytearray):
        return data.decode(&#39;utf-16&#39;).encode(self.codec)

    @classmethod
    def handles(cls, data):
        if encoding := guess_text_encoding(data):
            return encoding.step == 2</code></pre>
</details>
</dd>
<dt id="refinery.shell.ucrypt"><code class="flex name class">
<span>class <span class="ident">ucrypt</span></span>
<span>(</span><span>size=13, salt=b'AA')</span>
</code></dt>
<dd>
<section class="desc"><p>Implements the classic Unix crypt algorithm.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/keyderive/unixcrypt.py#L334-L352" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ucrypt(KeyDerivation):
    &#34;&#34;&#34;
    Implements the classic Unix crypt algorithm.
    &#34;&#34;&#34;
    def __init__(
        self,
        size: Param[int, Arg(help=&#39;The number of bytes to generate, default is 13.&#39;)] = 13,
        salt: Param[buf, Arg(help=&#39;Salt for the derivation, the default is &#34;AA&#34;.&#39;)] = B&#39;AA&#39;
    ):
        super().__init__(size=size, salt=salt)

    def process(self, data):
        crypted = bytes(UnixCrypt(data, salt=self.args.salt))
        if len(crypted) &lt; self.args.size:
            raise RefineryPartialResult(
                F&#39;unix crypt only provided {len(crypted)} bytes, but {self.args.size} &#39;
                F&#39;were requested.&#39;, partial=crypted
            )
        return crypted[:self.args.size]</code></pre>
</details>
</dd>
<dt id="refinery.shell.url"><code class="flex name class">
<span>class <span class="ident">url</span></span>
<span>(</span><span>plus=False, hex=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Decodes and encodes URL-encoding, which preserves only alphanumeric characters and the
following symbols: <code>_</code>, <code>.</code>, <code>-</code>, <code>~</code>, <code>\</code>, <code>/</code>. Every other character is escaped by
hex-encoding it and prefixing it with a percent symbol.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/url.py#L11-L52" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class url(Unit):
    &#34;&#34;&#34;
    Decodes and encodes URL-encoding, which preserves only alphanumeric characters and the
    following symbols: `_`, `.`, `-`, `~`, `\\`, `/`. Every other character is escaped by
    hex-encoding it and prefixing it with a percent symbol.
    &#34;&#34;&#34;

    def __init__(
        self,
        plus: Param[bool, Arg.Switch(&#39;-p&#39;, help=&#39;also replace plus signs by spaces&#39;)] = False,
        hex: Param[bool, Arg.Switch(&#39;-x&#39;, help=&#39;hex encode every character in reverse mode&#39;)] = False
    ):
        super().__init__(plus=plus, hex=hex)

    def process(self, data):
        if self.args.plus:
            data = data.replace(B&#39;+&#39;, B&#39; &#39;)
        data = unquote_to_bytes(bytes(data))
        data = re.sub(
            B&#39;%[uU]([0-9a-fA-F]{4})&#39;,
            lambda m: int(m[1], 16).to_bytes(2, &#39;little&#39;),
            data)
        return data

    def reverse(self, data):
        if self.args.hex:
            result = bytearray(len(data) * 3)
            offset = 0
            for byte in data:
                result[offset + 0] = 0x25
                offset += 1
                result[offset:offset + 2] = B&#39;%02X&#39; % byte
                offset += 2
            return result
        elif self.args.plus:
            def replace(m):
                c = m[0][0]
                return b&#39;+&#39; if c == 0x20 else B&#39;%%%02X&#39; % c
        else:
            def replace(m):
                return B&#39;%%%02X&#39; % m[0][0]
        return re.sub(B&#39;[^a-zA-Z0-9_.-~\\/]&#39;, replace, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.urlfix"><code class="flex name class">
<span>class <span class="ident">urlfix</span></span>
<span>(</span><span>meta=False, keep=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Removes fragments, query strings, and parameters from input URLs. It also correctly escapes all
characters in the URL path component and normalizes the network location part to lowercase. Note
that URLs without a scheme will not be recognized as valid URLs; chunks that do not look like a
URL will be swallowed and not return any output.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/misc/urlfix.py#L9-L52" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class urlfix(Unit):
    &#34;&#34;&#34;
    Removes fragments, query strings, and parameters from input URLs. It also correctly escapes all
    characters in the URL path component and normalizes the network location part to lowercase. Note
    that URLs without a scheme will not be recognized as valid URLs; chunks that do not look like a
    URL will be swallowed and not return any output.
    &#34;&#34;&#34;
    def __init__(
        self,
        meta: Param[bool, Arg.Switch(&#39;-m&#39;, help=&#39;Extract the query string parameters as metadata.&#39;)] = False,
        keep: Param[int, Arg.Counts(&#39;-k&#39;, help=(
            &#39;If specified once, keeps the it keeps the URL params and query string. If specified &#39;
            &#39;twice, it keeps the URL fragment as well. At this level, the unit still filters out &#39;
            &#39;anything that does not parse as a URL.&#39;
        ))] = 0
    ):
        super().__init__(keep=keep, meta=meta)

    def process(self, data):
        def fix(string):
            return quote(unquote(string))
        keep = self.args.keep
        meta = self.args.meta
        parsed = urlparse(data.decode(self.codec))
        if not parsed.scheme or not parsed.netloc:
            return None
        query_dict = {key: unquote(value) for key, value in parse_qsl(parsed.query)}
        query_string = &#39;&amp;&#39;.join(F&#39;{key}={quote(value)}&#39; for key, value in query_dict.items())
        replacements = dict(
            netloc=parsed.netloc.lower(),
            params=fix(parsed.params),
            path=fix(parsed.path),
            query=query_string,
            fragment=fix(parsed.fragment),
        )
        if keep &lt; 2:
            replacements.update(fragment=&#39;&#39;)
            if keep &lt; 1:
                replacements.update(params=&#39;&#39;, query=&#39;&#39;)
        url = urlunparse(parsed._replace(**replacements))
        url = url.encode(self.codec)
        if meta:
            url = self.labelled(url, **query_dict)
        return url</code></pre>
</details>
</dd>
<dt id="refinery.shell.urlguards"><code class="flex name class">
<span>class <span class="ident">urlguards</span></span>
</code></dt>
<dd>
<section class="desc"><p>Restores the original URLs from their 'protected' versions as generated by
Outlook protection and ProofPoint.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/pattern/urlguards.py#L23-L119" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class urlguards(Unit):
    &#34;&#34;&#34;
    Restores the original URLs from their &#39;protected&#39; versions as generated by
    Outlook protection and ProofPoint.
    &#34;&#34;&#34;

    _PP3RLENC = {
        letter: rl for rl, letter in enumerate(
            &#39;ABCDEFGHIJKLMNOPQRSTUVWXYZ&#39;
            &#39;abcdefghijklmnopqrstuvwxyz&#39;
            &#39;0123456789-_&#39;, 2
        )
    }

    @unguard(r&#39;https?://urldefense(?:\.proofpoint)?\.com/v([12])/url\?([:;/_=!?#&amp;.,\w\%\-\+|]+)&#39;)
    def _proofpointV2(self, match):
        version = int(match[1])
        self.log_info(&#39;proofpoint match:&#39;, version)
        argmatch = re.match(
            R&#39;^u=(.+?)&amp;(?:amp;)?{}=&#39;.format(&#39;k&#39; if version == 1 else &#39;[dc]&#39;),
            match[2],
            flags=re.DOTALL
        )
        if not argmatch:
            self.log_warn(&#39;not able to translate unexpected proofpoint format:&#39;, match)
            return match[0]
        encoded = argmatch[1]
        if match[1] == &#39;2&#39;:
            encoded = encoded.translate(str.maketrans(&#39;-_&#39;, &#39;%/&#39;))
        return unescape(unquote(encoded))

    @unguard(r&#39;https?://urldefense(?:\.proofpoint)?\.com/v3/__(.+?)__;(.*?)![-\w!?$]+&#39;)
    def _proofpointV3(self, match):
        data = unquote(match[1])
        cmap = match[2] + &#39;=&#39; * (-len(match[2]) % 4)
        cmap = urlsafe_b64decode(cmap).decode(&#39;UTF-8&#39;)
        cursor = 0
        result = &#39;&#39;
        for k in range(len(cmap)):
            ast = data.find(&#39;*&#39;, cursor)
            if ast &lt; 0:
                break
            result += data[cursor:ast]
            if data[ast + 1] == &#39;*&#39;:
                end = self._PP3RLENC[data[ast + 2]]
                result += cmap[k:end]
                ast += 2
            else:
                result += cmap[k]
            cursor = ast + 1
        self.log_debug(result)
        self.log_debug(data[cursor:])
        return result + data[cursor:]

    @unguard(r&#39;https?://\w+.safelinks\.protection\.outlook\.com/([:;/_=!?#&amp;.,\w\%\-\+|]+)&#39;)
    def _outlook(self, match):
        result = match[0]
        self.log_info(&#39;outlook match:&#39;, result)
        parsed = urlparse(result)
        params = parse_qs(parsed.query)
        try:
            result = unquote(params[&#39;url&#39;][0])
        except Exception:
            pass
        return result

    @unguard(r&#39;https?://outlook.office.com/actions/ei\?u=([:;/_=!?#&amp;.,\w\%\-\+|]+)&#39;)
    def _outlook_image_proxy(self, match):
        return unquote(match[1])

    @unguard(r&#39;https?://(?:[\w-]+\.)?trendmicro.com(?::\d+)?/wis/clicktime/v[12]/(?:query|clickthrough)[:;/_=!?#&amp;.,\w\%\-\+|]+&#39;)
    def _trendmicro(self, match):
        result = match[0]
        self.log_info(&#39;trendmicro match:&#39;, result)
        parsed = urlparse(result)
        params = parse_qs(parsed.query)
        try:
            result = unquote(params[&#39;url&#39;][0])
        except Exception:
            pass
        return result

    @unicoded
    def process(self, data: str) -&gt; str:
        newsize, size = 0, len(data)
        while newsize != size:
            for handler in (
                self._proofpointV2,
                self._proofpointV3,
                self._outlook,
                self._outlook_image_proxy,
                self._trendmicro
            ):
                data = handler(data)
            size = newsize
            newsize = len(data)
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.urn"><code class="flex name class">
<span>class <span class="ident">urn</span></span>
<span>(</span><span>size='N:N', keep=False, sort=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Treat the chunks in the current frame as items in an urn and produce every possible sequence
that could occur as a sequence of draws. For example, selecting both -k and -s is equivalent
to generating all possible permutations of these chunks.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/urn.py#L11-L59" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class urn(Unit):
    &#34;&#34;&#34;
    Treat the chunks in the current frame as items in an urn and produce every possible sequence
    that could occur as a sequence of draws. For example, selecting both -k and -s is equivalent
    to generating all possible permutations of these chunks.
    &#34;&#34;&#34;

    def __init__(self,
        size: Param[str, Arg.String(metavar=&#39;a:b&#39;, help=(
            &#39;Generate sequences of length x, where x is in [a:b]. The default value is {default}, &#39;
            &#39;where N is the number of chunks in the current frame.&#39;))] = &#39;N:N&#39;,
        keep: Param[bool, Arg.Switch(&#39;-k&#39;, help=(
            &#39;Chunks are not returned back to the urn after being drawn.&#39;))] = False,
        sort: Param[bool, Arg.Switch(&#39;-s&#39;, help=(
            &#39;The order of items does not matter; for the output, chunks are sorted according to &#39;
            &#39;their original position in the frame.&#39;))] = False
    ):
        super().__init__(size=size, keep=keep, sort=sort)

    def process(self, data: Chunk):
        yield from data.temp

    def filter(self, chunks: Iterable[Chunk]):
        it = iter(chunks)
        head = next(it)
        buffer = [bytes(head)]
        buffer.extend(bytes(c) for c in it)
        head = head.copy(meta=True, data=False)
        head.meta[&#39;N&#39;] = len(buffer)
        size = sliceobj(self.args.size, head)
        a = size.start or 1
        b = size.stop or len(buffer)
        b = max(b, a + 1)
        c = size.step or 1
        self.log_debug(F&#39;using size [{a}:{b}:{c}]&#39;)
        s = 1 if self.args.sort else 0
        k = 1 if self.args.keep else 0
        m = (s &lt;&lt; 1) | k
        method = {
            0b00: lambda i, r: product(i, repeat=r),
            0b01: combinations,
            0b10: combinations_with_replacement,
            0b11: permutations
        }[m]
        self.log_info(F&#39;choosing {method.__name__}&#39;)
        for n in range(a, b, c):
            self.log_debug(F&#39;generating sequences of length {n}&#39;)
            for head.temp in method(buffer, n):
                yield head</code></pre>
</details>
</dd>
<dt id="refinery.shell.uuenc"><code class="flex name class">
<span>class <span class="ident">uuenc</span></span>
</code></dt>
<dd>
<section class="desc"><p>Unit for uuencode.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/uuenc.py#L13-L77" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class uuenc(Unit):
    &#34;&#34;&#34;
    Unit for uuencode.
    &#34;&#34;&#34;
    def process(self, data):
        header = re.search(
            B&#39;^begin ([0-7]{3}) (.*?)$&#39;, data, flags=re.M)
        if header is None:
            raise ValueError(&#39;invalid uu header&#39;)
        output = bytearray()
        view = memoryview(data)
        breaks = [m.end() for m in iter(re.finditer(B&#39;^&#39;, data, flags=re.M))]
        eol = False
        for k, br in enumerate(itertools.islice(breaks, 1, None)):
            if eol and view[br:br + 3] == b&#39;end&#39;:
                path = header[2]
                if path != B&#39;-&#39;:
                    output = self.labelled(output, path=path)
                return output
            count = view[br] - 0x20
            if count not in range(0x41):
                raise ValueError(F&#39;Invalid length encoding 0x{view[br]:02X} in line {k}.&#39;)
            count %= 0x40
            cursor = len(output)
            q, r = divmod(count, 3)
            q += int(bool(r))
            end = br + 1 + q * 4
            for b in range(br + 1, end, 4):
                chunk = 0
                for j in range(4):
                    character = view[b + j]
                    if character not in range(0x21, 0x61):
                        raise ValueError(F&#39;Invalid character 0x{character:02X} in line {k}.&#39;)
                    chunk = ((character - 0x20) % 0x40) | (chunk &lt;&lt; 6)
                output.extend(chunk.to_bytes(3, &#39;big&#39;))
            del output[cursor + count:]
            eol = count == 0
            if len(output) &lt; cursor + count:
                break
        raise RefineryPartialResult(F&#39;Data truncated in line {k}&#39;, output)

    def reverse(self, data):
        meta = metavars(data)
        path = meta.get(&#39;path&#39;, None)
        name = path and pathlib.Path(path).name or &#39;-&#39;
        view = memoryview(data)
        with MemoryFile() as stream:
            stream.write(B&#39;begin 666 &#39;)
            stream.write(name.encode(self.codec))
            for k in range(0, len(view), 45):
                slice = view[k:k + 45]
                stream.write_byte(0x0A)
                stream.write_byte(0x20 + len(slice))
                for chunk in chunks.unpack(slice, 3, bigendian=True, pad=True):
                    for j in range(3, -1, -1):
                        stream.write_byte(0x20 + (((chunk &gt;&gt; j * 6) &amp; 0x3F) or 0x40))
            stream.write(B&#39;\n`\nend\n&#39;)
            return stream.getvalue()

    @classmethod
    def handles(cls, data):
        if len(data) &lt; 16:
            return False
        if data[:6] == B&#39;begin &#39;:
            return re.fullmatch(B&#39;[0-7]{3}&#39;, data[6:9]) is not None</code></pre>
</details>
</dd>
<dt id="refinery.shell.vaddr"><code class="flex name class">
<span>class <span class="ident">vaddr</span></span>
<span>(</span><span>*name, base=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Converts a metadata variable holding a file offset to a virtual address. This unit only works when the
chunk body contains a PE, ELF, or MachO executable. The variable will be substituted in place. If you
would like to retain the original value, it is recommended to use the <code><a title="refinery.put" href="index.html#refinery.put">put</a></code> unit first to create
a copy of an already existing variable, and then convert the copy.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/exe/vaddr.py#L9-L45" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class vaddr(Unit):
    &#34;&#34;&#34;
    Converts a metadata variable holding a file offset to a virtual address. This unit only works when the
    chunk body contains a PE, ELF, or MachO executable. The variable will be substituted in place. If you
    would like to retain the original value, it is recommended to use the `refinery.put` unit first to create
    a copy of an already existing variable, and then convert the copy.
    &#34;&#34;&#34;

    def __init__(
        self, *name: Param[str, Arg.String(help=&#39;The name of a metadata variable holding an integer.&#39;)],
        base: Param[int, Arg.Number(&#39;-b&#39;, metavar=&#39;ADDR&#39;, help=&#39;Optionally specify a custom base address B.&#39;)] = None
    ):
        return super().__init__(names=name, base=base)

    def process(self, data):
        try:
            exe = Executable.Load(data, self.args.base)
        except Exception:
            self.log_warn(&#39;unable to parse input as executable; no variable conversion was performed&#39;)
            return data
        meta = metavars(data)
        for name in self.args.names:
            value = meta[name]
            meta[name] = exe.location_from_offset(value).virtual.position
        return data

    def reverse(self, data):
        try:
            exe = Executable.Load(data, self.args.base)
        except Exception:
            self.log_warn(&#39;unable to parse input as executable; no variable conversion was performed&#39;)
            return data
        meta = metavars(data)
        for name in self.args.names:
            value = meta[name]
            meta[name] = exe.location_from_address(value).physical.position
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.vbapc"><code class="flex name class">
<span>class <span class="ident">vbapc</span></span>
<span>(</span><span>raw=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract VBA macro p-code from Office documents. By default, the unit also uses pcode2code to
decompile the disassembled p-code. This unit is specifically useful for macro documents that
use VBA code stomping, i.e. the embedded macro source code is stomped and does not represent
the p-code functionality that the document will actually execute.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/office/vbapc.py#L12-L44" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class vbapc(Unit):
    &#34;&#34;&#34;
    Extract VBA macro p-code from Office documents. By default, the unit also uses pcode2code to
    decompile the disassembled p-code. This unit is specifically useful for macro documents that
    use VBA code stomping, i.e. the embedded macro source code is stomped and does not represent
    the p-code functionality that the document will actually execute.
    &#34;&#34;&#34;
    def __init__(self, raw: Param[bool, Arg.Switch(&#39;-r&#39;, help=&#39;Return disassembled p-code, do not try to decompile.&#39;)] = False):
        super().__init__(raw=raw)

    @Unit.Requires(&#39;oletools&#39;, [&#39;formats&#39;, &#39;office&#39;, &#39;extended&#39;])
    def _pcodedmp():
        with NoLogging(NoLogging.Mode.ALL):
            import pcodedmp.pcodedmp
            return pcodedmp.pcodedmp

    def process(self, data):
        class args:
            disasmOnly = True
            verbose = False
        with io.StringIO() as output:
            with VirtualFileSystem() as vfs:
                vf = vfs.new(data)
                self._pcodedmp.processFile(vf, args, output)
            code = output.getvalue()
            if not self.args.raw:
                from refinery.lib.thirdparty.pcode2code import Parser
                parser = Parser(code)
                parser.parseInput()
                parser.processInput(False)
                code = parser.getOutput()
                code = re.sub(R&#39;(?m)^((?:Sub|Function).*?)$(?!\n[^\s])&#39;, r&#39;\n\1&#39;, code)
            return code.encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.vbastr"><code class="flex name class">
<span>class <span class="ident">vbastr</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract VBA macro variables from Office documents. The items are extracted in a directory
hierarchy that specifies their corresponding OLE stream. The stem of their file name is the
same as the variable's name. The variable can define a caption, a control tip text, and a
value; the unit extracts these with the synthesized file extension "cap", "tip", and "val",
respectively.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/office/vbastr.py#L25-L70" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class vbastr(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract VBA macro variables from Office documents. The items are extracted in a directory
    hierarchy that specifies their corresponding OLE stream. The stem of their file name is the
    same as the variable&#39;s name. The variable can define a caption, a control tip text, and a
    value; the unit extracts these with the synthesized file extension &#34;cap&#34;, &#34;tip&#34;, and &#34;val&#34;,
    respectively.
    &#34;&#34;&#34;
    @PathExtractorUnit.Requires(&#39;oletools&#39;, [&#39;formats&#39;, &#39;office&#39;])
    def _olevba():
        from oletools import olevba
        return olevba

    def unpack(self, value):
        try:
            parser = self._olevba.VBA_Parser(&#39;.&#39;, data=bytes(value), relaxed=True)
        except self._olevba.FileOpenError:
            raise ValueError(&#39;Input data not recognized by VBA parser&#39;)
        try:
            for path, name, vars in parser.extract_form_strings_extended():
                if not vars:
                    continue
                name = _txt(vars[&#39;name&#39;])
                for ext, key in {
                    &#39;cap&#39;: &#39;caption&#39;,
                    &#39;tip&#39;: &#39;control_tip_text&#39;,
                    &#39;val&#39;: &#39;value&#39;,
                }.items():
                    value = _bin(vars.get(key))
                    if not value:
                        continue
                    yield UnpackResult(F&#39;{path!s}/{name!s}/{name}.{ext}&#39;, value)
        except self._olevba.oleform.OleFormParsingError as error:
            from collections import Counter
            self.log_debug(str(error))
            self.log_info(&#39;extended form extraction failed with error; falling back to simple method&#39;)
            form_strings = list(parser.extract_form_strings())
            name_counter = Counter(name for _, name, _ in form_strings)
            dedup = Counter()
            for path, name, string in form_strings:
                if string is None:
                    continue
                if name_counter[name] &gt; 1:
                    dedup[name] += 1
                    name = F&#39;{name!s}.v{dedup[name]}&#39;
                yield UnpackResult(F&#39;{path!s}/{name!s}.val&#39;, _bin(string))</code></pre>
</details>
</dd>
<dt id="refinery.shell.vigenere"><code class="flex name class">
<span>class <span class="ident">vigenere</span></span>
<span>(</span><span>key, alphabet=b'abcdefghijklmnopqrstuvwxyz', operator='add', case_sensitive=False, ignore_unknown=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Encryption and decryption using the Vigenère-Bellaso polyalphabetic cipher.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/vigenere.py#L21-L93" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class vigenere(Unit):
    &#34;&#34;&#34;
    Encryption and decryption using the Vigenère-Bellaso polyalphabetic cipher.
    &#34;&#34;&#34;

    def __init__(
        self,
        key: Param[buf, Arg(help=&#39;The encryption key&#39;)],
        alphabet: Param[buf, Arg(
            help=&#39;The alphabet, by default the Latin one is used: &#34;{default}&#34;&#39;
        )] = b&#39;abcdefghijklmnopqrstuvwxyz&#39;,
        operator: Param[str, Arg.Choice(&#39;-:&#39;, choices=[&#39;add&#39;, &#39;sub&#39;, &#39;xor&#39;], metavar=&#39;OP&#39;, help=(
            &#39;Choose the vigenere block operation. The default is {default}, and the available options are: {choices}&#39;))] = &#39;add&#39;,
        case_sensitive: Param[bool, Arg.Switch(&#39;-c&#39;, help=(
            &#39;Unless this option is set, the key will be case insensitive. Uppercase letters from the input are transformed &#39;
            &#39;using the same shift as would be the lowercase variant, but case is retained.&#39;))] = False,
        ignore_unknown: Param[bool, Arg.Switch(&#39;-i&#39;, help=(
            &#39;Unless this option is set, the key stream will be iterated even &#39;
            &#39;for letters that are not contained in the alphabet.&#39;
        ))] = False
    ):
        if not callable(operator):
            operator = {
                &#39;add&#39;: __add__,
                &#39;sub&#39;: __sub__,
                &#39;xor&#39;: __xor__,
            }.get(operator.lower(), None)
            if operator is None:
                raise ValueError(F&#39;The value {operator!r} is not valid as an operator.&#39;)
        self.superinit(super(), **vars())

    def _tabula_recta(self, data, reverse=True):
        key: str = self.args.key.decode(self.codec)
        alphabet: str = self.args.alphabet.decode(self.codec)
        operator = self.args.operator
        case_sensitive: bool = self.args.case_sensitive
        ignore_unknown: bool = self.args.ignore_unknown
        if not case_sensitive:
            key = key.lower()
            alphabet = alphabet.lower()
            if len(set(alphabet)) != len(alphabet):
                raise ValueError(&#39;Duplicate entries detected in alphabet.&#39;)
        if not set(key) &lt;= set(alphabet):
            diff = set(key) - set(alphabet)
            diff = &#39;, &#39;.join(diff)
            raise ValueError(F&#39;key contains letters which are not from the given alphabet: {diff}&#39;)
        self.log_info(F&#39;using key {key} and alphabet {alphabet}&#39;)
        keystream = cycle(key)
        alph_size = len(alphabet)
        if reverse:
            operator = _opeator_inverse[operator]
        for letter in data:
            uppercase = not case_sensitive and letter.isupper()
            if uppercase:
                letter = letter.lower()
            try:
                position = alphabet.index(letter)
            except ValueError:
                yield letter
                if not ignore_unknown:
                    next(keystream)
                continue
            shift = alphabet.index(next(keystream))
            result = alphabet[operator(position, shift) % alph_size]
            yield result.upper() if uppercase else result

    @unicoded
    def process(self, data):
        return &#39;&#39;.join(self._tabula_recta(data, True))

    @unicoded
    def reverse(self, data):
        return &#39;&#39;.join(self._tabula_recta(data, False))</code></pre>
</details>
</dd>
<dt id="refinery.shell.vmemref"><code class="flex name class">
<span>class <span class="ident">vmemref</span></span>
<span>(</span><span>*address, take=None, base=None, deref_count=1, deref_depth=2)</span>
</code></dt>
<dd>
<section class="desc"><p>The unit expects an executable as input (PE/ELF/MachO) and scans a function at a given virtual
address for memory references. For each memory reference, the unit looks up the corresponding
section and file offset for the reference. It then returns all data from that section starting
at the given offset.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/exe/vmemref.py#L15-L185" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class vmemref(Unit):
    &#34;&#34;&#34;
    The unit expects an executable as input (PE/ELF/MachO) and scans a function at a given virtual
    address for memory references. For each memory reference, the unit looks up the corresponding
    section and file offset for the reference. It then returns all data from that section starting
    at the given offset.
    &#34;&#34;&#34;

    @Unit.Requires(&#39;smda&lt;2.0&#39;, [&#39;all&#39;])
    def _smda():
        import datetime
        datetime.UTC = datetime.timezone.utc
        import smda
        import smda.Disassembler
        import smda.DisassemblyResult
        return smda

    def _memory_references(
        self,
        exe: Executable,
        function: SmdaFunction,
        codes: Container[Range],
        max_dereference_depth: int,
        max_dereference_count: int,
        references: dict,
    ):
        def is_valid_data_address(address):
            if not isinstance(address, int):
                return False
            if address not in exe:
                return False
            if address in instructions:
                return False
            for code in codes:
                if address in code:
                    return False
            return True

        def dereference(address):
            return int.from_bytes(exe[address:address + pointer_size], exe.byte_order().value)

        pointer_size = exe.pointer_size // 8

        with NoLogging():
            instructions = {op.offset: op for op in function.getInstructions()}

        for op in instructions.values():
            try:
                with NoLogging():
                    refs = list(op.getDataRefs())
            except Exception:
                continue
            for address in refs:
                try:
                    address = int(address)
                except Exception:
                    continue
                addresses = deque([address])
                while addresses:
                    address = addresses.pop()
                    if not is_valid_data_address(address):
                        continue
                    if (count := references.get(address, 0)) &gt; max_dereference_depth:
                        continue
                    elif not count:
                        yield address
                    references[address] = count + 1
                    for _ in range(max_dereference_count):
                        try:
                            point = dereference(address)
                        except Exception:
                            pass
                        else:
                            addresses.appendleft(point)
                        finally:
                            address += pointer_size

    def __init__(
        self,
        *address: Param[int, Arg.Number(metavar=&#39;ADDR&#39;, help=(
            &#39;Specify the address of a function to scan. If no argument is given, the unit will scan&#39;
            &#39; all functions for memory references.&#39;))],
        take: Param[int, Arg.Number(&#39;-t&#39;, metavar=&#39;SIZE&#39;, help=(
            &#39;Optionally specify the number of bytes to read from each reference; by default, all &#39;
            &#39;data until the end of the section is returned.&#39;))] = None,
        base: Param[int, Arg.Number(&#39;-b&#39;, metavar=&#39;ADDR&#39;,
            help=&#39;Optionally specify a custom base address B.&#39;)] = None,
        deref_count: Param[int, Arg.Number(&#39;-c&#39;, help=(
            &#39;Optionally specify the number of items to inspect at a discovered memory address as &#39;
            &#39;as a potential pointer. The default is {default}.&#39;))] = 1,
        deref_depth: Param[int, Arg.Number(&#39;-d&#39;, help=(
            &#39;Optionally specify the maximum number of times that referenced data is dereferenced &#39;
            &#39;as a pointer, potentially leading to another referenced memory location. The default &#39;
            &#39;is {default}.&#39;))] = 2,
    ):
        super().__init__(
            address=address,
            take=take,
            base=base,
            deref_count=deref_count,
            deref_depth=deref_depth,
        )

    def process(self, data):
        smda = self._smda
        take = self.args.take
        exe = Executable.Load(data, self.args.base)
        fmt = exe.pointer_size // 4
        addresses = self.args.address

        self.log_info(&#39;disassembling and exploring call graph using smda&#39;)
        with NoLogging():
            cfg = smda.Disassembler.SmdaConfig()
            cfg.CALCULATE_SCC = False
            cfg.CALCULATE_NESTING = False
            cfg.TIMEOUT = 600
            dsm = smda.Disassembler.Disassembler(cfg)
            _input = data
            if not isinstance(_input, bytes):
                _input = bytes(data)
            graph = dsm.disassembleUnmappedBuffer(_input)

        self.log_info(&#39;collecting code addresses for memory reference exclusion list&#39;)
        visits = {}
        avoid = set()

        for symbol in exe.symbols():
            if not symbol.function:
                continue
            if not symbol.exported:
                continue
            avoid.add(exe.location_from_address(symbol.address).virtual.box)

        if addresses:
            reset = visits.clear
        else:
            def reset():
                pass
            self.log_info(&#39;scanning executable for functions&#39;)
            with NoLogging():
                addresses = [pfn.offset for pfn in graph.getFunctions()]
                addresses.sort()

        for a in addresses:
            reset()
            address, function = min(
                graph.xcfg.items(), key=lambda t: (abs(t[0] - a), t[0] &gt;= a))
            self.log_debug(F&#39;scanning function: 0x{address:0{fmt}X}&#39;)
            refs = list(self._memory_references(
                exe,
                function,
                avoid,
                self.args.deref_depth,
                self.args.deref_count,
                visits,
            ))
            refs.sort(reverse=True)
            last_start = None
            for ref in refs:
                try:
                    box = exe.location_from_address(ref)
                    end = box.physical.box.upper
                    if take is not None:
                        end = min(box.physical.position + take, end)
                    if last_start is not None:
                        end = min(last_start, end)
                    last_start = box.physical.position
                except CompartmentNotFound:
                    self.log_info(F&#39;memory reference could not be resolved: 0x{ref:0{fmt}X}&#39;)
                else:
                    yield exe.data[last_start:end]</code></pre>
</details>
</dd>
<dt id="refinery.shell.vsect"><code class="flex name class">
<span>class <span class="ident">vsect</span></span>
<span>(</span><span>*paths, meta=False, synthetic=False, path=b'path', regex=False, exact=False, fuzzy=0, drop_path=False, join_path=False, list=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract sections/segments from PE, ELF, and MachO executables.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/exe/vsect.py#L8-L45" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class vsect(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract sections/segments from PE, ELF, and MachO executables.
    &#34;&#34;&#34;
    def __init__(
        self, *paths,
        meta: Param[bool, Arg.Switch(&#39;-m&#39;, help=(
            &#39;Populates the metadata variables vaddr and vsize containing the virtual address and size &#39;
            &#39;of each section, respectively.&#39;))] = False,
        synthetic: Param[bool, Arg.Switch(&#39;-s&#39;, help=(
            &#39;Include synthesized sections: These represent data regions that are outside the sections &#39;
            &#39;as listed by the executable metadata, such as headers and overlays.&#39;))] = False,
        **keywords
    ):
        super().__init__(*paths, meta=meta, synthetic=synthetic, **keywords)

    def unpack(self, data):
        exe = Executable.Load(data)
        mv = memoryview(data)
        for k, section in enumerate(exe.sections()):
            if section.synthetic and not self.args.synthetic:
                continue
            start = section.physical.lower
            end = section.physical.upper
            va = section.virtual.lower
            vs = len(section.virtual)
            kwargs = {&#39;offset&#39;: start}
            if self.args.meta:
                if va is not None:
                    kwargs[&#39;vaddr&#39;] = va
                if vs is not None:
                    kwargs[&#39;vsize&#39;] = vs
            name = section.name
            if not name:
                addr = F&#39;{section.virtual.lower:0{exe.pointer_size // 4}X}&#39;
                self.log_warn(F&#39;section {k} had no name, synthesizing name from virtual address 0x{addr}&#39;)
                name = F&#39;.{addr}&#39;
            yield UnpackResult(name, mv[start:end], **kwargs)</code></pre>
</details>
</dd>
<dt id="refinery.shell.vsnip"><code class="flex name class">
<span>class <span class="ident">vsnip</span></span>
<span>(</span><span>*addresses, ascii=False, utf16=False, until=b'', base=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract data from PE, ELF, and MachO files based on virtual offsets.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/exe/vsnip.py#L25-L78" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class vsnip(Unit):
    &#34;&#34;&#34;
    Extract data from PE, ELF, and MachO files based on virtual offsets.
    &#34;&#34;&#34;

    def __init__(
        self, *addresses: Param[slice, Arg.Bounds(metavar=&#39;start:count:align&#39;, help=(
            &#39;Use Python slice syntax to describe an area of virtual memory to read. If a chunksize is &#39;
            &#39;specified, then the unit will always read a multiple of that number of bytes&#39;))],
        ascii: Param[bool, Arg.Switch(&#39;-a&#39;, group=&#39;END&#39;,
            help=&#39;Read ASCII strings; equivalent to -th:00&#39;)] = False,
        utf16: Param[bool, Arg.Switch(&#39;-u&#39;, group=&#39;END&#39;,
            help=&#39;Read UTF16 strings; equivalent to -th:0000 (also sets chunksize to 2)&#39;)] = False,
        until: Param[buf, Arg.Binary(&#39;-t&#39;, group=&#39;END&#39;,
            help=&#39;Read until sequence {varname} is read.&#39;)] = B&#39;&#39;,
        base: Param[int | None, Arg.Number(&#39;-b&#39;, metavar=&#39;ADDR&#39;,
            help=&#39;Optionally specify a custom base address B.&#39;)] = None,
    ):
        if sum(1 for t in (until, utf16, ascii) if t) &gt; 1:
            raise ValueError(&#39;Only one of utf16, ascii, and until can be specified.&#39;)
        return super().__init__(addresses=addresses, utf16=utf16, ascii=ascii, until=until, base=base)

    def process(self, data: bytearray):
        until = self.args.until
        addrs = self.args.addresses
        if self.args.ascii:
            until = B&#39;\0&#39;
        if self.args.utf16:
            until = B&#39;\0\0&#39;
            addrs = (slice(a.start, a.stop, 2) for a in addrs)

        exe = Executable.Load(data, self.args.base)

        for addr in addrs:
            area = MemoryArea(addr)
            location = exe.location_from_address(area.start)
            offset = location.physical.position
            max_offset = location.physical.box.upper
            if not until:
                end = max_offset
            else:
                end = offset - 1
                align = area.align
                while True:
                    end = data.find(until, end + 1)
                    if end not in range(offset, max_offset):
                        raise EndOfStringNotFound
                    if (end - offset) % align == 0:
                        break

            if area.count:
                end = min(end, offset + area.count)

            yield self.labelled(data[offset:end], offset=offset)</code></pre>
</details>
</dd>
<dt id="refinery.shell.vstack"><code class="flex name class">
<span>class <span class="ident">vstack</span></span>
<span>(</span><span>*address, base=None, arch=Arch.X32, engine=Engine.unicorn, se=False, ic=False, uc=False, registers=False, timeout=0, patch_range=slice(5, None, None), write_range=slice(1, None, None), wait=20, wait_calls=False, skip_calls=0, stack_size=65536, stack_push=None, show_apis=False, show_code=False, show_memory=False, block_size=4096, max_visits=65536, log_writes_in_calls=False, log_stack_addresses=False, log_other_addresses=False, log_zero_overwrites=False, log_stack_cookies=False)</span>
</code></dt>
<dd>
<section class="desc"><p>The unit emulates instructions at a given address in the input executable (PE/ELF/MachO) and
extracts data patches that are written to memory during emulation. The unit can also be used
to emulate shellcode blobs, in which case it defaults to emulating 32bit x86 instructions.</p>
<p>Emulation is halted as soon as a certain number of instructions have not performed any memory
writes, or when an error occurs. By default, most registers are set to the current location in
the emulated stack. If you want to initialize some of them differently, the <code>-r</code> switch maes
the unit initialize register values from meta variables:</p>
<pre><code>emit shellcode [| put eax 0x2000 | vstack -r ]
</code></pre>
<p>In this pipeline, the eax register is set to <code>0x2000</code> before emulation begins.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/exe/vstack.py#L374-L567" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class vstack(EmulatingUnit):
    &#34;&#34;&#34;
    The unit emulates instructions at a given address in the input executable (PE/ELF/MachO) and
    extracts data patches that are written to memory during emulation. The unit can also be used
    to emulate shellcode blobs, in which case it defaults to emulating 32bit x86 instructions.

    Emulation is halted as soon as a certain number of instructions have not performed any memory
    writes, or when an error occurs. By default, most registers are set to the current location in
    the emulated stack. If you want to initialize some of them differently, the `-r` switch maes
    the unit initialize register values from meta variables:

        emit shellcode [| put eax 0x2000 | vstack -r ]

    In this pipeline, the eax register is set to `0x2000` before emulation begins.
    &#34;&#34;&#34;
    def __init__(
        self,
        *address: Param[str, Arg.String(metavar=&#39;a[:end|::size]&#39;,
            help=&#39;Specify a symbol name or the (virtual) addresses of what to emulate; optionally specify a stop address or a length.&#39;)],
        base=None, arch=Arch.X32, engine=Engine.unicorn, se=False, ic=False, uc=False,
        registers: Param[bool, Arg.Switch(&#39;-r&#39;, help=(
            &#39;Consume register initialization values from the chunk\&#39;s metadata. If the value is a byte string, &#39;
            &#39;the data will be mapped.&#39;))] = False,
        timeout: Param[int, Arg.Number(&#39;-t&#39;, help=&#39;Optionally stop emulating after a given number of instructions.&#39;)] = 0,
        patch_range: Param[slice, Arg.Bounds(&#39;-p&#39;, metavar=&#39;MIN:MAX&#39;,
            help=&#39;Extract only patches that are in the given range, default is {default}.&#39;)] = slice(5, None),
        write_range: Param[slice, Arg.Bounds(&#39;-n&#39;, metavar=&#39;MIN:MAX&#39;,
            help=&#39;Log only writes whose size is in the given range, default is {default}.&#39;)] = slice(1, None),
        wait: Param[int, Arg.Number(&#39;-w&#39;, help=(
            &#39;When this many instructions did not write to memory, emulation is halted. The default is {default}.&#39;))] = 20,
        wait_calls: Param[bool, Arg.Switch(&#39;-c&#39;, group=&#39;CALL&#39;,
            help=&#39;Wait indefinitely when inside a function call.&#39;)] = False,
        skip_calls: Param[int, Arg.Counts(&#39;-C&#39;, group=&#39;CALL&#39;,
            help=&#39;Skip function calls entirely. Use twice to treat each call as allocating memory.&#39;)] = 0,
        stack_size: Param[int, Arg.Number(&#39;-S&#39;, help=&#39;Optionally specify the stack size. The default is 0x{default:X}.&#39;)] = 0x10000,
        stack_push: Param[tuple[str] | None, Arg(&#39;-u&#39;, action=&#39;append&#39;, metavar=&#39;REG&#39;,
            help=&#39;Push the value of a register to the stack before beginning emulation; implies -r.&#39;)] = None,
        show_apis: Param[bool, Arg.Switch(&#39;-A&#39;, help=&#39;Show API calls in the debug log.&#39;)] = False,
        show_code: Param[bool, Arg.Switch(&#39;-I&#39;, help=&#39;Show all executed instructions in the debug log.&#39;)] = False,
        show_memory: Param[bool, Arg.Switch(&#39;-M&#39;, help=&#39;Show all memory writes in the debug log.&#39;)] = False,
        block_size: Param[int, Arg.Number(&#39;-B&#39;, help=&#39;Standard memory block size for the emulator, 0x{default:X} by default.&#39;)] = 0x1000,
        max_visits: Param[int, Arg.Number(&#39;-V&#39;, help=&#39;Maximum number of times a code address is visited. Default is {default}.&#39;)] = 0x10000,
        log_writes_in_calls: Param[bool, Arg.Switch(&#39;-W&#39;, help=&#39;Log writes of values that occur in functions calls.&#39;)] = False,
        log_stack_addresses: Param[bool, Arg.Switch(&#39;-X&#39;, help=&#39;Log writes of values that are stack addresses.&#39;)] = False,
        log_other_addresses: Param[bool, Arg.Switch(&#39;-Y&#39;, help=&#39;Log writes of values that are addresses to mapped segments.&#39;)] = False,
        log_zero_overwrites: Param[bool, Arg.Switch(&#39;-Z&#39;, help=&#39;Log writes of zeros to memory that contained nonzero values.&#39;)] = False,
        log_stack_cookies: Param[bool, Arg.Switch(&#39;-E&#39;, help=&#39;Log writes that look like stack cookies.&#39;)] = False,
    ):
        super().__init__(
            base=base,
            arch=arch,
            engine=engine,
            se=se,
            ic=ic,
            uc=uc,
            address=address,
            registers=registers,
            timeout=timeout,
            patch_range=patch_range,
            write_range=write_range,
            wait=wait,
            stack_size=stack_size,
            stack_push=stack_push,
            wait_calls=wait_calls,
            skip_calls=skip_calls,
            block_size=block_size,
            max_visits=max_visits,
            show_apis=show_apis,
            show_code=show_code,
            show_memory=show_memory,
            log_writes_in_calls=log_writes_in_calls,
            log_stack_addresses=log_stack_addresses,
            log_other_addresses=log_other_addresses,
            log_zero_overwrites=log_zero_overwrites,
            log_stack_cookies=log_stack_cookies
        )

    def process(self, data: Chunk):
        meta = metavars(data)
        args = self.args

        engine = self._engine()
        flags = Hook.Default | Hook.ApiCall
        self.log_debug(F&#39;attempting to use {engine.name}&#39;)

        Emu = EmuFactory(engine.value)

        emu = Emu(
            data,
            args.base,
            args.arch,
            flags,
            args.block_size,
            args.stack_size,
        )

        cfg = EmuConfig(
            args.wait_calls,
            args.skip_calls,
            args.write_range,
            args.wait,
            args.block_size,
            args.stack_size,
            args.max_visits,
            args.log_stack_cookies,
            args.log_writes_in_calls,
            args.log_stack_addresses,
            args.log_other_addresses,
            args.log_zero_overwrites,
            args.show_apis,
            args.show_code,
            args.show_memory,
        )

        register_values: dict[Register, int | str | bytes] = {}
        emu.reset(None)

        if args.registers or args.stack_push:
            for var, value in list(meta.items()):
                try:
                    register = emu.lookup_register(var)
                except LookupError:
                    continue
                meta.discard(var)
                register_values[register] = value

        if not (addresses := [
            self._parse_address(data, emu.exe, a) for a in args.address
        ]):
            for symbol in emu.exe.symbols():
                if symbol.name is None:
                    addresses.append(slice(symbol.address, None))
                    break

        for cursor in addresses:
            state = EmuState(cfg, cursor.start, emu.exe.pointer_size // 4, stop=cursor.stop)
            emu.reset(state)
            emu.push((1 &lt;&lt; emu.exe.pointer_size) - 1)

            for reg in emu.general_purpose_registers():
                emu.set_register(reg, 0)

            for reg in register_values:
                # check if we are tainting a general purpose register
                emu.set_register(reg, 1)

            for reg in emu.general_purpose_registers():
                if emu.get_register(reg) == 0:
                    state.init_registers.append(reg)

            for reg, value in register_values.items():
                if isinstance(value, int):
                    self.log_info(F&#39;setting {reg.name} to integer value 0x{value:X}&#39;)
                    emu.set_register(reg, value)
                    continue
                if isinstance(value, str):
                    value = value.encode()
                if isbuffer(value):
                    start = emu.malloc(len(value))
                    emu.mem_write(start, bytes(value))
                    emu.set_register(reg, start)
                    self.log_info(F&#39;setting {reg.name} to mapped buffer of size 0x{len(value):X}&#39;)
                    continue
                _tn = value.__class__.__name__
                self.log_warn(F&#39;canot interpret value of type {_tn} for register {reg.name}&#39;)

            if push := args.stack_push:
                for reg in push:
                    emu.push_register(reg)

            timeout = args.timeout
            if timeout:
                self.log_info(F&#39;setting timeout of {timeout} steps&#39;)
                state.ticks = timeout

            try:
                emu.emulate(
                    emu.base_exe_to_emu(cursor.start),
                    emu.base_exe_to_emu(cursor.stop),
                )
            except EmulationError:
                pass

            for patch, api in state.synthesized.items():
                chunk = self.labelled(patch, src=api)
                yield chunk

            valid = bounds[args.patch_range]
            for base, patch in state.memory:
                if len(patch) not in valid or not any(patch):
                    continue
                self.log_info(F&#39;memory patch at {state.fmt(base)} of size {len(patch)}&#39;)
                chunk = self.labelled(patch, src=base)
                yield chunk</code></pre>
</details>
</dd>
<dt id="refinery.shell.winreg"><code class="flex name class">
<span>class <span class="ident">winreg</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract values from a Windows registry hive or from a registry export (.reg file).</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/winreg.py#L31-L182" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class winreg(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract values from a Windows registry hive or from a registry export (.reg file).
    &#34;&#34;&#34;
    @PathExtractorUnit.Requires(&#39;python-registry&#39;, [&#39;formats&#39;])
    def _registry():
        import Registry
        import Registry.Registry
        import Registry.RegistryParse
        return Registry

    @staticmethod
    def _walk(patterns: list[PathPattern], key: RegistryKey, *path: str):
        here = &#39;/&#39;.join(path)
        if not any(p.reach(here) for p in patterns):
            winreg.log_debug(F&#39;pruning search at {here}&#39;)
            return
        for value in key.values():
            def raw(v: RegistryValue = value):
                return v.raw_data()
            vpath = here
            vname = value.name()
            if vname != &#39;(default)&#39;:
                vpath = F&#39;{vpath}/{vname}&#39;
            yield UnpackResult(vpath, raw)
        for subkey in key.subkeys():
            yield from winreg._walk(patterns, subkey, *path, subkey.name())

    def _unpack_hive(self, data: bytearray):
        try:
            with MemoryFile(data) as stream:
                root = self._registry.Registry.Registry(stream).root()
                yield from self._walk(self._patterns, root, root.name())
        except self._registry.RegistryParse.ParseException:
            raise ParseException

    def _decode_registry_export(self, data: str):
        def REG_BINARY(data: str) -&gt; bytes:
            return bytes.fromhex(re.sub(&#39;[^a-f0-9]+&#39;, &#39;&#39;, data))

        def REG_SZ(data: str) -&gt; bytes:
            return data.encode(self.codec) | esc(quoted=True) | bytes

        def REG_EXPAND_SZ(data: str):
            return REG_BINARY(data).decode(&#39;UTF-16LE&#39;).rstrip(&#39;\0&#39;).encode(self.codec)

        def REG_MULTI_SZ(data: str):
            for string in REG_BINARY(data).decode(&#39;UTF-16LE&#39;).split(&#39;\0&#39;):
                if string:
                    yield string.encode(self.codec)

        def REG_DWORD(data: str):
            value = int(data, 16)
            return F&#39;0x{value:X}&#39;.encode(self.codec)

        def REG_QWORD(data: str):
            value = int.from_bytes(REG_BINARY(data), &#39;little&#39;)
            return F&#39;0x{value:X}&#39;.encode(self.codec)

        class Missing:
            def __init__(self, name: str): self.name = name
            def __str__(self): return self.name

        REG_NONE = REG_EXPAND_SZ
        REG_DWORD_BIG_ENDIAN = Missing(&#39;REG_DWORD_BIG_ENDIAN&#39;)
        REG_LINK = Missing(&#39;REG_LINK&#39;)
        REG_RESOURCE_LIST = Missing(&#39;REG_RESOURCE_LIST&#39;)
        REG_FULL_RESOURCE_DESCRIPTOR = Missing(&#39;REG_FULL_RESOURCE_DESCRIPTOR&#39;)
        REG_RESOURCE_REQUIREMENTS_LIST = Missing(&#39;REG_RESOURCE_REQUIREMENTS_LIST&#39;)

        prefix, _, encoded = data.partition(&#39;:&#39;)

        try:
            decoder = {
                &#39;hex(0)&#39; : REG_NONE,
                &#39;hex(1)&#39; : REG_SZ,
                &#39;hex(2)&#39; : REG_EXPAND_SZ,
                &#39;hex(3)&#39; : REG_BINARY,
                &#39;hex&#39;    : REG_BINARY,
                &#39;hex(4)&#39; : REG_DWORD,
                &#39;dword&#39;  : REG_DWORD,
                &#39;hex(5)&#39; : REG_DWORD_BIG_ENDIAN,
                &#39;hex(6)&#39; : REG_LINK,
                &#39;hex(7)&#39; : REG_MULTI_SZ,
                &#39;hex(8)&#39; : REG_RESOURCE_LIST,
                &#39;hex(9)&#39; : REG_FULL_RESOURCE_DESCRIPTOR,
                &#39;hex(a)&#39; : REG_RESOURCE_REQUIREMENTS_LIST,
                &#39;hex(b)&#39; : REG_QWORD,
            }[prefix]
        except KeyError:
            decoder = REG_SZ
            encoded = data

        if isinstance(decoder, Missing):
            self.log_warn(F&#39;Found registry type {decoder!s}; no decoder implemented.&#39;)
            return
        self.log_debug(F&#39;decoding as {decoder.__name__}: {encoded}&#39;)
        it = decoder(encoded)
        if not inspect.isgenerator(it):
            it = (it,)
        yield from it

    def _unpack_file(self, data: bytearray):
        for codec in (&#39;utf16&#39;, &#39;utf-16le&#39;, &#39;utf8&#39;):
            try:
                reg = data.decode(codec).splitlines(keepends=True)
            except UnicodeError:
                continue
            lines = iter(reg)
            if next(lines).startswith(&#39;Windows Registry Editor&#39;):
                break
        else:
            raise ParseException

        def _parse():
            parser = WinRegFileParser()
            section.seek(0)
            parser.read_file(section)
            for key in parser.sections():
                self.log_debug(key)
                for value in parser[key]:
                    name = next(iter(shlex.split(value)))
                    path = Path(key)
                    if name != &#39;@&#39;:
                        path = path / Path(name)
                    decoded = list(self._decode_registry_export(parser[key][value]))
                    if len(decoded) == 1:
                        yield UnpackResult(str(path), decoded[0])
                        continue
                    for k, d in enumerate(decoded):
                        yield UnpackResult(F&#39;{path!s}.{k}&#39;, d)

        section = io.StringIO()

        for line in lines:
            if line.lstrip().startswith(&#39;[&#39;):
                yield from _parse()
                section.seek(0)
                section.truncate(0)
            section.write(line)

        yield from _parse()

    def unpack(self, data):
        with contextlib.suppress(ParseException):
            yield from self._unpack_hive(data)
            return
        yield from self._unpack_file(data)

    @classmethod
    def handles(cls, data):
        return get_reg_export_type(data) is not None</code></pre>
</details>
</dd>
<dt id="refinery.shell.wshenc"><code class="flex name class">
<span>class <span class="ident">wshenc</span></span>
<span>(</span><span>marker=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Windows Scripting Host encoding and decoding of VBScript (VBS/VBE) and JScript (JS/JSE).</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/wshenc.py#L11-L169" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class wshenc(Unit):
    &#34;&#34;&#34;
    Windows Scripting Host encoding and decoding of VBScript (VBS/VBE) and JScript (JS/JSE).
    &#34;&#34;&#34;

    _MARKER_INIT = RB&#39;#@~^BINREF==&#39;
    _MARKER_STOP = RB&#39;BINREF==^#~@&#39;

    _CHUNKS = (
        0x57, 0x6E, 0x7B, 0x4A, 0x4C, 0x41, 0x0B, 0x0B, 0x0B, 0x0C, 0x0C, 0x0C, 0x4A, 0x4C, 0x41,
        0x0E, 0x0E, 0x0E, 0x0F, 0x0F, 0x0F, 0x10, 0x10, 0x10, 0x11, 0x11, 0x11, 0x12, 0x12, 0x12,
        0x13, 0x13, 0x13, 0x14, 0x14, 0x14, 0x15, 0x15, 0x15, 0x16, 0x16, 0x16, 0x17, 0x17, 0x17,
        0x18, 0x18, 0x18, 0x19, 0x19, 0x19, 0x1A, 0x1A, 0x1A, 0x1B, 0x1B, 0x1B, 0x1C, 0x1C, 0x1C,
        0x1D, 0x1D, 0x1D, 0x1E, 0x1E, 0x1E, 0x1F, 0x1F, 0x1F, 0x2E, 0x2D, 0x32, 0x47, 0x75, 0x30,
        0x7A, 0x52, 0x21, 0x56, 0x60, 0x29, 0x42, 0x71, 0x5B, 0x6A, 0x5E, 0x38, 0x2F, 0x49, 0x33,
        0x26, 0x5C, 0x3D, 0x49, 0x62, 0x58, 0x41, 0x7D, 0x3A, 0x34, 0x29, 0x35, 0x32, 0x36, 0x65,
        0x5B, 0x20, 0x39, 0x76, 0x7C, 0x5C, 0x72, 0x7A, 0x56, 0x43, 0x7F, 0x73, 0x38, 0x6B, 0x66,
        0x39, 0x63, 0x4E, 0x70, 0x33, 0x45, 0x45, 0x2B, 0x6B, 0x68, 0x68, 0x62, 0x71, 0x51, 0x59,
        0x4F, 0x66, 0x78, 0x09, 0x76, 0x5E, 0x62, 0x31, 0x7D, 0x44, 0x64, 0x4A, 0x23, 0x54, 0x6D,
        0x75, 0x43, 0x71, 0x4A, 0x4C, 0x41, 0x7E, 0x3A, 0x60, 0x4A, 0x4C, 0x41, 0x5E, 0x7E, 0x53,
        0x40, 0x4C, 0x40, 0x77, 0x45, 0x42, 0x4A, 0x2C, 0x27, 0x61, 0x2A, 0x48, 0x5D, 0x74, 0x72,
        0x22, 0x27, 0x75, 0x4B, 0x37, 0x31, 0x6F, 0x44, 0x37, 0x4E, 0x79, 0x4D, 0x3B, 0x59, 0x52,
        0x4C, 0x2F, 0x22, 0x50, 0x6F, 0x54, 0x67, 0x26, 0x6A, 0x2A, 0x72, 0x47, 0x7D, 0x6A, 0x64,
        0x74, 0x39, 0x2D, 0x54, 0x7B, 0x20, 0x2B, 0x3F, 0x7F, 0x2D, 0x38, 0x2E, 0x2C, 0x77, 0x4C,
        0x30, 0x67, 0x5D, 0x6E, 0x53, 0x7E, 0x6B, 0x47, 0x6C, 0x66, 0x34, 0x6F, 0x35, 0x78, 0x79,
        0x25, 0x5D, 0x74, 0x21, 0x30, 0x43, 0x64, 0x23, 0x26, 0x4D, 0x5A, 0x76, 0x52, 0x5B, 0x25,
        0x63, 0x6C, 0x24, 0x3F, 0x48, 0x2B, 0x7B, 0x55, 0x28, 0x78, 0x70, 0x23, 0x29, 0x69, 0x41,
        0x28, 0x2E, 0x34, 0x73, 0x4C, 0x09, 0x59, 0x21, 0x2A, 0x33, 0x24, 0x44, 0x7F, 0x4E, 0x3F,
        0x6D, 0x50, 0x77, 0x55, 0x09, 0x3B, 0x53, 0x56, 0x55, 0x7C, 0x73, 0x69, 0x3A, 0x35, 0x61,
        0x5F, 0x61, 0x63, 0x65, 0x4B, 0x50, 0x46, 0x58, 0x67, 0x58, 0x3B, 0x51, 0x31, 0x57, 0x49,
        0x69, 0x22, 0x4F, 0x6C, 0x6D, 0x46, 0x5A, 0x4D, 0x68, 0x48, 0x25, 0x7C, 0x27, 0x28, 0x36,
        0x5C, 0x46, 0x70, 0x3D, 0x4A, 0x6E, 0x24, 0x32, 0x7A, 0x79, 0x41, 0x2F, 0x37, 0x3D, 0x5F,
        0x60, 0x5F, 0x4B, 0x51, 0x4F, 0x5A, 0x20, 0x42, 0x2C, 0x36, 0x65, 0x57)
    _OFFSETS = (
        0, 1, 2, 0, 1, 2, 1, 2, 2, 1, 2, 1, 0, 2, 1, 2, 0, 2, 1, 2, 0, 0, 1, 2, 2, 1, 0, 2, 1, 2, 2, 1,
        0, 0, 2, 1, 2, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 1, 0, 2, 1, 2, 0, 0, 1, 2, 2, 0, 0, 1, 2, 0, 2, 1)
    _ENCODER = {
        0x09 : [0x37, 0x69, 0x64], 0x0B : [0x0B, 0x0B, 0x0B], 0x0C : [0x0C, 0x0C, 0x0C],
        0x0E : [0x0E, 0x0E, 0x0E], 0x0F : [0x0F, 0x0F, 0x0F], 0x10 : [0x10, 0x10, 0x10],
        0x11 : [0x11, 0x11, 0x11], 0x12 : [0x12, 0x12, 0x12], 0x13 : [0x13, 0x13, 0x13],
        0x14 : [0x14, 0x14, 0x14], 0x15 : [0x15, 0x15, 0x15], 0x16 : [0x16, 0x16, 0x16],
        0x17 : [0x17, 0x17, 0x17], 0x18 : [0x18, 0x18, 0x18], 0x19 : [0x19, 0x19, 0x19],
        0x1A : [0x1A, 0x1A, 0x1A], 0x1B : [0x1B, 0x1B, 0x1B], 0x1C : [0x1C, 0x1C, 0x1C],
        0x1D : [0x1D, 0x1D, 0x1D], 0x1E : [0x1E, 0x1E, 0x1E], 0x1F : [0x1F, 0x1F, 0x1F],
        0x20 : [0x7E, 0x2C, 0x50], 0x21 : [0x5A, 0x65, 0x22], 0x22 : [0x45, 0x72, 0x4A],
        0x23 : [0x3A, 0x5B, 0x61], 0x24 : [0x79, 0x66, 0x5E], 0x25 : [0x59, 0x75, 0x5D],
        0x26 : [0x27, 0x4C, 0x5B], 0x27 : [0x76, 0x45, 0x42], 0x28 : [0x63, 0x76, 0x60],
        0x29 : [0x62, 0x2A, 0x23], 0x2A : [0x4D, 0x43, 0x65], 0x2B : [0x51, 0x33, 0x5F],
        0x2C : [0x53, 0x42, 0x7E], 0x2D : [0x52, 0x20, 0x4F], 0x2E : [0x20, 0x63, 0x52],
        0x2F : [0x26, 0x4A, 0x7A], 0x30 : [0x54, 0x5A, 0x21], 0x31 : [0x71, 0x38, 0x46],
        0x32 : [0x2B, 0x79, 0x20], 0x33 : [0x66, 0x32, 0x26], 0x34 : [0x2A, 0x57, 0x63],
        0x35 : [0x58, 0x6C, 0x2A], 0x36 : [0x7F, 0x2B, 0x76], 0x37 : [0x7B, 0x46, 0x47],
        0x38 : [0x30, 0x52, 0x25], 0x39 : [0x31, 0x4F, 0x2C], 0x3A : [0x6C, 0x3D, 0x29],
        0x3B : [0x49, 0x70, 0x69], 0x3D : [0x78, 0x7B, 0x27], 0x3F : [0x5F, 0x51, 0x67],
        0x40 : [0x40, 0x40, 0x40], 0x41 : [0x29, 0x7A, 0x62], 0x42 : [0x24, 0x7E, 0x41], # noqa
        0x43 : [0x2F, 0x3B, 0x5A], 0x44 : [0x39, 0x47, 0x66], 0x45 : [0x33, 0x41, 0x32],
        0x46 : [0x6F, 0x77, 0x73], 0x47 : [0x21, 0x56, 0x4D], 0x48 : [0x75, 0x5F, 0x43],
        0x49 : [0x28, 0x26, 0x71], 0x4A : [0x42, 0x78, 0x39], 0x4B : [0x46, 0x6E, 0x7C],
        0x4C : [0x4A, 0x64, 0x53], 0x4D : [0x5C, 0x74, 0x48], 0x4E : [0x48, 0x67, 0x31],
        0x4F : [0x36, 0x7D, 0x72], 0x50 : [0x4B, 0x68, 0x6E], 0x51 : [0x7D, 0x35, 0x70],
        0x52 : [0x5D, 0x22, 0x49], 0x53 : [0x6A, 0x55, 0x3F], 0x54 : [0x50, 0x3A, 0x4B],
        0x55 : [0x69, 0x60, 0x6A], 0x56 : [0x23, 0x6A, 0x2E], 0x57 : [0x09, 0x71, 0x7F],
        0x58 : [0x70, 0x6F, 0x28], 0x59 : [0x65, 0x49, 0x35], 0x5A : [0x74, 0x5C, 0x7D],
        0x5B : [0x2C, 0x5D, 0x24], 0x5C : [0x77, 0x27, 0x2D], 0x5D : [0x44, 0x59, 0x54],
        0x5E : [0x3F, 0x25, 0x37], 0x5F : [0x6D, 0x7C, 0x7B], 0x60 : [0x7C, 0x23, 0x3D],
        0x61 : [0x43, 0x6D, 0x6C], 0x62 : [0x38, 0x28, 0x34], 0x63 : [0x5E, 0x31, 0x6D],
        0x64 : [0x5B, 0x39, 0x4E], 0x65 : [0x6E, 0x7F, 0x2B], 0x66 : [0x57, 0x36, 0x30],
        0x67 : [0x4C, 0x54, 0x6F], 0x68 : [0x34, 0x34, 0x74], 0x69 : [0x72, 0x62, 0x6B],
        0x6A : [0x25, 0x4E, 0x4C], 0x6B : [0x56, 0x30, 0x33], 0x6C : [0x73, 0x5E, 0x56],
        0x6D : [0x68, 0x73, 0x3A], 0x6E : [0x55, 0x09, 0x78], 0x6F : [0x47, 0x4B, 0x57],
        0x70 : [0x32, 0x61, 0x77], 0x71 : [0x35, 0x24, 0x3B], 0x72 : [0x2E, 0x4D, 0x44],
        0x73 : [0x64, 0x6B, 0x2F], 0x74 : [0x4F, 0x44, 0x59], 0x75 : [0x3B, 0x21, 0x45],
        0x76 : [0x2D, 0x37, 0x5C], 0x77 : [0x41, 0x53, 0x68], 0x78 : [0x61, 0x58, 0x36],
        0x79 : [0x7A, 0x48, 0x58], 0x7A : [0x22, 0x2E, 0x79], 0x7B : [0x60, 0x50, 0x09],
        0x7C : [0x6B, 0x2D, 0x75], 0x7D : [0x4E, 0x29, 0x38], 0x7E : [0x3D, 0x3F, 0x55],
        0x7F : [0x67, 0x2F, 0x51]
    }

    _ESCAPE = {
        0x40: B&#39;@$&#39;,
        0x3C: B&#39;@!&#39;,
        0x3E: B&#39;@*&#39;,
        0x0D: B&#39;@#&#39;,
        0x0A: B&#39;@&amp;&#39;,
    }

    _UNESCAPE = {
        B&#39;@$&#39;: B&#39;@&#39;,
        B&#39;@!&#39;: B&#39;&lt;&#39;,
        B&#39;@*&#39;: B&#39;&gt;&#39;,
        B&#39;@#&#39;: B&#39;\r&#39;,
        B&#39;@&amp;&#39;: B&#39;\n&#39;,
    }

    def __init__(
        self,
        marker: Param[bool, Arg.Switch(&#39;-m&#39;, &#39;--no-marker&#39;, off=True, help=(
            &#39;Do not require magic marker when encoding and do not search for &#39;
            &#39;marker when decoding.&#39;)
        )] = True
    ):
        super().__init__(marker=marker)

    @classmethod
    def _chunk(cls, byte, index):
        k = byte - 9
        c = cls._CHUNKS[k * 3 : k * 3 + 3]
        return c[cls._OFFSETS[index % 64]]

    def _escape(self, iterable):
        if self.args.marker:
            yield from self._MARKER_INIT
        for byte in iterable:
            try:
                yield from self._ESCAPE[byte]
            except KeyError:
                yield byte
        if self.args.marker:
            yield from self._MARKER_STOP

    def _unescape(self, data):
        def unescaper(m): return self._UNESCAPE[m[0]]
        return re.sub(RB&#39;@[$!*#&amp;]&#39;, unescaper, data)

    @classmethod
    def _decoded(cls, data):
        index = -1
        for byte in data:
            if byte &lt; 128:
                index += 1
            if byte == 9 or byte in range(32, 128) and byte not in (60, 62, 64):
                byte = cls._chunk(byte, index)
            yield byte

    @classmethod
    def _encoded(cls, data):
        for i, byte in enumerate(data):
            try:
                sequence = cls._ENCODER[byte]
            except KeyError:
                yield byte
            else:
                offset = cls._OFFSETS[i % 0x40]
                yield sequence[offset]

    def reverse(self, data):
        return bytearray(self._escape(self._encoded(data)))

    def process(self, data):
        if self.args.marker:
            match = formats.wshenc.search(data)
            if not match:
                raise ValueError(&#39;Encoded script marker was not found.&#39;)
            data = match[0][12:-12]
        return bytearray(self._decoded(self._unescape(data)))

    @classmethod
    def handles(cls, data):
        return is_likely_vbe(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xchacha"><code class="flex name class">
<span>class <span class="ident">xchacha</span></span>
<span>(</span><span>key, stateful=False, discard=0, nonce=b'REFINERY', magic=b'', offset=0, rounds=20)</span>
</code></dt>
<dd>
<section class="desc"><p>XChaCha encryption and decryption. The nonce must be 24 bytes long.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/chacha.py#L82-L98" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xchacha(LatinCipherUnit):
    &#34;&#34;&#34;
    XChaCha encryption and decryption. The nonce must be 24 bytes long.
    &#34;&#34;&#34;
    def keystream(self) -&gt; Iterable[int]:
        kdp, kdn, nonce = struct.unpack(&#39;&lt;Q8s8s&#39;, self.args.nonce)
        yield from LatinX(
            ChaChaCipher,
            (0, 1, 2, 3, 12, 13, 14, 15),
            self.args.key,
            kdn,
            kdp,
            nonce,
            self.args.magic,
            self.args.rounds,
            self.args.offset,
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.xfcc"><code class="flex name class">
<span>class <span class="ident">xfcc</span></span>
<span>(</span><span>variable='count', relative=False)</span>
</code></dt>
<dd>
<section class="desc"><p>The cross frame chunk count unit! It computes the number of times a chunk occurs across several frames
of input. It consumes all frames at its current level of the frame tree and counts the number of times
each item occurs in each of them. It converts a frame tree of depth 2 into a new frame tree of depth 2
where the parent of every leaf has this leaf as its only child. The leaves of this tree have been
enriched with a meta variable containing the number of times the corresponding chunk has occurred in
the input frame tree. The variable that stores this information is scoped at the first layer of this
subtree, which means that a frame can be closed once after invocation of xfcc and the variable remains
accessible. This unit can be used to compute set intersections across frames as follows:</p>
<pre><code>(1) [| (2) [| dedup | xfcc -r t ]| iff t==1 | (3) ]
</code></pre>
<p>A sequence of chunks is emitted at (1), each of which has chunks extracted at (2). It is then important
to use dedup before calling xfcc, since xfcc performs an absolute count. The frame at (3) contains the
intersection of all datasets that were extracted at (2).</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/meta/xfcc.py#L10-L77" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xfcc(Unit):
    &#34;&#34;&#34;
    The cross frame chunk count unit! It computes the number of times a chunk occurs across several frames
    of input. It consumes all frames at its current level of the frame tree and counts the number of times
    each item occurs in each of them. It converts a frame tree of depth 2 into a new frame tree of depth 2
    where the parent of every leaf has this leaf as its only child. The leaves of this tree have been
    enriched with a meta variable containing the number of times the corresponding chunk has occurred in
    the input frame tree. The variable that stores this information is scoped at the first layer of this
    subtree, which means that a frame can be closed once after invocation of xfcc and the variable remains
    accessible. This unit can be used to compute set intersections across frames as follows:

        (1) [| (2) [| dedup | xfcc -r t ]| iff t==1 | (3) ]

    A sequence of chunks is emitted at (1), each of which has chunks extracted at (2). It is then important
    to use dedup before calling xfcc, since xfcc performs an absolute count. The frame at (3) contains the
    intersection of all datasets that were extracted at (2).
    &#34;&#34;&#34;
    def __init__(
        self,
        variable: Param[str, Arg(help=&#39;The variable which is used as the accumulator&#39;)] = &#39;count&#39;,
        relative: Param[bool, Arg.Switch(&#39;-r&#39;, help=&#39;Normalize the accumulator to a number between 0 and 1.&#39;)] = False
    ):
        super().__init__(variable=variable, relative=relative)
        self._trunk = None
        self._store: dict[Chunk, int] = defaultdict(int)

    def finish(self):
        vn = self.args.variable
        rc = self.args.relative
        if rc and self._store:
            maximum = max(self._store.values())
        for index, (chunk, count) in enumerate(self._store.items()):
            if rc:
                count /= maximum
            chunk.path[-2] = 0
            chunk.path[-1] = index
            chunk.meta[vn] = count
            chunk.meta.set_scope(vn, chunk.scope - 1)
            yield chunk
        self._store.clear()

    def _getcount(self, chunk):
        try:
            count = int(chunk.meta[self.args.variable])
        except (AttributeError, KeyError, TypeError):
            return 1
        else:
            return count

    def filter(self, chunks: Iterable[Chunk]):
        it = iter(chunks)
        try:
            head = next(it)
        except StopIteration:
            return
        if len(head.path) &lt; 2:
            self.log_warn(F&#39;the current frame is nested {len(head.path)} layers deep, at least two layers are required.&#39;)
            yield head
            yield from it
            return
        trunk = head.path[:-2]
        store = self._store
        if trunk != self._trunk:
            yield from self.finish()
            self._trunk = trunk
        store[head] += self._getcount(head)
        for chunk in it:
            store[chunk] += self._getcount(chunk)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xj0"><code class="flex name class">
<span>class <span class="ident">xj0</span></span>
<span>(</span><span>*fmt, all=False, one=False, raw=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts a single field from a JSON document at depth 0. By default, the unit applies a
heuristic to extract remaining fields as metadata: String values are extracted only if
they do not exceed 80 characters in length and do not contain any line breaks.
Floating-point, integer, boolean values, and lists of the latter are also extracted.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/json.py#L55-L130" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xj0(Unit):
    &#34;&#34;&#34;
    Extracts a single field from a JSON document at depth 0. By default, the unit applies a
    heuristic to extract remaining fields as metadata: String values are extracted only if
    they do not exceed 80 characters in length and do not contain any line breaks.
    Floating-point, integer, boolean values, and lists of the latter are also extracted.
    &#34;&#34;&#34;
    def __init__(
        self,
        *fmt: Param[str, Arg.String(help=(
            &#39;Format expression for the output chunk; may use previously extracted JSON items &#39;
            &#39;as format expressions. By default, the input data is returned.&#39;))],
        all: Param[bool, Arg.Switch(&#39;-a&#39;, group=&#39;META&#39;,
            help=&#39;Extract all other fields as metadata regardless of length and type.&#39;)] = False,
        one: Param[bool, Arg.Switch(&#39;-x&#39;, group=&#39;META&#39;,
            help=&#39;Do not extract any other fields as metadata.&#39;)] = False,
        raw: Param[bool, Arg.Switch(&#39;-r&#39;,
            help=&#39;Disable conversion of JSON strings to binary strings in metadata&#39;)] = False,
    ):
        super().__init__(fmt=fmt, one=one, raw=raw, all=all)

    def process(self, data: Chunk):

        def convert(value, iskey=False):
            if self.args.raw:
                return value
            if isinstance(value, (float, int, bool)):
                return value
            if isinstance(value, str):
                return value.encode(self.codec)
            if iskey:
                raise TypeError
            if isinstance(value, dict):
                return {convert(k): convert(v) for k, v in value.items()}
            if isinstance(value, list):
                return [convert(k) for k in value]

        def acceptable(key, value, nested=False, convert=False):
            if not is_valid_variable_name(key):
                self.log_info(F&#39;rejecting item with invalid name {key}&#39;)
                return None
            if isinstance(value, (float, int, bool)):
                return True
            if isinstance(value, dict):
                if not self.args.all:
                    self.log_info(F&#39;rejecting item {key} with dictionary value&#39;)
                    return False
                return True
            if isinstance(value, list):
                if nested:
                    self.log_info(F&#39;rejecting item {key} containing a doubly nested list&#39;)
                    return False
                return all(acceptable(key, t, True) for t in value)
            if isinstance(value, str):
                if not self.args.all:
                    if len(value) not in range(1, 80):
                        self.log_info(F&#39;rejecting string item {key} because {len(value)} exceeds the length limit&#39;)
                        return False
                    if &#39;\n&#39; in value:
                        self.log_info(F&#39;rejecting string item {key} because it contains line breaks&#39;)
                        return False
                return True
            return False

        jdoc: dict = json.loads(data)
        if not isinstance(jdoc, dict):
            raise ValueError(&#39;The input must be a JSON dictionary.&#39;)
        meta = metavars(data)
        args = {k: convert(v) for k, v in jdoc.items() if acceptable(k, v)}
        used = set()
        data[:] = meta.format_bin(&#39; &#39;.join(self.args.fmt), self.codec, [data], args, used)
        for u in used:
            args.pop(u, None)
        if not self.args.one:
            data.meta.update(args)
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.xjl"><code class="flex name class">
<span>class <span class="ident">xjl</span></span>
</code></dt>
<dd>
<section class="desc"><p>Returns all JSON elements from a JSON iterable as individual outputs. When reversed, the unit
collects all chunks in the frame and wraps them as a JSON list.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/json.py#L133-L166" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xjl(Unit):
    &#34;&#34;&#34;
    Returns all JSON elements from a JSON iterable as individual outputs. When reversed, the unit
    collects all chunks in the frame and wraps them as a JSON list.
    &#34;&#34;&#34;

    def process(self, data):
        try:
            doc: list | dict = json.loads(data)
        except Exception:
            from refinery.units.pattern.carve_json import carve_json
            doc = data | carve_json | json.loads
        try:
            it = doc.values()
        except AttributeError:
            it = doc
        for item in it:
            yield json.dumps(item, indent=4).encode(self.codec)

    def reverse(self, data):
        return json.dumps(data.temp).encode(self.codec)

    def filter(self, chunks: Iterable[Chunk]):
        if not self.args.reverse:
            yield from chunks

        from refinery.lib.tools import begin

        if it := begin(chunks):
            head, rest = it
            collected = [head.decode(self.codec)]
            collected.extend(chunk.decode(self.codec) for chunk in rest)
            head.temp = collected
            yield head</code></pre>
</details>
</dd>
<dt id="refinery.shell.xkey"><code class="flex name class">
<span>class <span class="ident">xkey</span></span>
<span>(</span><span>range=slice(1, 32, None), plaintext=b'', searchpos=slice(0, None, None), alph=False, crib=False, freq=False)</span>
</code></dt>
<dd>
<section class="desc"><p>The unit expects encrypted input which was encrypted byte-wise with a polyalphabetic key. For
both bit-wise and byte-wise addition, it can attempt do determine this key by three methods:</p>
<ol>
<li>Known plaintext cribs: The unit contains a library of file signatures that are expected to
occur at specific offsets. It uses these to attempt a known-plaintext attack against the
input. If a key is found that is at most half the size of such a crib, it is returned.</li>
<li>Known alphabets: For each given key length, the input is split into slices that would have
been encrypted with a single byte for keys of that length. Each such slice undergoes a
character frequency analysis. If the histogram indicates that an alphabet of a small size
was used (i.e. base64), then the unit attempts to determine the key based on this.</li>
<li>Known high frequency glyph: Works if the plaintext contains one letter that occurs with
very high frequency, i.e. zero padding in PE or ELF files, and the space character in text.
Based on this assumption, the unit computes the most likely key. This method will work best
on uncompressed files that were encrypted with a short key.</li>
</ol>
<p>When no option is set, the unit uses all the above methods by default. When at least one of
the methods is selected, it will attempt only selected methods. When a custom plaintext is given,
the other methods are disabled by default.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/misc/xkey.py#L46-L363" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xkey(Unit):
    &#34;&#34;&#34;
    The unit expects encrypted input which was encrypted byte-wise with a polyalphabetic key. For
    both bit-wise and byte-wise addition, it can attempt do determine this key by three methods:

    1. Known plaintext cribs: The unit contains a library of file signatures that are expected to
       occur at specific offsets. It uses these to attempt a known-plaintext attack against the
       input. If a key is found that is at most half the size of such a crib, it is returned.
    2. Known alphabets: For each given key length, the input is split into slices that would have
       been encrypted with a single byte for keys of that length. Each such slice undergoes a
       character frequency analysis. If the histogram indicates that an alphabet of a small size
       was used (i.e. base64), then the unit attempts to determine the key based on this.
    3. Known high frequency glyph: Works if the plaintext contains one letter that occurs with
       very high frequency, i.e. zero padding in PE or ELF files, and the space character in text.
       Based on this assumption, the unit computes the most likely key. This method will work best
       on uncompressed files that were encrypted with a short key.

    When no option is set, the unit uses all the above methods by default. When at least one of
    the methods is selected, it will attempt only selected methods. When a custom plaintext is given,
    the other methods are disabled by default.
    &#34;&#34;&#34;

    _CRIBS: dict[range, dict[str, bytes | tuple[bytes | tuple[bytes, ...], ...]]] = {
        range(0, 64, 4): {
            &#39;ZIP&#39;           : (B&#39;PK\x03\x04&#39;, (B&#39;\x14\x00&#39;, B&#39;\x0A\x00&#39;), (B&#39;\x08\x00&#39;, B&#39;\x00\x00&#39;)),
            &#39;RAR&#39;           : (B&#39;Rar!\x1A\x07&#39;, (B&#39;\x01\x00&#39;, B&#39;\x00&#39;)),
            &#39;ZPAQ&#39;          : (B&#39;\x37\x6B\x53\x74\xA0\x31\x83\xD3\x8C\xB2\x28\xB0\xD3\x7A\x50\x51&#39;),
            &#39;ZSTD&#39;          : (B&#39;\x28\xB5\x2F\xFD&#39;),
            &#39;ZZip&#39;          : (B&#39;7z\xBC\xAF\x27\x1C&#39;, (B&#39;\x00\x02&#39;, B&#39;\x00\x03&#39;, B&#39;\x00\x04&#39;)),
            &#39;APLib&#39;         : (B&#39;AP32\x18\0\0\0&#39;),
            &#39;BZip&#39;          : (B&#39;BZh&#39;),
            &#39;LNK&#39;           : (B&#39;L\0\0\0\01\x14\02\0\0\0\0\0\xC0\0\0\0\0\0\0F&#39;, (B&#39;&#39;, B&#39;\x9B&#39;)),
            &#39;DDS&#39;           : (B&#39;\x00\x00\x00\x01Bud1&#39;),
            &#39;ELF&#39;           : (B&#39;\x7FELF&#39;),
            &#39;JavaClass&#39;     : (B&#39;\xCA\xFE\xBA\xBE&#39;),
            &#39;LZIP&#39;          : (B&#39;LZIP&#39;),
            &#39;SZDD&#39;          : (B&#39;SZDD\x88\xF0\x27\x33&#39;),
            &#39;LZMA&#39;          : (B&#39;\x5D\x00\x00\x00&#39;),
            &#39;LZMA/XZ&#39;       : (B&#39;\xFD7zXZ&#39;),
            &#39;LZO&#39;           : (B&#39;\x89\x4c\x5a\x4f\x00\x0d\x0a\x1a\x0a&#39;),
            &#39;MachO/BE&#39;      : (B&#39;\xCA\xFE\xBA\xBE&#39;),
            &#39;MachO/LE&#39;      : (B&#39;\xBE\xBA\xFE\xCA&#39;),
            &#39;MSCF&#39;          : (B&#39;\x0A\x51\xE5\xC0&#39;),
            &#39;OleDocument&#39;   : (B&#39;\xD0\xCF\x11\xE0&#39;, (B&#39;&#39;, B&#39;\xA1\xB1\x1A\xE1&#39;), (B&#39;&#39;, B&#39;\0\0\0\0\0\0\0\0&#39;)),
            &#39;PdfDocument&#39;   : (B&#39;%PDF-&#39;, _S(B&#39;12&#39;), (B&#39;.&#39;), _S(B&#39;0123456789&#39;), _S(B&#39;\r\n&#39;)),
            &#39;SQLite&#39;        : (B&#39;SQLite format 3\0&#39;),
            &#39;GIF&#39;           : (B&#39;GIF87a&#39;, B&#39;GIF89a&#39;),
            &#39;PNG&#39;           : (B&#39;\x89PNG\r\n\x1A\n&#39;),
            &#39;DEX&#39;           : (B&#39;dex\n035\0&#39;),
            &#39;JPG&#39;           : (B&#39;\xFF\xD8\xFF&#39;, _S(B&#39;\xE0\xE1\xEE&#39;), (B&#39;\x00\x10\x4A\x46\x49\x46\x00\x01&#39;, B&#39;&#39;)),
            &#39;OneNote&#39;       : (B&#39;\xE4\x52\x5C\x7B\x8C\xD8\xA7\x4D\xAE\xB1\x53\x78\xD0\x29\x96\xD3&#39;),
            &#39;A3xScript&#39;     : (B&#39;\xA3\x48\x4B\xBE\x98\x6C\x4A\xA9\x99\x4C\x53\x0A\x86\xD6\x48\x7DAU3!EA0&#39;, _S(B&#39;56&#39;)),
            &#39;RTFDocument&#39;   : (B&#39;{\\rtf1&#39;, (B&#39;\\adeflang&#39;, B&#39;\\ansi&#39;, B&#39;&#39;)),
            &#39;CallToPop&#39;     : (B&#39;\xE8\0\0\0\0&#39;, (
                               B&#39;\x41\x58&#39;, B&#39;\x41\x59&#39;, B&#39;\x41\x5A&#39;, B&#39;\x41\x5B&#39;,
                                   B&#39;\x58&#39;,     B&#39;\x59&#39;,     B&#39;\x5A&#39;,     B&#39;\x5B&#39;,   # noqa
                                   B&#39;\x5C&#39;,     B&#39;\x5D&#39;,     B&#39;\x5E&#39;,     B&#39;\x5F&#39;,   # noqa
                               )),
            &#39;Cert&#39;          : (B&#39;-----BEGIN CERTIFICATE-----&#39;),
            &#39;PrivateKey&#39;    : (B&#39;-----BEGIN PRIVATE KEY-----&#39;),
            &#39;PrivateKeyDSA&#39; : (B&#39;-----BEGIN DSA PRIVATE KEY-----&#39;),
            &#39;PrivateKeyRSA&#39; : (B&#39;-----BEGIN RSA PRIVATE KEY-----&#39;),
            &#39;PrivateKeySSH&#39; : (B&#39;-----BEGIN OPENSSH PRIVATE KEY-----&#39;),
            &#39;PEM&#39;           : (B&#39;-----BEGIN &#39;),
            &#39;PuTTY-Key&#39;     : (B&#39;PuTTY-User-Key-File-&#39;, (B&#39;2:&#39;, B&#39;3:&#39;)),
            &#39;MsAccess&#39;      : (B&#39;\0\01\0\0Standard &#39;, (B&#39;ACE&#39;, B&#39;Jet&#39;), B&#39; DB&#39;),
        },
        range(0x10, 0x11): {
            &#39;ASAR&#39;          : (B&#39;{&#34;files&#34;:{&#34;&#39;),
        },
        range(0x10): {
            &#39;DocTypeLower&#39;  : (B&#39;&lt;!doctype\x20&#39;, (B&#39;&#39;, B&#39;html&#39;)),
            &#39;DocTypeUpper&#39;  : (B&#39;&lt;!DOCTYPE\x20&#39;, (B&#39;&#39;, B&#39;HTML&#39;)),
            &#39;HTMLLower&#39;     : (B&#39;&lt;html&gt;&#39;),
            &#39;HTMLUpper&#39;     : (B&#39;&lt;HTML&gt;&#39;),
            &#39;XML&#39;           : (B&#39;&lt;?xml version=&#34;&#39;),
            &#39;Ace&#39;           : (B&#39;**ACE**&#39;),
        },
        range(0x36, 0x41): {
            &#39;PEStub&#39;: (
                B&#39;\0\x0E\x1F\xBA\x0E\x00\xB4\x09\xCD\x21\xB8\x01\x4C\xCD\x21&#39;
                B&#39;This program cannot be run in DOS mode.\r&#39;
            ),
            &#39;PEDelphiStub&#39;: (
                B&#39;\0\xBA\x10\x00\x0E\x1F\xB4\x09\xCD\x21\xB8\x01\x4C\xCD\x21\x90\x90&#39;
                B&#39;This program must be run under Win&#39;, (B&#39;32&#39;, B&#39;64&#39;), B&#39;\x0D\x0A&#39;
            ),
        },
        range(0x48, 0x60): {
            &#39;PEStubMsg&#39;      : (B&#39;This program cannot be run in DOS mode.\r&#39;),
            &#39;PEDelphiStubMsg&#39;: (B&#39;This program must be run under Win&#39;, (B&#39;32&#39;, B&#39;64&#39;), B&#39;\x0D\x0A&#39;),
        },
        range(0xD0, 0xD1): {
            &#39;Tar&#39;           : (B&#39;\x00&#39; * 0x30 + B&#39;ustar&#39;, (B&#39;\x20\x20\x00&#39;, B&#39;\x00\x30\x30&#39;)),
        },
    }

    _ENC_ALPHABETS = [
        B&#39;0123456789,&#39;,
        B&#39;0123456789;&#39;,
        B&#39;0123456789ABCDEF&#39;,
        B&#39;0123456789abcdef&#39;,
        B&#39;ABCDEFGHIJKLMNOPQRSTUVWXYZ234567&#39;,
        B&#39;abcdefghijklmnopqrstuvwxyz234567&#39;,
        B&#39;0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ+/&#39;,
        B&#39;0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ-_&#39;,
    ]

    _WSH_ALPHABET = bytes(set(range(0x20, 0x80)) - {0x3C, 0x3E} | {0x09})

    class _rt(enum.IntEnum):
        crib = 0
        alph = 1
        freq = 2

    class _result(NamedTuple):
        key: bytes
        how: xkey._rt
        xor: bool | None = None
        score: float = 0.0

    def __init__(
        self,
        range: Param[slice, Arg.Bounds(help=(
            &#39;range of length values to try in Python slice syntax, the default is {default}.&#39;
        ))] = slice(1, 32),
        plaintext: Param[buf, Arg.Binary(&#39;-p&#39;, help=(
            &#39;Provide a buffer of known plaintext. Without a search position, this can slow &#39;
            &#39;down the key search significantly.&#39;
        ))] = B&#39;&#39;,
        searchpos: Param[slice, Arg.Bounds(&#39;-s&#39;, metavar=&#39;S:E&#39;, help=(
            &#39;Only used when a known plaintext buffer is provided; In this case it narrows the &#39;
            &#39;search range for the offset of that data to between S and E.&#39;
        ))] = slice(0, None),
        alph: Param[bool, Arg.Switch(&#39;-a&#39;,
            help=&#39;Enable search for keys via known encoder alphabets.&#39;)] = False,
        crib: Param[bool, Arg.Switch(&#39;-c&#39;,
            help=&#39;Enable search for keys via known plaintext cribs.&#39;)] = False,
        freq: Param[bool, Arg.Switch(&#39;-f&#39;,
            help=&#39;Enable search for keys via frequency analysis.&#39;)] = False,
    ):
        if not any((alph, crib, freq)) and not plaintext:
            alph = crib = freq = True
        super().__init__(
            range=range,
            plaintext=plaintext,
            searchpos=searchpos,
            alph=alph,
            crib=crib,
            freq=freq,
        )

    def process(self, data: bytearray):
        for result in self._attack(data):
            out = result.key
            if how := result.how:
                out = self.labelled(out, method=how)
            return out

    def _attack(self, data: bytearray):
        bounds: slice = self.args.range
        view = memoryview(data)
        length = len(view)

        if length &lt;= 1:
            return

        if length &gt;= 0x100:
            view = view[:-4]

        start = bounds.start or 1
        stop = min(bounds.stop or length, length)

        if (step := bounds.step) is None:
            step = 1
        elif bounds.start is None:
            start *= step

        self.log_debug(
            F&#39;received input range [{bounds.start}:{bounds.stop}:{bounds.step}], &#39;
            F&#39;using [{start}:{stop}:{step}]&#39;)

        criblist: list[tuple[range, dict[str, bytes | tuple[bytes | tuple[bytes, ...], ...]]]] = []

        if p := self.args.plaintext:
            pos: slice = self.args.searchpos
            end = len(data) - len(p) if pos.stop is None else pos.stop
            criblist.append((range(pos.start or 0, end + 1), {&#39;Plaintext&#39;: (p,)}))
        if self.args.crib:
            for r, byname in self._CRIBS.items():
                compiled = {
                    name: tuple(_generate_cribs(cribs))
                    for name, cribs in byname.items()
                }
                criblist.append((r, compiled))

        if self.args.alph:
            alphabets: dict[int, list[bytes]] | None = {}
            for alphabet in self._ENC_ALPHABETS:
                for suffix in (B&#39;&#39;, B&#39;\x20&#39;, B&#39;\x0A&#39;, B&#39;\x20\x0A&#39;):
                    a = alphabet + suffix
                    alphabets.setdefault(len(a), []).append(a)
            alphabets[len(self._WSH_ALPHABET)] = [self._WSH_ALPHABET]
        else:
            alphabets = None

        for xor in (True, False):
            if key := self._process_crib(view, xor, criblist):
                yield self._result(key, self._rt.crib, xor)

        hist = {}
        freq = []

        for xor in (True, False):
            result = self._process_freq(view, (start, stop, step), alphabets, xor, hist)
            if result is None or not result.key:
                continue
            if result.how == self._rt.freq:
                freq.append(result)
                continue
            yield result

        yield from freq

    def _process_crib(
        self,
        view: memoryview,
        xor: bool,
        criblist: list[tuple[range, dict[str, list[bytes]]]]
    ):
        for offsets, cribs_by_type in criblist:
            for name, cribs in cribs_by_type.items():
                for crib in cribs:
                    cn = len(crib)
                    for offset in offsets:
                        test = view[offset:offset + cn]
                        if len(test) != cn:
                            continue
                        key = strxor(test, crib) if xor else bytes(
                            a - b &amp; 0xFF for a, b in zip(test, crib))
                        if key := _cyclic_base(key):
                            self.log_info(F&#39;found key via crib {name}:&#39;, crib, clip=True)
                            shift = -offset % len(key)
                            return key[shift:] + key[:shift]

    def _process_freq(
        self,
        view: memoryview,
        bounds: tuple[int, int, int],
        alphabets: dict[int, list[bytes]] | None,
        xor: bool,
        hist: dict[int, tuple[list[bytes], list[Counter]]],
    ):
        n = len(view)
        start, stop, step = bounds
        score = 0
        guess = None
        first = not hist

        for keylen in range(start, stop + 1, step):
            try:
                cached = hist[keylen]
            except KeyError:
                patches = [view[j::keylen] for j in range(keylen)]
                histograms = [Counter(p) for p in patches]
                hist[keylen] = patches, histograms
            else:
                patches, histograms = cached

            if alphabets is not None:
                hlc = Counter(len(h) for h in histograms)
                base, coverage = hlc.most_common(1)[0]

                if coverage * 2 &gt; keylen and base in alphabets:
                    self.log_debug(F&#39;solving for potential plaintext alphabet of size 0x{base:02X} at {keylen}&#39;)
                    keys: dict[bytes, bytes] = {}
                    for alphabet in alphabets[base]:
                        key = bytearray(keylen)
                        for k, patch in enumerate(patches):
                            keybyte = set(range(0x100))
                            for c in patch:
                                keybyte &amp;= (
                                    {c ^ p &amp; 0xFF for p in alphabet}
                                ) if xor else (
                                    {c - p &amp; 0xFF for p in alphabet}
                                )
                                if len(keybyte) == 1:
                                    key[k] = next(iter(keybyte))
                                    break
                            else:
                                key = None
                                break
                        if key is not None:
                            keys[alphabet] = key
                    if len(keys) == 1:
                        self.log_debug(F&#39;discovered plaintext alphabet of size 0x{base:02X} at {keylen}&#39;)
                        alphabet, key = keys.popitem()
                        return self._result(bytes(key), self._rt.alph, xor)

            if not first or not self.args.freq:
                continue

            _guess = [h.most_common(1)[0] for h in histograms]
            _score = sum(letter_count for _, letter_count in _guess) / n
            # This scaling accounts for the smaller probability of larger keys. No proper statistical analysis has been
            # conducted to derive it; there might be plenty of room for improvement here.
            _score = _score * ((n - keylen) / (n - 1)) ** keylen

            logmsg = F&#39;[{{}}] score {_score * 100:05.2f}% for key length {keylen}&#39;
            if _score &gt; score:
                self.log_info(logmsg.format(&#39;+&#39;))
                score = _score
                guess = bytes(value for value, _ in _guess)
            else:
                self.log_debug(logmsg.format(&#39; &#39;))

        if guess is not None:
            return self._result(guess, self._rt.freq, score=score * 100)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xlmdeobf"><code class="flex name class">
<span>class <span class="ident">xlmdeobf</span></span>
<span>(</span><span>extract_only=False, sort_formulas=False, with_ms_excel=False, day=-1, output_formula_format='CELL:[[CELL-ADDR]], [[STATUS]], [[INT-FORMULA]]', extract_formula_format='CELL:[[CELL-ADDR]], [[CELL-FORMULA]], [[CELL-VALUE]]', no_indent=False, start_point='', password='', output_level=0, timeout=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Wrapper around XLMMacroDeobfuscator to decode obfuscated Excel v4.0 (XLM) macros.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/office/xlmdeobf.py#L9-L101" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xlmdeobf(Unit):
    &#34;&#34;&#34;
    Wrapper around XLMMacroDeobfuscator to decode obfuscated Excel v4.0 (XLM) macros.
    &#34;&#34;&#34;

    def __init__(
        self,
        extract_only: Param[bool, Arg.Switch(
            &#39;-x&#39;, help=&#39;Only extract cells without any emulation.&#39;
        )] = False,
        sort_formulas: Param[bool, Arg.Switch(
            &#39;-s&#39;, &#39;--sort-formulas&#39;,
            help=&#39;Sort extracted formulas based on their cell address (implies -x).&#39;,
        )] = False,
        with_ms_excel: Param[bool, Arg.Switch(
            &#39;-X&#39;, &#39;--with-ms-excel&#39;, help=&#39;Use MS Excel to process XLS files.&#39;
        )] = False,
        day: Param[int, Arg.Number(
            &#39;-d&#39;,
            &#39;--day&#39;,
            help=&#39;Specify the day of month&#39;,
        )] = -1,
        output_formula_format: Param[str, Arg.String(
            &#39;-O&#39;, &#39;--output-format&#39;,
            metavar=&#39;FMT&#39;,
            help=&#39;Specify the format for output formulas (using [[CELL-ADDR]], [[INT-FORMULA]], and [[STATUS]])&#39;,
        )] = &#39;CELL:[[CELL-ADDR]], [[STATUS]], [[INT-FORMULA]]&#39;,
        extract_formula_format: Param[str, Arg.String(
            &#39;-E&#39;, &#39;--extract-format&#39;,
            metavar=&#39;FMT&#39;,
            help=&#39;Specify the format for extracted formulas (using [[CELL-ADDR]], [[CELL-FORMULA]], and [[CELL-VALUE]])&#39;,
        )] = &#39;CELL:[[CELL-ADDR]], [[CELL-FORMULA]], [[CELL-VALUE]]&#39;,
        no_indent: Param[bool, Arg.Switch(
            &#39;-I&#39;, &#39;--no-indent&#39;,
            help=&#39;Do not show indent before formulas&#39;,
        )] = False,
        start_point: Param[str, Arg.String(
            &#39;-c&#39;, &#39;--start-point&#39;,
            help=&#39;Start interpretation from a specific cell address&#39;,
            metavar=&#39;CELL&#39;,
        )] = &#39;&#39;,
        password: Param[str, Arg.String(
            &#39;-p&#39;,
            &#39;--password&#39;,
            help=&#39;Password to decrypt the protected document&#39;,
        )] = &#39;&#39;,
        output_level: Param[int, Arg.Number(
            &#39;-o&#39;,
            &#39;--output-level&#39;,
            help=(
                &#39;Set the level of details to be shown (0:all commands, 1: commands no jump 2:important &#39;
                &#39;commands 3:strings in important commands).&#39;
            ),
        )] = 0,
        timeout: Param[int, Arg.Number(
            &#39;-t&#39;,
            &#39;--timeout&#39;,
            help=&#39;Stop emulation after N seconds (0: not interruption N&gt;0: stop emulation after N seconds)&#39;,
        )] = 0,
    ):
        extract_only = sort_formulas or extract_only
        self.superinit(super(), **vars())

    @Unit.Requires(&#39;XLMMacroDeobfuscator&#39;, [&#39;formats&#39;, &#39;office&#39;])
    def _process_file():
        with NoLogging(NoLogging.Mode.ALL):
            from XLMMacroDeobfuscator.configs import settings
            settings.SILENT = True
            from XLMMacroDeobfuscator.deobfuscator import process_file
            return process_file

    def process(self, data: bytearray):
        with VirtualFileSystem() as vfs, NoLogging(NoLogging.Mode.ALL):
            result = self._process_file(
                file=vfs.new(data),
                noninteractive=True,
                return_deobfuscated=True,
                extract_only=self.args.extract_only,
                silent=True,
                sort_formulas=self.args.sort_formulas,
                defined_names=False,
                with_ms_excel=self.args.with_ms_excel,
                start_with_shell=False,
                day=self.args.day,
                output_formula_format=self.args.output_formula_format,
                extract_formula_format=self.args.extract_formula_format,
                no_indent=self.args.no_indent,
                start_point=self.args.start_point,
                password=self.args.password,
                output_level=self.args.output_level,
                timeout=self.args.timeout,
            )
        return &#39;\n&#39;.join(result).encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xlxtr"><code class="flex name class">
<span>class <span class="ident">xlxtr</span></span>
<span>(</span><span>*references)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract data from Microsoft Excel documents, both Legacy and new XML type documents. A sheet
reference is of the form <code>B1</code> or <code>1.2</code>, both specifying the first cell of the second column.
A cell range can be specified as <code>B1:C12</code>, or <code>1.2:C12</code>, or <code>1.2:12.3</code>. Finally, the unit will
always refer to the first sheet in the document and to change this, specify the sheet name or
index separated by a hashtag, i.e. <code>sheet#B1:C12</code> or <code>1#B1:C12</code>. Note that indices are
1-based. To get all elements of one sheet, use <code>sheet#</code>. The unit If parsing a sheet reference
fails, the script will assume that the given reference specifies a sheet.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/office/xlxtr.py#L278-L332" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xlxtr(_ExcelUnit):
    &#34;&#34;&#34;
    Extract data from Microsoft Excel documents, both Legacy and new XML type documents. A sheet
    reference is of the form `B1` or `1.2`, both specifying the first cell of the second column.
    A cell range can be specified as `B1:C12`, or `1.2:C12`, or `1.2:12.3`. Finally, the unit will
    always refer to the first sheet in the document and to change this, specify the sheet name or
    index separated by a hashtag, i.e. `sheet#B1:C12` or `1#B1:C12`. Note that indices are
    1-based. To get all elements of one sheet, use `sheet#`. The unit If parsing a sheet reference
    fails, the script will assume that the given reference specifies a sheet.
    &#34;&#34;&#34;
    def __init__(
        self,
        *references: Param[SheetReference, Arg(
            metavar=&#39;reference&#39;,
            type=SheetReference,
            help=(
                &#39;A sheet reference to be extracted. &#39;
                &#39;If no sheet references are given, the unit lists all sheet names.&#39;
            )
        )]
    ):
        if not references:
            references = SheetReference(&#39;*&#39;),
        super().__init__(references=references)

    def process(self, data):
        try:
            wb = Workbook(data, self)
        except ImportError:
            raise
        except Exception as E:
            raise ValueError(&#39;Input not recognized as Excel document.&#39;) from E
        for ref in self.args.references:
            ref: SheetReference
            for k, name in enumerate(wb.sheets()):
                if not ref.match(k, name):
                    continue
                try:
                    data = wb.get_sheet_data(name)
                except Exception as error:
                    self.log_info(F&#39;error reading sheet {name}:&#39;, error)
                    continue
                for r, row in enumerate(data, 1):
                    for c, value in enumerate(row, 1):
                        if (r, c) not in ref:
                            continue
                        if value is None:
                            continue
                        yield self.labelled(
                            str(value).encode(self.codec),
                            row=r,
                            col=c,
                            ref=_rc2ref(r, c),
                            sheet=name
                        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.xor"><code class="flex name class">
<span>class <span class="ident">xor</span></span>
<span>(</span><span>*argument, bigendian=False, blocksize=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Form the exclusive or of the input data with the given argument.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/blockwise/xor.py#L8-L34" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xor(BinaryOperationWithAutoBlockAdjustment):
    &#34;&#34;&#34;
    Form the exclusive or of the input data with the given argument.
    &#34;&#34;&#34;
    @staticmethod
    def operate(a, b): return a ^ b
    @staticmethod
    def inplace(a, b): a ^= b

    def _fastblock_fallback(self, data):
        from Cryptodome.Util import strxor
        size = len(data)
        it, masked = self._argument_parse_hook(self.args.argument[0])
        arg0 = self._infinitize_argument(len(data), it, masked)
        take = len(data) // self.blocksize + 1
        argb = self.unchunk(islice(arg0, take))
        del argb[size:]
        return strxor.strxor(data, argb)

    def _fastblock(self, data):
        try:
            return super()._fastblock(data)
        except FastBlockError as E:
            try:
                return self._fastblock_fallback(data)
            except Exception:
                raise E</code></pre>
</details>
</dd>
<dt id="refinery.shell.xsalsa"><code class="flex name class">
<span>class <span class="ident">xsalsa</span></span>
<span>(</span><span>key, stateful=False, discard=0, nonce=b'REFINERY', magic=b'', offset=0, rounds=20)</span>
</code></dt>
<dd>
<section class="desc"><p>XSalsa encryption and decryption. The nonce must be 24 bytes long.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/salsa.py#L178-L194" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xsalsa(LatinCipherUnit):
    &#34;&#34;&#34;
    XSalsa encryption and decryption. The nonce must be 24 bytes long.
    &#34;&#34;&#34;
    def keystream(self) -&gt; Iterable[int]:
        kdn, kdp, nonce = struct.unpack(&#39;&lt;8sQ8s&#39;, self.args.nonce)
        yield from LatinX(
            SalsaCipher,
            (0, 5, 10, 15, 6, 7, 8, 9),
            self.args.key,
            kdn,
            kdp,
            nonce,
            self.args.magic,
            self.args.rounds,
            self.args.offset,
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.xt"><code class="flex name class">
<span>class <span class="ident">xt</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>This unit generically extracts files from archives. It attempts to identify the archive format
and use the corresponding specific extractor from among the ones implemented in refinery.</p>
<p>This unit is a path extractor which extracts data from a hierarchical structure. Each extracted
item is emitted as a separate chunk and has attached to it a meta variable that contains its
path within the source structure. The positional arguments to the command are patterns that can
be used to filter the extracted items by their path. To view only the paths of all chunks, use
the listing switch:</p>
<pre><code>emit something | xt --list
</code></pre>
<p>Otherwise, extracted items are written to the standard output port and usually require a frame
to properly process. In order to dump all extracted data to disk, the following pipeline can be
used:</p>
<pre><code>emit something | xt [| dump {path} ]
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/xt.py#L7-L161" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xt(ArchiveUnit, docs=&#39;{0}{p}{PathExtractorUnit}&#39;):
    &#34;&#34;&#34;
    This unit generically extracts files from archives. It attempts to identify the archive format
    and use the corresponding specific extractor from among the ones implemented in refinery.
    &#34;&#34;&#34;
    @classmethod
    def handles(cls, data) -&gt; bool | None:
        out = False
        for engine in cls.handlers():
            engine_verdict = engine.handles(data)
            if engine_verdict is True:
                return True
            if engine_verdict is None:
                out = None
        return out

    @staticmethod
    def handlers():
        &#34;&#34;&#34;
        Returns all archive handlers supported by the unit.
        &#34;&#34;&#34;
        # units that check fixed offsets
        from refinery.units.formats.archive.xtsql import xtsql        ; yield xtsql     # noqa
        from refinery.units.formats.archive.xttar import xttar        ; yield xttar     # noqa
        from refinery.units.formats.archive.xtiso import xtiso        ; yield xtiso     # noqa
        from refinery.units.formats.archive.xtchm import xtchm        ; yield xtchm     # noqa
        from refinery.units.formats.archive.xtcab import xtcab        ; yield xtcab     # noqa
        from refinery.units.formats.archive.xtace import xtace        ; yield xtace     # noqa
        from refinery.units.formats.archive.xtmacho import xtmacho    ; yield xtmacho   # noqa
        from refinery.units.formats.archive.xtasar import xtasar      ; yield xtasar    # noqa
        from refinery.units.formats.office.xtrtf import xtrtf         ; yield xtrtf     # noqa
        from refinery.units.formats.pdf import xtpdf                  ; yield xtpdf     # noqa
        from refinery.units.formats.winreg import winreg              ; yield winreg    # noqa
        from refinery.units.formats.archive.xtgz import xtgz          ; yield xtgz      # noqa
        from refinery.units.formats.archive.xtcpio import xtcpio      ; yield xtcpio    # noqa
        # units that use fixed offsets + file magic
        from refinery.units.formats.msi import xtmsi                  ; yield xtmsi     # noqa
        # units that search for markers
        from refinery.units.formats.archive.xt7z import xt7z          ; yield xt7z      # noqa
        from refinery.units.formats.archive.xtzip import xtzip        ; yield xtzip     # noqa
        from refinery.units.formats.pe.dotnet.dnsfx import dnsfx      ; yield dnsfx     # noqa
        from refinery.units.formats.archive.xtinno import xtinno      ; yield xtinno    # noqa
        from refinery.units.formats.archive.xtiss import xtiss        ; yield xtiss     # noqa
        from refinery.units.formats.archive.xtnsis import xtnsis      ; yield xtnsis    # noqa
        from refinery.units.formats.archive.xtpyi import xtpyi        ; yield xtpyi     # noqa
        from refinery.units.formats.a3x import a3x                    ; yield a3x       # noqa
        from refinery.units.formats.archive.xtnode import xtnode      ; yield xtnode    # noqa
        from refinery.units.formats.archive.xtzpaq import xtzpaq      ; yield xtzpaq    # noqa
        from refinery.units.formats.email import xtmail               ; yield xtmail    # noqa
        from refinery.units.formats.office.xtone import xtone         ; yield xtone     # noqa
        from refinery.units.formats.office.xtdoc import xtdoc         ; yield xtdoc     # noqa
        # units that implement more complex parsing / searching:
        from refinery.units.formats.archive.xtsim import xtsim        ; yield xtsim     # noqa
        from refinery.units.formats.archive.xtnuitka import xtnuitka  ; yield xtnuitka  # noqa
        # fallbacks that have to be attempted last
        from refinery.units.formats.json import xtjson                ; yield xtjson    # noqa
        from refinery.units.formats.xml import xtxml                  ; yield xtxml     # noqa
        from refinery.units.formats.html import xthtml                ; yield xthtml    # noqa
        from refinery.units.formats.exe.vsect import vsect            ; yield vsect     # noqa

    def unpack(self, data):
        fallback: list[type[PathExtractorUnit]] = []
        errors = {}
        pos_args = self.args.paths
        key_args = dict(
            list=self.args.list,
            path=self.args.path,
            date=self.args.date,
            join_path=self.args.join,
            drop_path=self.args.drop,
        )
        if self.args.pwd:
            key_args.update(pwd=self.args.pwd)
        if self.args.regex:
            key_args.update(regex=self.args.regex)

        class unpacker:
            unit = self

            def __init__(self, handler: type[PathExtractorUnit], fallback: bool):
                self.success = False
                self.handler = handler
                self.fallback = fallback
                self.count = 0

            def __iter__(self):
                handler = self.handler
                if self.fallback:
                    verdict = True
                else:
                    verdict = handler.handles(data)
                if verdict is False:
                    self.unit.log_debug(F&#39;rejected: {handler.name}&#39;)
                elif verdict is True:
                    if not self.fallback:
                        self.unit.log_info(F&#39;accepted: {handler.name}&#39;)
                    try:
                        unit = handler(*pos_args, **key_args)
                        unit.args.lenient = self.unit.args.lenient
                        unit.args.quiet = self.unit.args.quiet
                        unit.log_level = self.unit.log_level
                    except TypeError as error:
                        self.unit.log_debug(&#39;handler construction failed:&#39;, error)
                        return
                    try:
                        test_unpack = not self.unit.args.list
                        for filtered in unit.filter([data]):
                            for item in unit.unpack(filtered):
                                if test_unpack:
                                    item.get_data()
                                    test_unpack = False
                                self.count += 1
                                yield item
                    except Exception as error:
                        if not self.fallback:
                            errors[handler.name] = error
                        if isinstance(error, MultipleArchives):
                            self.unit.log_warn(error)
                        else:
                            if self.unit.log_debug():
                                raise error
                            self.unit.log_info(&#39;handler unpacking failed:&#39;, error)
                    else:
                        self.success = True
                elif verdict is None:
                    fallback.append(handler)

        extracted = 0

        for handler in self.handlers():
            self.CustomPathSeparator = handler.CustomPathSeparator
            it = unpacker(handler, fallback=False)
            yield from it
            if it.success:
                extracted += it.count
                if extracted != 0:
                    break
                self.log_debug(&#39;handler extracted zero items, continuing&#39;)

        if extracted &gt; 0:
            return

        self.log_debug(&#39;fallback order:&#39;, lambda: &#39;, &#39;.join(h.name for h in fallback))

        for handler in fallback:
            it = unpacker(handler, fallback=True)
            yield from it
            if it.success:
                return

        if not errors:
            raise ValueError(&#39;input data did not match any known archive format&#39;)
        for name, error in errors.items():
            self.log_info(F&#39;error when trying to unpack with {name}:&#39;, error)
        raise RefineryException(&#39;none of the available unpackers could handle this data&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xt7z"><code class="flex name class">
<span>class <span class="ident">xt7z</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from a 7zip archive. This unit is a path extractor which extracts data from a hierarchical structure. Each extracted
item is emitted as a separate chunk and has attached to it a meta variable that contains its
path within the source structure. The positional arguments to the command are patterns that can
be used to filter the extracted items by their path. To view only the paths of all chunks, use
the listing switch:</p>
<pre><code>emit something | xt7z --list
</code></pre>
<p>Otherwise, extracted items are written to the standard output port and usually require a frame
to properly process. In order to dump all extracted data to disk, the following pipeline can be
used:</p>
<pre><code>emit something | xt7z [| dump {path} ]
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/xt7z.py#L30-L140" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xt7z(ArchiveUnit, docs=&#39;{0}{s}{PathExtractorUnit}&#39;):
    &#34;&#34;&#34;
    Extract files from a 7zip archive.
    &#34;&#34;&#34;
    @ArchiveUnit.Requires(&#39;py7zr&#39;, [&#39;arc&#39;, &#39;default&#39;, &#39;extended&#39;])
    def _py7zr():
        import py7zr
        import py7zr.exceptions
        return py7zr

    def unpack(self, data: bytearray):
        for match in re.finditer(re.escape(_SIGNATURE), data):
            start = match.start()
            if start != 0:
                self.log_info(F&#39;found a header at offset 0x{start:X}, trying to extract from there.&#39;)
            try:
                yield from self._unpack_from(data, start)
            except self._py7zr.Bad7zFile:
                continue
            else:
                break

    def _unpack_from(self, data: bytearray, zp: int = 0):
        def mk7z(**keywords):
            return self._py7zr.SevenZipFile(MemoryFile(mv[zp:]), **keywords)

        pwd = self.args.pwd
        mv = memoryview(data)
        archive = None

        def test(archive: SevenZipFile):
            if self.args.list:
                archive.list()
                return False
            return archive.testzip()

        if pwd:
            try:
                archive = mk7z(password=pwd.decode(self.codec))
            except self._py7zr.Bad7zFile:
                raise ValueError(&#39;corrupt archive; the password is likely invalid.&#39;)
        else:
            def passwords():
                yield None
                yield from self._COMMON_PASSWORDS
            for pwd in passwords():
                if pwd is None:
                    self.log_debug(&#39;trying empty password&#39;)
                else:
                    self.log_debug(F&#39;trying password: {pwd}&#39;)
                try:
                    archive = mk7z(password=pwd)
                    problem = test(archive)
                except self._py7zr.PasswordRequired:
                    problem = True
                except self._py7zr.UnsupportedCompressionMethodError as E:
                    raise ValueError(E.message)
                except self._py7zr.exceptions.InternalError:
                    # ignore internal errors during testzip
                    break
                except SystemError:
                    problem = True
                except Exception:
                    if pwd is None:
                        raise
                    problem = True
                if not problem:
                    break
            else:
                raise ValueError(&#39;a password is required and none of the default passwords worked.&#39;)

        assert archive is not None
        has_read_method = hasattr(archive, &#39;read&#39;)

        for info in archive.list():
            if has_read_method:
                def extract(archive: SevenZipFile = archive, name: str = info.filename):
                    archive.reset()
                    io = archive.read([name])
                    io = io[name]
                    io.seek(0)
                    return io.read()
            else:
                def extract(archive: SevenZipFile = archive, name: str = info.filename):
                    io = _IOFactory()
                    archive.reset()
                    archive.extract(None, [name], factory=io)
                    return io.buffer.getvalue()

            if info.is_directory:
                continue

            yield self._pack(
                info.filename,
                info.creationtime,
                extract,
                crc32=info.crc32,
                uncompressed=info.uncompressed
            )

    @classmethod
    def handles(cls, data) -&gt; bool | None:
        if data[:6] == _SIGNATURE:
            return True
        if not is_likely_pe(data):
            return None
        offset = get_pe_size(data)
        memory = memoryview(data)
        memory = memory[offset:]
        if memory[:10] == B&#39;;!@Install&#39; and buffer_offset(memory, _SIGNATURE, 0, 0x1000) &gt; 0:
            return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtace"><code class="flex name class">
<span>class <span class="ident">xtace</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from an ACE archive. This unit is a path extractor which extracts data from a hierarchical structure. Each extracted
item is emitted as a separate chunk and has attached to it a meta variable that contains its
path within the source structure. The positional arguments to the command are patterns that can
be used to filter the extracted items by their path. To view only the paths of all chunks, use
the listing switch:</p>
<pre><code>emit something | xtace --list
</code></pre>
<p>Otherwise, extracted items are written to the standard output port and usually require a frame
to properly process. In order to dump all extracted data to disk, the following pipeline can be
used:</p>
<pre><code>emit something | xtace [| dump {path} ]
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/xtace.py#L8-L26" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtace(ArchiveUnit, docs=&#39;{0}{s}{PathExtractorUnit}&#39;):
    &#34;&#34;&#34;
    Extract files from an ACE archive.
    &#34;&#34;&#34;
    def unpack(self, data):
        ace = acefile.open(MemoryFile(data, output=bytes))
        for member in ace.getmembers():
            member: acefile.AceMember
            comment = {} if not member.comment else {&#39;comment&#39;: member.comment}
            yield self._pack(
                member.filename,
                member.datetime,
                lambda a=ace, m=member: a.read(m, pwd=self.args.pwd),
                **comment
            )

    @classmethod
    def handles(cls, data) -&gt; bool:
        return data[7:14] == b&#39;**ACE**&#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtasar"><code class="flex name class">
<span>class <span class="ident">xtasar</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from Atom Shell Archives (ASAR). These are often used to bundle Electron application
data and resources. This unit is a path extractor which extracts data from a hierarchical structure. Each extracted
item is emitted as a separate chunk and has attached to it a meta variable that contains its
path within the source structure. The positional arguments to the command are patterns that can
be used to filter the extracted items by their path. To view only the paths of all chunks, use
the listing switch:</p>
<pre><code>emit something | xtasar --list
</code></pre>
<p>Otherwise, extracted items are written to the standard output port and usually require a frame
to properly process. In order to dump all extracted data to disk, the following pipeline can be
used:</p>
<pre><code>emit something | xtasar [| dump {path} ]
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/xtasar.py#L32-L67" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtasar(ArchiveUnit, docs=&#39;{0}{s}{PathExtractorUnit}&#39;):
    &#34;&#34;&#34;
    Extract files from Atom Shell Archives (ASAR). These are often used to bundle Electron application
    data and resources.
    &#34;&#34;&#34;
    def unpack(self, data: bytearray):
        def _unpack(dir: JSONDict, *path):
            for name, listing in dir.get(&#39;files&#39;, {}).items():
                yield from _unpack(listing, *path, name)
            try:
                offset = dir[&#39;offset&#39;]
                size = dir[&#39;size&#39;]
            except KeyError:
                return
            try:
                offset = int(offset) + header.base
                end = int(size) + offset
            except TypeError:
                self.log_warn(F&#39;unable to convert offset &#34;{offset}&#34; and size &#34;{size}&#34; to integers&#39;)
                return
            if not path:
                self.log_warn(F&#39;not processing item at root with offset {offset} and size {size}&#39;)
                return
            yield UnpackResult(
                &#39;/&#39;.join(path),
                lambda a=offset, b=end: data[a:b],
                offset=offset
            )

        header = AsarHeader(data)
        self.log_debug(F&#39;header read successfully, base offset is {header.base}.&#39;)
        yield from _unpack(header.directory)

    @classmethod
    def handles(cls, data) -&gt; bool | None:
        return data[:4] == b&#39;\04\0\0\0&#39; and data[0x10:0x18] == B&#39;{&#34;files&#34;&#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtcab"><code class="flex name class">
<span>class <span class="ident">xtcab</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from CAB (cabinet) archives. Multi-volume archives can be extracted if all
required disks are present as chunks within the current frame.</p>
<p>This unit is a path extractor which extracts data from a hierarchical structure. Each extracted
item is emitted as a separate chunk and has attached to it a meta variable that contains its
path within the source structure. The positional arguments to the command are patterns that can
be used to filter the extracted items by their path. To view only the paths of all chunks, use
the listing switch:</p>
<pre><code>emit something | xtcab --list
</code></pre>
<p>Otherwise, extracted items are written to the standard output port and usually require a frame
to properly process. In order to dump all extracted data to disk, the following pipeline can be
used:</p>
<pre><code>emit something | xtcab [| dump {path} ]
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/xtcab.py#L8-L44" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtcab(ArchiveUnit, docs=&#39;{0}{p}{PathExtractorUnit}&#39;):
    &#34;&#34;&#34;
    Extract files from CAB (cabinet) archives. Multi-volume archives can be extracted if all
    required disks are present as chunks within the current frame.
    &#34;&#34;&#34;
    def unpack(self, data: Chunk):
        arc: Cabinet = data.temp
        arc.check()
        arc.process()
        one = len(arc.files) == 1
        self.log_info(F&#39;processing CAB with {len(arc)} disks&#39;)
        for id, files in arc.files.items():
            for file in files:
                path = file.name
                if not one:
                    path = F&#39;CAB{id:04X}/{path}&#39;
                yield self._pack(path, file.timestamp, lambda f=file: f.decompress())

    def filter(self, chunks):
        box = None
        cab = Cabinet()
        for chunk in chunks:
            if box is None:
                box = chunk
                box.temp = cab
            if cab.needs_more_disks():
                cab.append(memoryview(chunk))
            else:
                yield box
                box = chunk
                cab = box.temp = Cabinet()
        if box:
            yield box

    @classmethod
    def handles(cls, data):
        return data[:4] == CabDisk.MAGIC</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtchm"><code class="flex name class">
<span>class <span class="ident">xtchm</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from CHM (Windows Help) files.</p>
<p>This unit is a path extractor which extracts data from a hierarchical structure. Each extracted
item is emitted as a separate chunk and has attached to it a meta variable that contains its
path within the source structure. The positional arguments to the command are patterns that can
be used to filter the extracted items by their path. To view only the paths of all chunks, use
the listing switch:</p>
<pre><code>emit something | xtchm --list
</code></pre>
<p>Otherwise, extracted items are written to the standard output port and usually require a frame
to properly process. In order to dump all extracted data to disk, the following pipeline can be
used:</p>
<pre><code>emit something | xtchm [| dump {path} ]
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/xtchm.py#L7-L28" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtchm(PathExtractorUnit, docs=&#39;{0}{p}{PathExtractorUnit}&#39;):
    &#34;&#34;&#34;
    Extract files from CHM (Windows Help) files.
    &#34;&#34;&#34;
    def unpack(self, data):
        chm = CHM(memoryview(data))

        self.log_info(F&#39;language: {chm.header.language_name}&#39;)
        self.log_info(F&#39;codepage: {chm.header.codepage}&#39;)

        for path, record in chm.filesystem.items():
            def extract(chm=chm, record=record):
                return chm.read(record)
            if record.length &lt;= 0:
                continue
            if path.startswith(&#39;::DataSpace&#39;):
                continue
            yield UnpackResult(path, extract)

    @classmethod
    def handles(cls, data):
        return data[:4] == ChmHeader.Magic</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtcpio"><code class="flex name class">
<span>class <span class="ident">xtcpio</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from a CPIO archive. This unit is a path extractor which extracts data from a hierarchical structure. Each extracted
item is emitted as a separate chunk and has attached to it a meta variable that contains its
path within the source structure. The positional arguments to the command are patterns that can
be used to filter the extracted items by their path. To view only the paths of all chunks, use
the listing switch:</p>
<pre><code>emit something | xtcpio --list
</code></pre>
<p>Otherwise, extracted items are written to the standard output port and usually require a frame
to properly process. In order to dump all extracted data to disk, the following pipeline can be
used:</p>
<pre><code>emit something | xtcpio [| dump {path} ]
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/xtcpio.py#L35-L51" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtcpio(ArchiveUnit, docs=&#39;{0}{s}{PathExtractorUnit}&#39;):
    &#34;&#34;&#34;
    Extract files from a CPIO archive.
    &#34;&#34;&#34;
    def unpack(self, data):
        def cpio():
            with suppress(EOF):
                return CPIOEntry(reader)
        reader = StructReader(memoryview(data))
        for entry in iter(cpio, None):
            if entry.name == &#39;TRAILER!!!&#39;:
                break
            yield self._pack(entry.name, entry.mtime, entry.data)

    @classmethod
    def handles(cls, data) -&gt; bool:
        return data[:6] == B&#39;070701&#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtdoc"><code class="flex name class">
<span>class <span class="ident">xtdoc</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from an OLE document such as a Microsoft Word DOCX file.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/office/xtdoc.py#L22-L57" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtdoc(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract files from an OLE document such as a Microsoft Word DOCX file.
    &#34;&#34;&#34;

    @PathExtractorUnit.Requires(&#39;olefile&#39;, [&#39;formats&#39;, &#39;office&#39;, &#39;extended&#39;])
    def _olefile():
        import olefile
        return olefile

    def unpack(self, data):
        with MemoryFile(data) as stream:
            try:
                oledoc = self._olefile.OleFileIO(stream)
            except OSError as error:
                self.log_info(F&#39;error, {error}, treating input as zip file&#39;)
                yield from xtzip().unpack(data)
                return
            for item in oledoc.listdir():
                if not item or not item[-1]:
                    continue
                path = &#39;/&#39;.join(item)
                olestream = oledoc.openstream(path)
                c0 = ord(item[-1][:1])
                if c0 &lt; 20:
                    item[-1] = F&#39;[{c0:d}]{item[-1][1:]}&#39;
                    path = &#39;/&#39;.join(item)
                path = convert_msi_name(path)
                self.log_debug(&#39;exploring:&#39;, path)
                yield UnpackResult(path, olestream.read())

    @classmethod
    def handles(cls, data) -&gt; bool | None:
        if data[:8] == B&#39;\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1&#39;:
            return True
        return is_likely_doc(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtea"><code class="flex name class">
<span>class <span class="ident">xtea</span></span>
<span>(</span><span>key, iv=b'', padding=None, mode=None, raw=False, swap=False, rounds=32)</span>
</code></dt>
<dd>
<section class="desc"><p>XTEA encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/xtea.py#L36-L39" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtea(TEAUnit, cipher=BlockCipherFactory(XTEA)):
    &#34;&#34;&#34;
    XTEA encryption and decryption.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtgz"><code class="flex name class">
<span>class <span class="ident">xtgz</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract a file from a GZip archive.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/xtgz.py#L29-L56" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtgz(ArchiveUnit):
    &#34;&#34;&#34;
    Extract a file from a GZip archive.
    &#34;&#34;&#34;
    def unpack(self, data: bytearray):
        archive = GzipHeader(data)
        path = archive.name
        date = archive.mtime
        date = date and datetime.fromtimestamp(date) or None
        if path is None:
            try:
                meta = metavars(data)
                path = Path(meta[&#39;path&#39;])
            except KeyError:
                path = &#39;ungz&#39;
            else:
                self.log_warn(path)
                suffix = path.suffix
                if suffix.lower() == &#39;.gz&#39;:
                    path = path.with_suffix(&#39;&#39;)
                else:
                    path = path.with_suffix(F&#39;{suffix}.ungz&#39;)
                path = path.as_posix()
        yield self._pack(path, date, archive.data)

    @classmethod
    def handles(cls, data) -&gt; bool:
        return data[:2] == B&#39;\x1F\x8B&#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.xthtml"><code class="flex name class">
<span>class <span class="ident">xthtml</span></span>
<span>(</span><span>*paths, outer=False, attributes=False, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>The unit processes an HTML document and extracts the contents of all elemnts in the DOM of the
given tag. The main purpose is to extract scripts from HTML documents.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/html.py#L146-L231" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xthtml(XMLToPathExtractorUnit):
    &#34;&#34;&#34;
    The unit processes an HTML document and extracts the contents of all elemnts in the DOM of the
    given tag. The main purpose is to extract scripts from HTML documents.
    &#34;&#34;&#34;
    def __init__(
        self, *paths,
        outer: Param[bool, Arg.Switch(&#39;-o&#39;, help=&#39;Include the HTML tags for an extracted element.&#39;)] = False,
        attributes: Param[bool, Arg.Switch(&#39;-a&#39;, help=&#39;Populate chunk metadata with HTML tag attributes.&#39;)] = False,
        list=False,
        join_path=False,
        drop_path=False,
        fuzzy=0,
        exact=False,
        regex=False,
        path=b&#39;path&#39;,
    ):
        super().__init__(
            *paths,
            outer=outer,
            attributes=attributes,
            format=&#39;{tag}&#39;,
            path=path,
            list=list,
            join_path=join_path,
            drop_path=drop_path,
            fuzzy=fuzzy,
            exact=exact,
            regex=regex,
        )

    def unpack(self, data):
        try:
            text = data.decode(self.codec)
        except UnicodeDecodeError:
            text = data.decode(&#39;latin1&#39;)

        html = HTMLTreeParser()
        html.feed(text)
        root = html.tos
        root.reindex()

        meta = metavars(data)
        path = self._make_path_builder(meta, root)

        while root.parent:
            self.log_info(F&#39;tag was not closed: {root.tag}&#39;)
            root = root.parent

        while len(root.children) == 1:
            child, = root.children
            if child.tag != root.tag:
                break
            root = child

        def tree(root: HTMLNode, *parts: str):

            def outer(root: HTMLNode = root):
                return root.recover(inner=False).encode(self.codec)

            def inner(root: HTMLNode = root):
                return root.recover().encode(self.codec)

            tagpath = &#39;/&#39;.join(parts)
            meta = {}

            if self.args.attributes:
                meta.update(root.attributes)

            if root.root:
                yield UnpackResult(tagpath, inner, **meta)
            elif self.args.outer:
                yield UnpackResult(tagpath, outer, **meta)
            else:
                yield UnpackResult(tagpath, inner, **meta)

            for child in root.children:
                if child.textual:
                    continue
                yield from tree(child, *parts, path(child))

        yield from tree(root, path(root))

    @classmethod
    def handles(cls, data):
        return is_likely_htm(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtinno"><code class="flex name class">
<span>class <span class="ident">xtinno</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from InnoSetup archives: This unit is a path extractor which extracts data from a hierarchical structure. Each extracted
item is emitted as a separate chunk and has attached to it a meta variable that contains its
path within the source structure. The positional arguments to the command are patterns that can
be used to filter the extracted items by their path. To view only the paths of all chunks, use
the listing switch:</p>
<pre><code>emit something | xtinno --list
</code></pre>
<p>Otherwise, extracted items are written to the standard output port and usually require a frame
to properly process. In order to dump all extracted data to disk, the following pipeline can be
used:</p>
<pre><code>emit something | xtinno [| dump {path} ]
</code></pre>
<p>Note: This unit generates the following synthetic metadata files under the "meta" directory:</p>
<ul>
<li><code>setup.bin</code> contains the raw bytes for the setup metadata</li>
<li><code>setup.template</code> contains the raw and unprocessed metadata in JSON format</li>
<li><code>setup.json</code> contains the setup metadata with all format fields expanded</li>
</ul>
<p>Similarly, there are <code>files.bin</code>, <code>files.template</code>, and <code>files.json</code> that contain the metadata
of the archived files. The files that are extracted under the "embedded" directory are usually
parts of the InnoSetup installer and not user data. All archived files are extracted within the
directory named "data".</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/xtinno.py#L29-L115" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtinno(ArchiveUnit, _ps, docs=&#39;{0} {PathExtractorUnit}{p}{_ps}&#39;):
    &#34;&#34;&#34;
    Extract files from InnoSetup archives:
    &#34;&#34;&#34;
    def unpack(self, data: bytearray):
        def post_process_json(doc):
            if isinstance(doc, dict):
                return {key: post_process_json(val) for key, val in doc.items()}
            if isinstance(doc, list):
                return [post_process_json(entry) for entry in doc]
            if not isinstance(doc, str):
                return doc
            try:
                return inno.emulator.reset().expand_constant(doc)
            except Exception:
                return doc

        inno = InnoArchive(data, self)

        password: bytes = self.args.pwd
        password = password.decode(self.codec) if password else None

        if any(file.encrypted for file in inno.files) and password is None:
            self.log_info(&#39;some files are password-protected and no password was given&#39;)

        with BytesAsArrayEncoder as encoder:
            yield self._pack(&#39;meta/setup.bin&#39;, None, inno.streams.TSetup.data)
            doc = inno.setup_info.json()
            yield self._pack(&#39;meta/setup.template&#39;, None, encoder.dumps(doc).encode(self.codec))
            doc = post_process_json(doc)
            yield self._pack(&#39;meta/setup.json&#39;, None, encoder.dumps(doc).encode(self.codec))

        with BytesAsArrayEncoder as encoder:
            yield self._pack(&#39;meta/files.bin&#39;, None, inno.streams.TData.data)
            doc = inno.setup_data.json()
            yield self._pack(&#39;meta/files.template&#39;, None, encoder.dumps(doc).encode(self.codec))
            doc = post_process_json(doc)
            yield self._pack(&#39;meta/files.json&#39;, None, encoder.dumps(doc).encode(self.codec))

        def _uninstaller(i=inno):
            return i.read_stream(i.streams.Uninstaller)
        yield self._pack(&#39;embedded/uninstaller.exe&#39;, None, _uninstaller)

        if license := inno.setup_info.Header.get_license():
            yield self._pack(&#39;embedded/license.rtf&#39;, None, license.encode(self.codec))

        if script := inno.setup_info.Header.get_script():
            yield self._pack(&#39;embedded/script.bin&#39;, None, script)
            yield self._pack(&#39;embedded/script.ps&#39;, None,
                lambda i=inno: i.ifps.disassembly().encode(self.codec))

        if dll := inno.setup_info.get_decompress_dll():
            yield self._pack(F&#39;embedded/decompress.{magic(dll).extension}&#39;, None, dll)

        if dll := inno.setup_info.get_decryption_dll():
            yield self._pack(F&#39;embedded/decryption.{magic(dll).extension}&#39;, None, dll)

        for size, images in (
            (&#39;small&#39;, inno.setup_info.get_wizard_images_small()),
            (&#39;large&#39;, inno.setup_info.get_wizard_images_large()),
        ):
            _formatting = len(str(len(images) + 1))
            for k, img in enumerate(images, 1):
                yield self._pack(F&#39;embedded/images/{size}{k:0{_formatting}d}.{magic(img).extension}&#39;, None, img)

        for file in inno.files:
            if file.dupe:
                continue

            def _read(inno=inno, file=file, pwd=password):
                if pwd is None:
                    inno.guess_password(10)
                if self.leniency &gt; 0:
                    return inno.read_file(file, pwd)
                try:
                    return inno.read_file_and_check(file, pwd)
                except InvalidPassword:
                    raise
                except Exception as E:
                    raise ValueError(F&#39;{E!s} [ignore this check with -L]&#39;) from E

            yield self._pack(file.path, file.date, _read,
                tags=[t.name for t in SetupFileFlags if t &amp; file.tags])

    @classmethod
    def handles(cls, data):
        return is_inno_setup(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtiso"><code class="flex name class">
<span>class <span class="ident">xtiso</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', fs='auto')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from a ISO archive. This unit is a path extractor which extracts data from a hierarchical structure. Each extracted
item is emitted as a separate chunk and has attached to it a meta variable that contains its
path within the source structure. The positional arguments to the command are patterns that can
be used to filter the extracted items by their path. To view only the paths of all chunks, use
the listing switch:</p>
<pre><code>emit something | xtiso --list
</code></pre>
<p>Otherwise, extracted items are written to the standard output port and usually require a frame
to properly process. In order to dump all extracted data to disk, the following pipeline can be
used:</p>
<pre><code>emit something | xtiso [| dump {path} ]
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/xtiso.py#L12-L123" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtiso(ArchiveUnit, docs=&#39;{0}{s}{PathExtractorUnit}&#39;):
    &#34;&#34;&#34;
    Extract files from a ISO archive.
    &#34;&#34;&#34;
    def __init__(
        self,
        *paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False,
        path=b&#39;path&#39;, date=b&#39;date&#39;,
        fs: Param[str, Arg.Choice(&#39;-s&#39;, metavar=&#39;TYPE&#39;, choices=_ISO_FILE_SYSTEMS, help=(
            &#39;Specify a file system ({choices}) extension to use. The default setting {default} will automatically &#39;
            &#39;detect the first of the other available options and use it.&#39;))] = &#39;auto&#39;
    ):
        if fs not in _ISO_FILE_SYSTEMS:
            raise ValueError(F&#39;invalid file system {fs}: must be udf, joliet, rr, iso, or auto.&#39;)
        super().__init__(
            *paths,
            list=list,
            join_path=join_path,
            drop_path=drop_path,
            fuzzy=fuzzy,
            exact=exact,
            regex=regex,
            path=path,
            date=date,
            fs=fs
        )

    @ArchiveUnit.Requires(&#39;pycdlib&#39;, [&#39;arc&#39;, &#39;default&#39;, &#39;extended&#39;])
    def _pycdlib():
        import pycdlib
        import pycdlib.dates

        def fixed_parse(self, datestr):
            datestr = datestr[:-3] + b&#39;00\0&#39;
            return original_parse(self, datestr)

        original_parse = pycdlib.dates.VolumeDescriptorDate.parse
        pycdlib.dates.VolumeDescriptorDate.parse = fixed_parse
        return pycdlib

    @staticmethod
    def _strip_revision(name: str):
        base, split, revision = name.partition(&#39;;&#39;)
        return base if split and revision.isdigit() else name

    def unpack(self, data):
        if not self.handles(data):
            self.log_warn(&#39;The data does not look like an ISO file.&#39;)
        with MemoryFile(data, output=bytes) as stream:
            iso = self._pycdlib.PyCdlib()
            iso.open_fp(stream)
            fs = self.args.fs
            if fs != &#39;auto&#39;:
                mkfacade = {
                    &#39;iso&#39;    : iso.get_iso9660_facade,
                    &#39;udf&#39;    : iso.get_udf_facade,
                    &#39;joliet&#39; : iso.get_joliet_facade,
                    &#39;rr&#39;     : iso.get_rock_ridge_facade,
                }
                facade = mkfacade[fs]()
            elif iso.has_udf():
                self.log_info(&#39;using format: udf&#39;)
                facade = iso.get_udf_facade()
            elif iso.has_joliet():
                self.log_info(&#39;using format: joliet&#39;)
                facade = iso.get_joliet_facade()
            elif iso.has_rock_ridge():
                self.log_info(&#39;using format: rr&#39;)
                facade = iso.get_rock_ridge_facade()
            else:
                self.log_info(&#39;using format: iso&#39;)
                facade = iso.get_iso9660_facade()

            for root, _, files in facade.walk(&#39;/&#39;):
                root = root.rstrip(&#39;/&#39;)
                for name in files:
                    name = name.lstrip(&#39;/&#39;)
                    path = F&#39;{root}/{name}&#39;
                    try:
                        info = facade.get_record(path)
                        date = info.date
                    except Exception:
                        info = None
                        date = None
                    else:
                        date = datetime.datetime(
                            date.years_since_1900 + 1900,
                            date.month,
                            date.day_of_month,
                            date.hour,
                            date.minute,
                            date.second,
                            tzinfo=datetime.timezone(datetime.timedelta(minutes=15 * date.gmtoffset))
                        )

                    def extract(info=info, path=path):
                        if info:
                            buffer = MemoryFile(bytearray(info.data_length))
                        else:
                            buffer = MemoryFile(bytearray())
                        facade.get_file_from_iso_fp(buffer, path)
                        return buffer.getvalue()

                    yield self._pack(self._strip_revision(path), date, extract)

    @classmethod
    def handles(cls, data) -&gt; bool:
        return any(data[k] == B&#39;CD001&#39; for k in (
            slice(0x8001, 0x8006),
            slice(0x8801, 0x8806),
            slice(0x9001, 0x9006),
        ))</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtiss"><code class="flex name class">
<span>class <span class="ident">xtiss</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts files from Install Shield Setup files. This unit is a path extractor which extracts data from a hierarchical structure. Each extracted
item is emitted as a separate chunk and has attached to it a meta variable that contains its
path within the source structure. The positional arguments to the command are patterns that can
be used to filter the extracted items by their path. To view only the paths of all chunks, use
the listing switch:</p>
<pre><code>emit something | xtiss --list
</code></pre>
<p>Otherwise, extracted items are written to the standard output port and usually require a frame
to properly process. In order to dump all extracted data to disk, the following pipeline can be
used:</p>
<pre><code>emit something | xtiss [| dump {path} ]
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/xtiss.py#L71-L94" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtiss(ArchiveUnit, docs=&#39;{0}{s}{PathExtractorUnit}&#39;):
    &#34;&#34;&#34;
    Extracts files from Install Shield Setup files.
    &#34;&#34;&#34;
    def unpack(self, data: bytearray):
        offset = max(data.rfind(magic) for magic in ISSReader.MAGIC)
        if offset &lt; 0:
            raise ValueError(&#39;ISS magic not found.&#39;)
        data[:offset] = []

        reader = ISSReader(data)
        count = reader.iss_archive_header()

        self.log_info(F&#39;archive contains {count} files according to header&#39;)

        for _ in range(count):
            name, data = reader.iss_file()
            yield self._pack(name, None, data)

    @classmethod
    def handles(cls, data) -&gt; bool | None:
        if data[:2] != B&#39;MZ&#39;:
            return False
        return any(buffer_contains(data, m) for m in ISSReader.MAGIC)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtjson"><code class="flex name class">
<span>class <span class="ident">xtjson</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract values from a JSON document.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/json.py#L14-L52" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtjson(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract values from a JSON document.
    &#34;&#34;&#34;
    CustomPathSeparator = &#39;.&#39;

    def unpack(self, data):

        sep = self.CustomPathSeparator

        def crawl(path, cursor):
            if isinstance(cursor, dict):
                for key, value in cursor.items():
                    yield from crawl(F&#39;{path}{sep}{key}&#39;, value)
            elif isinstance(cursor, list):
                for key, value in enumerate(cursor):
                    yield from crawl(F&#39;{path}{sep}{key:d}&#39;, value)
            if path:
                yield path, cursor, cursor.__class__.__name__

        if not isinstance(data, (dict, list)):
            data = json.loads(data)

        for path, item, typename in crawl(&#39;&#39;, data):
            def extract(item=item):
                if isinstance(item, (list, dict)):
                    dumped = json.dumps(item, indent=4)
                else:
                    dumped = str(item)
                try:
                    return dumped.encode(&#39;latin1&#39;)
                except UnicodeEncodeError:
                    return dumped.encode(&#39;utf8&#39;)

            yield UnpackResult(path, extract, type=typename)

    @classmethod
    def handles(cls, data) -&gt; bool | None:
        return is_likely_json(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtmacho"><code class="flex name class">
<span>class <span class="ident">xtmacho</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract the individual executables from a MachO universal binary (sometimes called a MachO fat file)."</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/xtmacho.py#L40-L72" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtmacho(ArchiveUnit):
    &#34;&#34;&#34;
    Extract the individual executables from a MachO universal binary (sometimes called a MachO fat file).&#34;
    &#34;&#34;&#34;
    _SIGNATURE_BE = B&#39;\xCA\xFE\xBA\xBE&#39;
    _SIGNATURE_LE = B&#39;\xBE\xBA\xFE\xCA&#39;

    def unpack(self, data: bytearray):
        view = memoryview(data)
        signature = bytes(view[:4])
        try:
            reader = StructReader(view, bigendian={
                self._SIGNATURE_BE: True,
                self._SIGNATURE_LE: False,
            }[signature])
        except KeyError as KE:
            raise ValueError(&#39;Not a MachO universal binary; invalid magic header bytes.&#39;) from KE
        else:
            reader.seekset(4)
        count = reader.u32()
        self.log_info(F&#39;reading {count} embedded executables&#39;)
        while count &gt; 0:
            fa = FatArch(reader)
            self.log_info(F&#39;reading item of size 0x{len(fa.data):08X}, arch {fa.cputype.name}&#39;)
            yield self._pack(fa.cputype.name, None, fa.data)
            count -= 1

    @classmethod
    def handles(cls, data):
        return data[:4] in (
            cls._SIGNATURE_BE,
            cls._SIGNATURE_LE,
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtmagtape"><code class="flex name class">
<span>class <span class="ident">xtmagtape</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from SIMH magtape files.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/xtmagtape.py#L9-L43" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtmagtape(Unit):
    &#34;&#34;&#34;
    Extract files from SIMH magtape files.
    &#34;&#34;&#34;
    def process(self, data: bytearray):
        reader = StructReader(data)

        for r in itertools.count():
            buffer = MemoryFile()

            for k in itertools.count():
                try:
                    head = reader.peek(4)
                    size = reader.read_integer(24)
                    mark = reader.read_byte()
                except EOFError:
                    self.log_info(&#39;end of file while reading chunk header, terminating&#39;)
                    return
                if not any(head):
                    if k == 0:
                        return
                    break
                if mark != 0:
                    self.log_warn(F&#39;error code 0x{mark:02X} in record {r}.{k}&#39;)
                buffer.write(reader.read(size))
                if reader.peek(4) != head:
                    if reader.tell() % 2 and reader.peek(5)[1:] == head:
                        padding = reader.read_byte()
                        if padding != 0:
                            self.log_info(F&#39;nonzero padding byte in record {r}.{k}&#39;)
                    else:
                        raise ValueError(&#39;Invalid footer, data is corrupted.&#39;)
                reader.seekrel(4)

            yield buffer.getvalue()</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtmail"><code class="flex name class">
<span>class <span class="ident">xtmail</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files and body from EMail messages. The unit supports both the Outlook message format
and regular MIME documents.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/email.py#L22-L225" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtmail(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract files and body from EMail messages. The unit supports both the Outlook message format
    and regular MIME documents.
    &#34;&#34;&#34;
    def _get_headparts(self, head: Iterable[tuple[str, str]]):
        def normalize_spaces(value: str):
            return &#39;&#39;.join(re.sub(R&#39;\A\s+&#39;, &#39;\x20&#39;, t) for t in value.splitlines(False))

        _headers: dict[str, list[str]] = {}
        for key, value in head:
            _headers.setdefault(key, []).append(mimewords.convert(normalize_spaces(value)))
        headers = {
            key: value[0] if len(value) == 1 else [t for t in value if t]
            for key, value in _headers.items()}

        yield UnpackResult(&#39;headers.txt&#39;,
            lambda h=head: &#39;\n&#39;.join(F&#39;{k}: {v}&#39; for k, v in h).encode(self.codec))

        received = []

        for recv in headers.get(&#39;Received&#39;, []):
            if not recv.startswith(&#39;from &#39;):
                received = None
                break
            recv = recv[5:]
            src, _, rest = recv.partition(&#39; by &#39;)
            dst, _, rest = rest.partition(&#39; with &#39;)
            received.append({
                &#39;Source&#39;: src.partition(&#39;\x20&#39;)[0],
                &#39;Target&#39;: dst.partition(&#39;\x20&#39;)[0],
            })

        if received:
            received.reverse()
            headers[&#39;ReceivedTrace&#39;] = received

        yield UnpackResult(&#39;headers.json&#39;,
            lambda jsn=headers: json.dumps(jsn, indent=4).encode(self.codec))

    @PathExtractorUnit.Requires(&#39;extract-msg&#39;, [&#39;formats&#39;, &#39;office&#39;, &#39;default&#39;, &#39;extended&#39;])
    def _extract_msg():
        import extract_msg.enums
        return extract_msg

    def _get_parts_outlook(self, data):
        def ensure_bytes(data: bytes | str | None):
            if data is None:
                return B&#39;&#39;
            elif isinstance(data, str):
                return data.encode(self.codec)
            else:
                return data

        def make_message(name, msg: Message):
            bodies = msg.detectedBodies
            BT = self._extract_msg.enums.BodyTypes
            if bodies &amp; BT.HTML:
                def htm(msg=msg):
                    with NoLogging():
                        try:
                            return ensure_bytes(msg.htmlBody)
                        except Exception:
                            return B&#39;&#39;
                yield UnpackResult(F&#39;{name}.htm&#39;, htm)
            if bodies &amp; BT.PLAIN:
                def txt(msg=msg):
                    with NoLogging():
                        try:
                            return ensure_bytes(msg.body)
                        except Exception:
                            return B&#39;&#39;
                yield UnpackResult(F&#39;{name}.txt&#39;, txt)
            if bodies &amp; BT.RTF:
                def rtf(msg=msg):
                    with NoLogging():
                        try:
                            return ensure_bytes(msg.rtfBody)
                        except Exception:
                            return B&#39;&#39;
                yield UnpackResult(F&#39;{name}.rtf&#39;, rtf)

        msgcount = 0

        with NoLogging():
            class ForgivingMessage(self._extract_msg.Message):
                &#34;&#34;&#34;
                If parsing the input bytes fails early, the &#34;__open&#34; private attribute may not
                yet exist. This hack prevents an exception to occur in the destructor.
                &#34;&#34;&#34;
                def __getattr__(self, key: str):
                    if key.endswith(&#39;_open&#39;):
                        return False
                    raise AttributeError(key)
            msg = ForgivingMessage(bytes(data))

        header = dict(msg.header)

        if x := msg.date:
            header[&#39;Date&#39;] = email.utils.format_datetime(x)
        if x := msg.sender:
            header[&#39;From&#39;] = x
        if x := msg.to:
            header[&#39;To&#39;] = x
        if x := msg.cc:
            header[&#39;Cc&#39;] = x
        if x := msg.bcc:
            header[&#39;Bcc&#39;] = x
        if x := msg.messageId:
            header[&#39;Message-Id&#39;] = x
        if x := msg.subject:
            header[&#39;Subject&#39;] = x

        for key, val in list(header.items()):
            if val := val.strip().replace(&#39;\0&#39;, &#39;&#39;):
                header[key] = val
            else:
                del header[key]

        yield from self._get_headparts(header.items())
        yield from make_message(&#39;body&#39;, msg)

        def attachments(msg):
            for attachment in getattr(msg, &#39;attachments&#39;, ()):
                yield attachment
                if attachment.type == &#39;data&#39;:
                    continue
                yield from attachments(attachment.data)

        for attachment in attachments(msg):
            at = attachment.type
            if at is self._extract_msg.enums.AttachmentType.MSG:
                msgcount += 1
                yield from make_message(F&#39;attachments/msg_{msgcount:d}&#39;, attachment.data)
                continue
            if not isbuffer(attachment.data):
                self.log_warn(F&#39;unknown attachment of type {at}, please report this!&#39;)
                continue
            path = attachment.longFilename or attachment.shortFilename
            path = path.rstrip(&#39;\0&#39;)
            yield UnpackResult(F&#39;attachments/{path}&#39;, attachment.data)

    @PathExtractorUnit.Requires(&#39;chardet&#39;, [&#39;default&#39;, &#39;extended&#39;])
    def _chardet():
        import chardet
        return chardet

    def _get_parts_regular(self, data: bytes):
        try:
            info = self._chardet.detect(data)
            msg = data.decode(str(info[&#39;encoding&#39;]))
        except UnicodeDecodeError:
            raise ValueError(&#39;This is not a plaintext email message.&#39;)
        else:
            msg = Parser().parsestr(msg)

        yield from self._get_headparts(msg.items())

        for k, part in enumerate(msg.walk()):
            path = part.get_filename()
            error_message = None
            result = None
            if path is None:
                extension = file_extension(part.get_content_type(), &#39;txt&#39;)
                path = F&#39;body.{extension}&#39;
            else:
                path = path | mimewords | str
                path = F&#39;attachments/{path}&#39;
            try:
                payload = part.get_payload(decode=True)
                if payload is None or isinstance(payload, bytes):
                    result = payload
                else:
                    raise TypeError
            except Exception as E:
                try:
                    payload = part.get_payload(decode=False)
                except Exception as E:
                    error_message = str(E)
                else:
                    from refinery.units.pattern.carve import carve
                    self.log_warn(F&#39;manually decoding part {k}, data might be corrupted: {path}&#39;)
                    if isinstance(payload, str):
                        payload = payload.encode(&#39;latin1&#39;)
                    if payload := asbuffer(payload):
                        result = next(payload | carve(&#39;b64&#39;, stripspace=True, single=True, decode=True))
                    else:
                        error_message = str(E)
                        result = None
            if not result:
                if error_message is not None:
                    self.log_warn(F&#39;could not get content of message part {k}: {error_message!s}&#39;)
                continue
            yield UnpackResult(path, result)

    def unpack(self, data):
        if data[:len(CDFv2_MARKER)] == CDFv2_MARKER:
            yield from self._get_parts_outlook(data)
        else:
            yield from self._get_parts_regular(data)

    @classmethod
    def handles(cls, data) -&gt; bool:
        return is_likely_email(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtmsi"><code class="flex name class">
<span>class <span class="ident">xtmsi</span></span>
<span>(</span><span>*paths, list=False, path=b'path', join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, nocab=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files and metadata from Microsoft Installer (MSI) archives. The synthetic file MsiTables.json contains
parsed MSI table information, similar to the output of the Orca tool. Binary streams are placed in a
virtual folder called "Binary", and extracted scripts from custom actions are separately extracted in
a virtual folder named "Action".</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/msi.py#L134-L429" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtmsi(xtdoc):
    &#34;&#34;&#34;
    Extract files and metadata from Microsoft Installer (MSI) archives. The synthetic file {FN} contains
    parsed MSI table information, similar to the output of the Orca tool. Binary streams are placed in a
    virtual folder called &#34;Binary&#34;, and extracted scripts from custom actions are separately extracted in
    a virtual folder named &#34;Action&#34;.
    &#34;&#34;&#34;

    _SYNTHETIC_STREAMS_FILENAME = &#39;MsiTables.json&#39;
    _SYNTHETIC_STREAMS_TOPLEVEL = &#39;MsiTables&#39;

    # https://learn.microsoft.com/en-us/windows/win32/msi/summary-list-of-all-custom-action-types
    _CUSTOM_ACTION_TYPES = {
        0x01: &#39;DLL file stored in a Binary table stream.&#39;,
        0x02: &#39;EXE file stored in a Binary table stream.&#39;,
        0x05: &#39;JScript file stored in a Binary table stream.&#39;,
        0x06: &#39;VBScript file stored in a Binary table stream.&#39;,
        0x11: &#39;DLL file that is installed with a product.&#39;,
        0x12: &#39;EXE file that is installed with a product.&#39;,
        0x13: &#39;Displays a specified error message and returns failure, terminating the installation.&#39;,
        0x15: &#39;JScript file that is installed with a product.&#39;,
        0x16: &#39;VBScript file that is installed with a product.&#39;,
        0x22: &#39;EXE file having a path referencing a directory.&#39;,
        0x23: &#39;Directory set with formatted text.&#39;,
        0x25: &#39;JScript text stored in this sequence table.&#39;,
        0x26: &#39;VBScript text stored in this sequence table.&#39;,
        0x32: &#39;EXE file having a path specified by a property value.&#39;,
        0x33: &#39;Property set with formatted text.&#39;,
        0x35: &#39;JScript text specified by a property value.&#39;,
        0x36: &#39;VBScript text specified by a property value.&#39;,
    }

    def __init__(
            self, *paths,
            list=False, path=b&#39;path&#39;, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False,
            nocab: Param[bool, Arg.Switch(&#39;-N&#39;, help=&#39;Do not list and extract embedded CAB archives.&#39;)] = False, **kw,
    ):
        super().__init__(
            *paths,
            list=list,
            path=path,
            join_path=join_path,
            drop_path=drop_path,
            nocab=nocab,
            fuzzy=fuzzy,
            exact=exact,
            regex=regex,
            **kw,
        )

    def unpack(self, data):
        streams = {result.path: result for result in super().unpack(data)}

        def stream(name: str):
            return streams.pop(name).get_data()

        def column_formats(table: dict[str, MSITableColumnInfo]) -&gt; str:
            return &#39;&#39;.join(v.struct_format for v in table.values())

        def stream_to_rows(data: buf, row_format: str):
            row_size = struct.calcsize(F&#39;&lt;{row_format}&#39;)
            row_count = int(len(data) / row_size)
            reader = StructReader(data)
            columns = [reader.read_struct(F&#39;&lt;{sc * row_count}&#39;) for sc in row_format]
            for i in range(row_count):
                yield [c[i] for c in columns]

        tables: dict[str, dict[str, MSITableColumnInfo]] = collections.defaultdict(collections.OrderedDict)
        strings = MSIStringData(stream(&#39;!_StringData&#39;), stream(&#39;!_StringPool&#39;))

        for tbl_name_id, col_number, col_name_id, col_attributes in stream_to_rows(stream(&#39;!_Columns&#39;), &#39;HHHH&#39;):
            tbl_name = strings.ref(tbl_name_id)
            col_name = strings.ref(col_name_id)
            tables[tbl_name][col_name] = MSITableColumnInfo(col_number, col_attributes)

        table_names_given = {strings.ref(k) for k in chunks.unpack(stream(&#39;!_Tables&#39;), 2, False)}
        table_names_known = set(tables)

        for name in table_names_known - table_names_given:
            self.log_warn(F&#39;table name known but not given: {name}&#39;)
        for name in table_names_given - table_names_known:
            self.log_warn(F&#39;table name given but not known: {name}&#39;)

        class ScriptItem(NamedTuple):
            row_index: int
            extension: str | None

        processed_table_data: dict[str, list[dict[str, str]]] = {}
        tbl_properties: dict[str, str] = {}
        tbl_files: dict[str, str] = {}
        tbl_components: dict[str, str] = {}
        postprocessing: list[ScriptItem] = []

        def format_string(string: str):
            # https://learn.microsoft.com/en-us/windows/win32/msi/formatted
            def _replace(match: re.Match[str]):
                _replace.done = False
                prefix, name = match.groups()
                if not prefix:
                    tbl = tbl_properties
                elif prefix in &#39;%&#39;:
                    name = name.rstrip(&#39;%&#39;).upper()
                    return F&#39;%{name}%&#39;
                elif prefix in &#39;!#&#39;:
                    tbl = tbl_files
                elif prefix in &#39;$&#39;:
                    tbl = tbl_components
                else:
                    raise ValueError
                return tbl.get(name, &#39;&#39;)
            while True:
                _replace.done = True
                string = re.sub(R&#39;&#39;&#39;(?x)
                    \[             # open square bracket
                      (?![~\\])    # not followed by escapes
                      ([%$!#]?)    # any of the valid prefix characters
                      ([^[\]{}]+)  # no brackets or braces
                    \]&#39;&#39;&#39;, _replace, string)
                if _replace.done:
                    break
            string = re.sub(r&#39;\[\\(.)\]&#39;, r&#39;\1&#39;, string)
            string = string.replace(&#39;[~]&#39;, &#39;\0&#39;)
            return string

        for table_name, table in tables.items():
            stream_name = F&#39;!{table_name}&#39;
            if stream_name not in streams:
                continue
            processed = []
            info = list(table.values())
            keys = list(table.keys())
            temp = [k.strip(&#39;_&#39;) for k in keys]
            if len(set(keys)) == len(set(temp)):
                keys = temp
            for r, row in enumerate(stream_to_rows(stream(stream_name), column_formats(table))):
                values = []
                for index, value in enumerate(row):
                    vt = info[index].type
                    if vt is MsiType.Long:
                        if value != 0:
                            value -= 0x80000000
                    elif vt is MsiType.Short:
                        if value != 0:
                            value -= 0x8000
                    elif value in strings:
                        value = strings.ref(value)
                    elif not info[index].is_integer:
                        value = &#39;&#39;
                    values.append(value)
                if table_name == &#39;Property&#39;:
                    tbl_properties[values[0]] = values[1]
                if table_name == &#39;File&#39;:
                    tbl_properties[values[0]] = values[2]
                if table_name == &#39;Component&#39;:
                    tbl_properties[values[0]] = F&#39;%{values[2]}%&#39;
                entry = dict(zip(keys, values))
                einfo = {t: i for t, i in zip(keys, info)}
                if table_name == &#39;MsiFileHash&#39;:
                    entry[&#39;Hash&#39;] = struct.pack(
                        &#39;&lt;IIII&#39;,
                        row[2] ^ 0x80000000,
                        row[3] ^ 0x80000000,
                        row[4] ^ 0x80000000,
                        row[5] ^ 0x80000000,
                    ).hex()
                if table_name == &#39;CustomAction&#39;:
                    code = row[1] &amp; 0x3F
                    try:
                        entry[&#39;Comment&#39;] = self._CUSTOM_ACTION_TYPES[code]
                    except LookupError:
                        pass
                    t = einfo.get(&#39;Target&#39;)
                    c = {0x25: &#39;js&#39;, 0x26: &#39;vbs&#39;, 0x33: None}
                    if code in c and t and not t.is_integer:
                        postprocessing.append(ScriptItem(r, c[code]))
                processed.append(entry)
            if processed:
                processed_table_data[table_name] = processed

        ca = processed_table_data.get(&#39;CustomAction&#39;, None)
        for item in postprocessing:
            entry = ca[item.row_index]
            try:
                path: str = entry[&#39;Action&#39;]
                data: str = entry[&#39;Target&#39;]
            except KeyError:
                continue
            root = F&#39;Action/{path}&#39;
            if item.extension:
                path = F&#39;{root}.{item.extension}&#39;
                streams[path] = UnpackResult(path, data.encode(self.codec))
                continue
            data = format_string(data)
            parts = [part.partition(&#39;\x02&#39;) for part in data.split(&#39;\x01&#39;)]
            if not all(part[1] == &#39;\x02&#39; for part in parts):
                continue
            for name, _, script in parts:
                if not name.lower().startswith(&#39;script&#39;):
                    continue
                if not script:
                    continue
                path = F&#39;{root}.{name}&#39;
                streams[path] = UnpackResult(path, script.encode(self.codec))

        for ignored_stream in [
            &#39;SummaryInformation&#39;,
            &#39;DocumentSummaryInformation&#39;,
            &#39;DigitalSignature&#39;,
            &#39;MsiDigitalSignatureEx&#39;
        ]:
            if r := streams.pop(F&#39;[5]{ignored_stream}&#39;, None):
                r.path = F&#39;Meta/{ignored_stream}&#39;
                yield r

        inconsistencies = 0
        w1 = len(str(len(strings)))
        w2 = len(str(max(max(strings.computed_ref_count), max(strings.provided_ref_count))))
        for k in range(len(strings)):
            c = strings.computed_ref_count[k]
            p = strings.provided_ref_count[k]
            if c != p and not self.log_debug(F&#39;string {k:0{w1}d} reference count computed={c:0{w2}d} provided={p:0{w2}d}&#39;):
                inconsistencies += 1
        if inconsistencies:
            self.log_info(F&#39;found {inconsistencies} incorrect string reference counts&#39;)

        def fix_msi_path(path: str):
            prefix, dot, name = path.partition(&#39;.&#39;)
            if dot == &#39;.&#39; and prefix in processed_table_data:
                path = F&#39;{prefix}/{name}&#39;
            return path

        if self.args.nocab:
            cabs = {}
        else:
            def _iscab(path):
                return media_info and any(item.get(&#39;Cabinet&#39;, &#39;&#39;) == F&#39;#{path}&#39; for item in media_info)
            media_info: list[JSONDict] = processed_table_data.get(&#39;Media&#39;, [])
            cabs: dict[str, UnpackResult] = {
                path: item for path, item in streams.items() if _iscab(path)}
            for cab in cabs:
                self.log_info(F&#39;found cab file: {cab}&#39;)
        if cabs:
            file_names: dict[str, JSONDict] = {}

            for file_info in processed_table_data.get(&#39;File&#39;, []):
                try:
                    src_name = file_info[&#39;File&#39;]
                    dst_name = file_info[&#39;FileName&#39;]
                except KeyError:
                    continue
                _, _, long = dst_name.partition(&#39;|&#39;)
                dst_name = long or dst_name
                file_names[src_name] = dst_name

            for path, cab in cabs.items():
                try:
                    _cabinet = Cabinet(cab.get_data())
                    unpacked = _cabinet.process().get_files()
                except Exception as e:
                    self.log_info(F&#39;unable to extract embedded cab file: {e!s}&#39;)
                    continue
                base, dot, ext = path.rpartition(&#39;.&#39;)
                if dot == &#39;.&#39; and ext.lower() == &#39;cab&#39;:
                    path = base
                else:
                    del streams[path]
                    cab.path = F&#39;{path}.cab&#39;
                    streams[cab.path] = cab
                for result in unpacked:
                    sub_path = file_names.get(result.name, result.name)
                    sub_path = self._get_path_separator().join((path, sub_path))
                    streams[sub_path] = UnpackResult(sub_path, lambda r=result: r.decompress())

        streams = {fix_msi_path(path): item for path, item in streams.items()}
        ds = UnpackResult(self._SYNTHETIC_STREAMS_FILENAME,
                json.dumps(processed_table_data, indent=4).encode(self.codec))
        streams[ds.path] = ds

        converter = csv()
        for key, data in processed_table_data.items():
            sk = key.strip(&#39;_&#39;)
            if sk not in processed_table_data:
                key = sk
            try:
                tbl = UnpackResult(F&#39;{self._SYNTHETIC_STREAMS_TOPLEVEL}/{key}.csv&#39;, converter.json_to_csv(data))
            except Exception:
                continue
            streams[tbl.path] = tbl

        for path in sorted(streams):
            streams[path].path = path
            yield streams[path]

    @classmethod
    def handles(cls, data):
        return is_likely_msi(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtnode"><code class="flex name class">
<span>class <span class="ident">xtnode</span></span>
<span>(</span><span>*paths, entry=False, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date')</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts and decompiles files from compiled Node.Js applications. Supports both nexe and pkg, two
utilities that are commonly used to generate stand-alone executables.</p>
<p>This unit is a path extractor which extracts data from a hierarchical structure. Each extracted
item is emitted as a separate chunk and has attached to it a meta variable that contains its
path within the source structure. The positional arguments to the command are patterns that can
be used to filter the extracted items by their path. To view only the paths of all chunks, use
the listing switch:</p>
<pre><code>emit something | xtnode --list
</code></pre>
<p>Otherwise, extracted items are written to the standard output port and usually require a frame
to properly process. In order to dump all extracted data to disk, the following pipeline can be
used:</p>
<pre><code>emit something | xtnode [| dump {path} ]
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/xtnode.py#L55-L227" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtnode(ArchiveUnit, docs=&#39;{0}{p}{PathExtractorUnit}&#39;):
    &#34;&#34;&#34;
    Extracts and decompiles files from compiled Node.Js applications. Supports both nexe and pkg, two
    utilities that are commonly used to generate stand-alone executables.
    &#34;&#34;&#34;

    _NEXE_SENTINEL = B&#39;&lt;nexe~~sentinel&gt;&#39;
    _PKG_PAYLOAD_P = B&#39;PAYLOAD_POSITION&#39;
    _PKG_PAYLOAD_S = B&#39;PAYLOAD_SIZE&#39;
    _PKG_PRELUDE_P = B&#39;PRELUDE_POSITION&#39;
    _PKG_PRELUDE_S = B&#39;PRELUDE_SIZE&#39;
    _PKG_COMMON_JS = B&#39;sourceMappingURL=common.js.map&#39;

    def __init__(
        self, *paths, entry: Param[bool, Arg.Switch(&#39;-u&#39;, help=&#39;Only extract the entry point.&#39;)] = False,
        list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False,
        path=b&#39;path&#39;, date=b&#39;date&#39;,
    ):
        super().__init__(*paths, entry=entry,
            list=list, join_path=join_path, drop_path=drop_path, fuzzy=fuzzy, exact=exact, regex=regex,
            path=path, date=date)

    def unpack(self, data: buf) -&gt; Iterable[UnpackResult]:
        if self._is_nexe(data):
            self.log_info(&#39;unpacking as nexe&#39;)
            yield from self._unpack_nexe(data)
            return
        if self._is_pkg(data):
            self.log_info(&#39;unpacking as pkg&#39;)
            yield from self._unpack_pkg(data)
            return

    def _unpack_nexe(self, data: buf):
        try:
            ep = re.compile(
                RB&#34;entry\s*=\s*path\.resolve\(path\.dirname\(process\.execPath\),\s*(%s)\)&#34; % formats.string)
            ep, = ep.finditer(data)
        except Exception:
            ep = None
            self.log_info(&#39;could not identify entry point&#39;)
        else:
            ep = ep.group(1) | esc(quoted=True) | str
            self.log_info(F&#39;entry point: {ep}&#39;)
        view = memoryview(data)
        for marker in re.finditer(re.escape(self._NEXE_SENTINEL), data):
            end = marker.end() + 16
            sizes = data[marker.end():end]
            if sizes.startswith(b&#34;&#39;)&#34;):
                continue
            reader = StructReader(sizes)
            code_size = int(reader.f64())
            blob_size = int(reader.f64())
            start = marker.start() - code_size - blob_size
            try:
                reader = StructReader(view[start:end])
                code = reader.read_exactly(code_size)
                blob = reader.read_exactly(blob_size)
            except EOFError:
                self.log_debug(F&#39;found marker at 0x{marker.start():X}, but failed to read data&#39;)
                continue
            else:
                self.log_debug(F&#39;found marker at 0x{marker.start():X}, data start at {start:X}&#39;)
            for rsrc in re.finditer(RB&#39;process\.__nexe\s*=&#39;, code):
                rsrc = JSONReader(code[rsrc.end():])
                rsrc = rsrc.read_json()
                if len(rsrc) == 1:
                    _, rsrc = rsrc.popitem()
                for path, (offset, length) in rsrc.items():
                    end = offset + length
                    if ep and self.args.entry and path != ep:
                        continue
                    yield UnpackResult(path, blob[offset:end])

    def _unpack_pkg(self, data: buf):
        def _extract_coordinates(*v: bytes):
            for name in v:
                pattern = name + BR&#39;&#39;&#39;\s{0,3}=\s{0,3}([&#39;&#34;])([\s\d]+)\1&#39;&#39;&#39;
                value, = re.finditer(pattern, data)
                yield int(value.group(2).decode(&#39;utf8&#39;).strip(), 0)

        def _extract_data(*v: bytes):
            try:
                offset, length = _extract_coordinates(*v)
            except Exception:
                return None
            return data[offset:offset + length]

        payload = _extract_data(self._PKG_PAYLOAD_P, self._PKG_PAYLOAD_S)
        if not payload:
            raise ValueError(&#39;unable to extract payload&#39;)
        prelude = _extract_data(self._PKG_PRELUDE_P, self._PKG_PRELUDE_S)
        if not prelude:
            raise ValueError(&#39;unable to extract prelude&#39;)
        mapping = re.search(re.escape(self._PKG_COMMON_JS) + BR&#39;\s*\},\s*\{&#39;, prelude)
        if not mapping:
            raise ValueError(&#39;unable to find common.js mapping&#39;)

        reader = JSONReader(prelude[mapping.end() - 1:])

        files: dict[str, dict] = reader.read_json()

        if files is None:
            raise ValueError(&#39;failed to read file list&#39;)

        entry = reader.skip_comma().read_string()
        links = reader.skip_comma().read_json()

        # _unknown1 = reader.skip_comma().read_json()
        # _unknown2 = reader.skip_comma().read_terminated_array(B&#39;)&#39;).strip()

        root = next(iter(files))
        skip = 0
        view = memoryview(payload)

        for k in range(len(root) + 1):
            test = root[:k].rstrip(&#39;/&#39;).rstrip(&#39;\\&#39;)
            if not all(path.startswith(test) for path in files):
                root = test[:-1]
                skip = k - 1
                break

        entry = entry[skip:]
        self.log_info(F&#39;detected root directory {root}, entry point is {entry}&#39;)

        for src, dst in links.items():
            new_files = {}
            self.log_info(&#39;link src:&#39;, src[skip:])
            self.log_info(&#39;link dst:&#39;, dst[skip:])
            for path, location in files.items():
                if not path.startswith(src):
                    continue
                new_path = dst + path[len(src):]
                new_files[new_path] = location
                self.log_debug(&#39;synthesizing linked file:&#39;, new_path)
            files.update(new_files)

        for path, location in files.items():
            path = path[skip:]
            if entry and self.args.entry and path != entry:
                continue
            data = None
            for kind, (offset, length) in location.items():
                stop = offset + length
                if kind == &#39;3&#39;:  # metadata
                    continue
                if kind == &#39;2&#39;:  # unknown
                    continue
                if kind in &#39;01&#39;:
                    data = view[offset:stop]
            if data is not None:
                yield UnpackResult(path, data)

    @classmethod
    def _is_nexe(cls, data: buf) -&gt; bool:
        return cls._NEXE_SENTINEL in data

    @classmethod
    def _is_pkg(cls, data: buf) -&gt; bool:
        if cls._PKG_PAYLOAD_P not in data:
            return False
        if cls._PKG_PAYLOAD_S not in data:
            return False
        if cls._PKG_PRELUDE_P not in data:
            return False
        if cls._PKG_PRELUDE_S not in data:
            return False
        if cls._PKG_COMMON_JS not in data:
            return False
        return True

    @classmethod
    def handles(cls, data: buf) -&gt; bool | None:
        return cls._is_nexe(data) or cls._is_pkg(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtnsis"><code class="flex name class">
<span>class <span class="ident">xtnsis</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from NSIS archives. This unit is a path extractor which extracts data from a hierarchical structure. Each extracted
item is emitted as a separate chunk and has attached to it a meta variable that contains its
path within the source structure. The positional arguments to the command are patterns that can
be used to filter the extracted items by their path. To view only the paths of all chunks, use
the listing switch:</p>
<pre><code>emit something | xtnsis --list
</code></pre>
<p>Otherwise, extracted items are written to the standard output port and usually require a frame
to properly process. In order to dump all extracted data to disk, the following pipeline can be
used:</p>
<pre><code>emit something | xtnsis [| dump {path} ]
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/xtnsis.py#L1283-L1364" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtnsis(ArchiveUnit, docs=&#39;{0}{s}{PathExtractorUnit}&#39;):
    &#34;&#34;&#34;
    Extract files from NSIS archives.
    &#34;&#34;&#34;

    @classmethod
    def _find_archive_offset(cls, data: bytearray, before: int = -1, flawmax=2):
        def signatures(*magics):
            for changes in range(flawmax + 1):
                for magic in magics:
                    if not changes:
                        yield 0, magic
                        continue
                    for positions in itertools.permutations(range(len(magic)), r=changes):
                        signature = bytearray(magic)
                        for p in positions:
                            signature[p] = 0x2E
                        yield changes, bytes(signature)
        best_guess = None
        search_space = memoryview(data)
        for flaws, sig in signatures(*NSArchive.MAGICS):
            if flaws &gt; 1:
                search_space = search_space[:0x20_000]
            matches = [m.start() - 4 for m in re.finditer(sig, search_space, flags=re.DOTALL)]
            if before &gt;= 0:
                matches = [match for match in matches if match &lt; before]
            matches.reverse()
            archive = None
            for match in matches:
                if match % 0x200 == 0:
                    archive = match
                    break
            if not archive:
                if matches and not best_guess:
                    best_guess = matches[-1]
            else:
                msg = F&#39;Archive signature was found at offset 0x{archive:X}&#39;
                if flaws &gt; 0:
                    msg = F&#39;{msg}; it has {flaws} imperfections and was likely modified&#39;
                cls.log_info(F&#39;{msg}.&#39;)
                return archive
        if best_guess:
            cls.log_info(F&#39;A signature was found at offset 0x{best_guess:08X}; it is not properly aligned.&#39;)
            return best_guess
        return None

    def unpack(self, data):
        memory = memoryview(data)
        before = -1
        _error = None
        while True:
            offset = self._find_archive_offset(data, before)
            if offset is None:
                _error = _error or ValueError(&#39;Unable to find an NSIS archive marker.&#39;)
                raise _error
            try:
                arc = NSArchive(memory[offset:])
            except Exception as e:
                _error = e
                before = offset
            else:
                break

        def info():
            yield F&#39;{arc.header.type.name} archive&#39;
            yield F&#39;compression type {arc.method.value}&#39;
            yield F&#39;mystery value 0x{arc.header.unknown_value:X}&#39;
            yield &#39;solid archive&#39; if arc.solid else &#39;fragmented archive&#39;
            yield &#39;64-bit header&#39; if arc.header.is64bit else &#39;32-bit header&#39;
            yield &#39;unicode&#39; if arc.header.unicode else &#39;ascii&#39;

        self.log_info(&#39;, &#39;.join(info()))

        for item in arc.header.items:
            yield self._pack(item.path, item.mtime, lambda i=item: arc._extract_item(i).data)

        yield self._pack(&#39;setup.bin&#39;, None, arc.header_data)
        yield self._pack(&#39;setup.nsis&#39;, None, arc.script.encode(self.codec))

    @classmethod
    def handles(cls, data) -&gt; bool:
        return any(buffer_contains(data, magic) for magic in NSArchive.MAGICS)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtnuitka"><code class="flex name class">
<span>class <span class="ident">xtnuitka</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts files packed by Nuitka using the &ndash;onefile option.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/xtnuitka.py#L10-L83" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtnuitka(PathExtractorUnit):
    &#34;&#34;&#34;
    Extracts files packed by Nuitka using the --onefile option.
    &#34;&#34;&#34;
    _MAGIC = B&#39;KA&#39;

    @PathExtractorUnit.Requires(&#39;pyzstd&#39;, [&#39;arc&#39;])
    def _pyzstd():
        import pyzstd
        return pyzstd

    def unpack(self, data: buf) -&gt; Iterable[UnpackResult]:
        class NuitkaData(Struct):
            unit = self

            def __init__(self, reader: StructReader):
                self.magic = reader.read_exactly(2)
                self.compression_flag = reader.read_exactly(1)
                if self.compressed:
                    zd = self.unit._pyzstd.ZstdDecompressor()
                    reader = StructReader(zd.decompress(reader.read()))
                self.files = {}
                self.truncated = False
                while not reader.eof:
                    path = reader.read_w_string(&#39;utf-16&#39;)
                    if not path:
                        break
                    size = reader.u64()
                    data = reader.read(size)
                    if len(data) == size:
                        self.files[path] = data
                    else:
                        self.truncated = True

            @property
            def compressed(self):
                return self.compression_flag == b&#39;Y&#39;

        if data.startswith(b&#39;MZ&#39;):
            arcs = list(self._pe_candidates(data))
        else:
            arcs = [data]

        for arc in arcs:
            archive = NuitkaData(arc)
            if archive.truncated:
                self.log_warn(&#39;the archive is truncated&#39;)
            if archive.magic != self._MAGIC:
                self.log_warn(&#39;the archive data does not start with the correct magic sequence&#39;)
            for path, data in archive.files.items():
                yield UnpackResult(path, data)

    @classmethod
    def handles(cls, data: buf) -&gt; bool | None:
        if data[:2] == b&#39;MZ&#39;:
            try:
                next(cls._pe_candidates(data))
            except StopIteration:
                return False
        else:
            return data[:2] == cls._MAGIC

    @classmethod
    def _pe_candidates(cls, data: buf):

        from refinery.units.formats.pe.peoverlay import peoverlay
        blob = data | peoverlay | bytearray
        if blob.startswith(cls._MAGIC):
            yield blob

        from refinery.units.formats.pe.perc import perc
        for blob in data | perc:
            if blob.startswith(cls._MAGIC):
                yield blob</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtone"><code class="flex name class">
<span>class <span class="ident">xtone</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract embedded files from OneNote documents.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/office/xtone.py#L10-L35" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtone(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract embedded files from OneNote documents.
    &#34;&#34;&#34;
    @PathExtractorUnit.Requires(&#39;pyonenote&#39;, [&#39;formats&#39;, &#39;office&#39;, &#39;extended&#39;])
    def _pyOneNote():
        import pyOneNote
        import pyOneNote.OneDocument
        return pyOneNote.OneDocument

    def unpack(self, data: bytearray):
        with MemoryFile(memoryview(data)) as stream:
            one = self._pyOneNote.OneDocment(stream)
        for guid, file in one.get_files().items():
            chunk = file[&#39;content&#39;]
            try:
                extension = file[&#39;extension&#39;]
            except KeyError:
                extension = F&#39;.{get_cached_file_magic_info(chunk).extension}&#39;
            yield UnpackResult(F&#39;{guid}{extension}&#39;, chunk)

    @classmethod
    def handles(cls, data) -&gt; bool | None:
        return re.search(
            br&#39;\xE4\x52\x5C\x7B\x8C\xD8\xA7\x4D\xAE\xB1\x53\x78\xD0\x29\x96\xD3&#39;, data
        ) is not None</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtp"><code class="flex name class">
<span>class <span class="ident">xtp</span></span>
<span>(</span><span>*pattern, filter=0, min=1, max=None, len=None, stripspace=False, duplicates=False, longest=False, take=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract Patterns: Uses regular expressions to extract indicators from the input data and
optionally filters these results heuristically. The unit is designed to extract indicators
such as domain names and IP addresses, see below for a complete list. To extract data
formats such as hex-encoded data, use <code><a title="refinery.carve" href="index.html#refinery.carve">carve</a></code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/pattern/xtp.py#L57-L427" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtp(PatternExtractor):
    &#34;&#34;&#34;
    Extract Patterns: Uses regular expressions to extract indicators from the input data and
    optionally filters these results heuristically. The unit is designed to extract indicators
    such as domain names and IP addresses, see below for a complete list. To extract data
    formats such as hex-encoded data, use `refinery.carve`.
    &#34;&#34;&#34;

    def __init__(
        self,
        *pattern: Param[str, Arg.String(&#39;pattern&#39;,
            default=(
                indicators.hostname.name,
                indicators.url.name,
                indicators.email.name,
            ), help=(
                &#39;Choose the pattern to extract. The unit uses {{default}} by default. Use an &#39;
                &#39;asterix character to select all available patterns. The available patterns &#39;
                &#39;are: {}&#39;.format(&#39;, &#39;.join(p.display for p in indicators))
            )
        )],
        filter: Param[int, Arg.Counts(&#39;-f&#39;, help=(
            &#39;If this setting is enabled, the xtp unit will attempt to reduce the number &#39;
            &#39;of false positives by certain crude heuristics. Specify multiple times to &#39;
            &#39;make the filtering more aggressive.&#39;)
        )] = 0,
        min=1, max=None, len=None, stripspace=False, duplicates=False, longest=False, take=None
    ):
        self.superinit(super(), **vars(), ascii=True, utf16=True)

        patterns = {
            p for name in pattern for p in indicators if fnmatch(p.display, name)
        }
        # if indicators.hostname in patterns:
        #     patterns.remove(indicators.hostname)
        #     patterns.add(indicators.ipv4)
        #     patterns.add(indicators.domain)
        patterns = [F&#39;(?P&lt;{p.name}&gt;{p.value})&#39; for p in patterns]
        if not patterns:
            raise RefineryCriticalException(&#39;The given mask does not match any known indicator pattern.&#39;)
        joined = &#39;|&#39;.join(patterns)
        self.args.pattern = re.compile(joined.encode(self.codec), flags=re.DOTALL)
        self.args.filter = filter

    _ALPHABETIC = ascii_letters.encode(&#39;ASCII&#39;)

    _LEGITIMATE_HOSTS = {
        &#39;acm.org&#39;                 : 1,
        &#39;adobe.com&#39;               : 1,
        &#39;aka.ms&#39;                  : 1,
        &#39;android.com&#39;             : 1,
        &#39;apache.org&#39;              : 1,
        &#39;apple.com&#39;               : 1,
        &#39;archive.org&#39;             : 2,
        &#39;azure.com&#39;               : 1,
        &#39;baidu.com&#39;               : 2,
        &#39;bootstrapcdn.com&#39;        : 2,
        &#39;cdnjs.cloudflare.com&#39;    : 4,
        &#39;comodo.net&#39;              : 1,
        &#39;comodoca.com&#39;            : 1,
        &#39;curl.haxx.se&#39;            : 1,
        &#39;curl.se&#39;                 : 1,
        &#39;digicert.com&#39;            : 1,
        &#39;dublincore.org&#39;          : 1,
        &#39;example.com&#39;             : 1,
        &#39;facebook.com&#39;            : 4,
        &#39;fontawesome.com&#39;         : 1,
        &#39;github.com&#39;              : 3,
        &#39;globalsign.com&#39;          : 1,
        &#39;globalsign.net&#39;          : 1,
        &#39;godaddy.com&#39;             : 1,
        &#39;golang.org&#39;              : 1,
        &#39;google.com&#39;              : 4,
        &#39;googleapis.com&#39;          : 5,
        &#39;googleusercontent.com&#39;   : 5,
        &#39;gov&#39;                     : 2,
        &#39;gstatic.com&#39;             : 2,
        &#39;iana.org&#39;                : 1,
        &#39;ietf.org&#39;                : 1,
        &#39;intel.com&#39;               : 1,
        &#39;jquery.com&#39;              : 1,
        &#39;jsdelivr.net&#39;            : 2,
        &#39;libssh.org&#39;              : 1,
        &#39;live.com&#39;                : 1,
        &#39;microsoft.com&#39;           : 1,
        &#39;mozilla.org&#39;             : 1,
        &#39;msdn.com&#39;                : 1,
        &#39;msn.com&#39;                 : 1,
        &#39;newtonsoft.com&#39;          : 3, # json.net
        &#39;nuget.org&#39;               : 3,
        &#39;office.com&#39;              : 1,
        &#39;office365.com&#39;           : 2,
        &#39;openssl.org&#39;             : 1,
        &#39;openssh.com&#39;             : 1,
        &#39;openxmlformats.org&#39;      : 1,
        &#39;oracle.com&#39;              : 1,
        &#39;purl.org&#39;                : 1,
        &#39;python.org&#39;              : 1,
        &#39;readthedocs.io&#39;          : 1,
        &#39;schema.org&#39;              : 2,
        &#39;sectigo.com&#39;             : 1,
        &#39;skype.com&#39;               : 1,
        &#39;sourceforge.net&#39;         : 4,
        &#39;stackoverflow.com&#39;       : 1,
        &#39;sun.com&#39;                 : 1,
        &#39;sway-cdn.com&#39;            : 1,
        &#39;sway-extensions.com&#39;     : 1,
        &#39;symantec.com&#39;            : 1,
        &#39;symauth.com&#39;             : 1,
        &#39;symcb.com&#39;               : 1,
        &#39;symcd.com&#39;               : 1,
        &#39;sysinternals.com&#39;        : 3,
        &#39;thawte.com&#39;              : 1,
        &#39;unicode.org&#39;             : 2,
        &#39;usertrust.com&#39;           : 1,
        &#39;verisign.com&#39;            : 1,
        &#39;w3.org&#39;                  : 1,
        &#39;wikipedia.org&#39;           : 1,
        &#39;wolfram.com&#39;             : 1,
        &#39;xml.org&#39;                 : 1,
        &#39;xmlsoap.org&#39;             : 1,
        &#39;yahoo.com&#39;               : 1,
    }

    for _ext in [
        &#39;build&#39;,
        &#39;data&#39;,
        &#39;do&#39;,
        &#39;help&#39;,
        &#39;java&#39;,
        &#39;md&#39;,
        &#39;mov&#39;,
        &#39;name&#39;,
        &#39;py&#39;,
        &#39;so&#39;,
        &#39;sys&#39;,
        &#39;zip&#39;,
    ]:
        _LEGITIMATE_HOSTS[_ext] = 4

    _DOMAIN_WHITELIST = {
        &#39;system.net&#39;,
        &#39;wscript.shell&#39;,
    }

    _BRACKETING = {
        B&#34;&#39;&#34;[0]: B&#34;&#39;&#34;,
        B&#39;&#34;&#39;[0]: B&#39;&#34;&#39;,
        B&#39;(&#39;[0]: B&#39;)&#39;,
        B&#39;{&#39;[0]: B&#39;}&#39;,
        B&#39;[&#39;[0]: B&#39;]&#39;,
        B&#39;&lt;&#39;[0]: B&#39;&gt;&#39;,
    }

    def _check_host(self, host: str, text: str):
        hl = host.lower()
        if hl in self._DOMAIN_WHITELIST:
            self.log_info(F&#39;excluding indicator because domain {hl} is forcefully ignored: {text}&#39;)
            return False
        for white, level in self._LEGITIMATE_HOSTS.items():
            if self.args.filter &gt;= level and (hl == white or hl.endswith(F&#39;.{white}&#39;)):
                self.log_info(F&#39;excluding indicator because domain {hl} is whitelisted: {text}&#39;, clip=True)
                self.log_debug(F&#39;reduce level below {level} to allow, current level is {self.args.filter}&#39;)
                return False
        return True

    def _check_match(self, data: memoryview | bytearray, pos: int, name: str, value: bytes):
        term = self._BRACKETING.get(data[pos - 1], None)
        text = value.decode(self.codec)
        if term:
            pos = value.find(term)
            if pos &gt; 0:
                value = value[:pos]
        if not self.args.filter:
            return value
        if name == indicators.hostname.name:
            if all(part.isdigit() for part in value.split(B&#39;.&#39;)):
                name = indicators.ipv4.name
            elif B&#39;.&#39; not in value:
                name = indicators.ipv6.name
            else:
                name = indicators.domain.name
        if name == indicators.ipv4.name:
            ocets = [int(x) for x in value.split(B&#39;.&#39;)]
            if ocets.count(0) &gt;= 3:
                self.log_info(F&#39;excluding ipv4 because it contains many zeros: {text}&#39;)
                return None
            if self.args.filter &gt; 2 and sum(ocets) &lt; 10:
                self.log_info(F&#39;excluding ipv4 because of low value ocets: {text}&#39;)
                return None
            if ocets[0] &lt;= 5 * self.args.filter:
                for area in (
                    bytes(data[pos - 20 : pos + 20]),
                    bytes(data[pos * 2 - 40 : pos * 2 + 40 : 2]),
                    bytes(data[pos * 2 - 41 : pos * 2 + 39 : 2]),
                ):
                    check = area.lower()
                    if B&#39;version&#39; in check or b&#39;build&#39; in check:
                        self.log_info(F&#39;excluding ipv4 because it might be a version: {text}&#39;)
                        return None
            small_ocet_count = sum(1 for ocet in ocets if ocet &lt; 10)
            if small_ocet_count &gt; max(0, 4 - self.args.filter):
                self.log_info(F&#39;excluding ipv4 because it has too many small ocets: {text}&#39;)
                return None
            ip = ip_address(text)
            if not ip.is_global:
                if self.args.filter &gt;= 3 or not ip.is_private:
                    self.log_info(F&#39;excluding ipv4 because it is not global: {text}&#39;)
                    return None
        elif name in {
            indicators.url.name,
            indicators.socket.name,
            indicators.hostname.name,
            indicators.domain.name,
            indicators.subdomain.name
        }:
            if self.args.filter &gt;= 2:
                if LetterWeights.IOC(value) &lt; 0.6:
                    self.log_info(F&#39;excluding indicator because with low score: {text}&#39;, clip=True)
                    return None
                if name != indicators.url.name and len(value) &gt; 0x100:
                    self.log_info(F&#39;excluding indicator because it is too long: {text}&#39;, clip=True)
                    return None
            ioc = text
            if &#39;://&#39; not in ioc:
                ioc = F&#39;tcp://{ioc}&#39;
            parts = urlparse(ioc)
            host, _, _ = parts.netloc.partition(&#39;:&#39;)
            if not self._check_host(host, text):
                return None
            if name == indicators.url.name:
                scheme = parts.scheme.lower()
                for p in (&#39;http&#39;, &#39;https&#39;, &#39;ftp&#39;, &#39;file&#39;, &#39;mailto&#39;):
                    if scheme.endswith(p):
                        pos = scheme.find(p)
                        value = value[pos:]
                        break
            if name in {
                indicators.hostname.name,
                indicators.domain.name,
                indicators.subdomain.name
            }:
                if data[pos - 1] in b&#39;/\\&#39; and self.args.filter &gt;= 2:
                    return None
                hostparts = host.split(&#39;.&#39;)
                if self.args.filter &gt;= 2:
                    if not all(p.isdigit() for p in hostparts) and all(len(p) &lt; 4 for p in hostparts):
                        self.log_info(F&#39;excluding host with too many short parts: {text}&#39;)
                        return None
                if self.args.filter &gt;= 3:
                    if len(hostparts) &lt;= sum(3 for p in hostparts if p != p.lower() and p != p.upper()):
                        self.log_info(F&#39;excluding host with too many mixed case parts: {text}&#39;)
                        return None
                # These heuristics attempt to filter out member access to variables in
                # scripts which can be mistaken for domains because of the TLD inflation
                # we&#39;ve had.
                uppercase = sum(1 for c in host if c.isalpha() and c.upper() == c)
                lowercase = sum(1 for c in host if c.isalpha() and c.lower() == c)
                if lowercase and uppercase:
                    caseratio = uppercase / lowercase
                    if 0.1 &lt; caseratio &lt; 0.9:
                        self.log_info(F&#39;excluding indicator with too much uppercase letters: {text}&#39;)
                        return None
                if all(x.isidentifier() for x in hostparts):
                    if len(hostparts) == 2 and hostparts[0] in (&#39;this&#39;, &#39;self&#39;):
                        self.log_info(F&#39;excluding host that looks like a code snippet: {text}&#39;)
                        return None
                    if len(hostparts[-2]) &lt; 3:
                        self.log_info(F&#39;excluding host with too short root domain name: {text}&#39;)
                        return None
                    if any(x.startswith(&#39;_&#39;) for x in hostparts):
                        self.log_info(F&#39;excluding host with underscores: {text}&#39;)
                        return None
                    if len(hostparts[-1]) &gt; 3:
                        prefix = &#39;.&#39;.join(hostparts[:-1])
                        seen_before = len(set(re.findall(
                            fR&#39;{prefix}(?:\.\w+)+&#39;.encode(&#39;ascii&#39;), data)))
                        if seen_before &gt; 2:
                            self.log_debug(F&#39;excluding indicator that was already seen: {text}&#39;)
                            return None
        elif name == indicators.email.name:
            _, _, host = value.partition(B&#39;@&#39;)
            host = host.decode(self.codec)
            if not self._check_host(host, text):
                return None
            at = value.find(B&#39;@&#39;)
            ix = 0
            while value[ix] not in self._ALPHABETIC:
                ix += 1
            return None if at - ix &lt; 3 else value[ix:]
        elif name in (
            indicators.path.name,
            indicators.winpath.name,
            indicators.nixpath.name,
        ):
            if len(value) &lt; 8:
                self.log_info(F&#39;excluding path because it is too short: {text}&#39;)
                return None
            if len(value) &gt; 16 and len(re.findall(RB&#39;\\x\d\d&#39;, value)) &gt; len(value) // 10:
                self.log_info(F&#39;excluding long path containign hex: {text}&#39;, clip=True)
                return None
            try:
                path_string = text
            except Exception:
                self.log_debug(F&#39;excluding path which did not decode: {value!r}&#39;, clip=True)
                return None
            try:
                path = Path(path_string)
            except Exception as E:
                self.log_debug(F&#39;error parsing path &#34;{path}&#34;: {E!s}&#39;)
                return None
            path_likeness = sum(v for v, x in [
                (1, path.suffix),
                (1, path_string.startswith(&#39;/&#39;)),
                (2, path_string.startswith(&#39;%&#39;)),
                (2, path_string.startswith(&#39;\\\\&#39;)),
                (2, path_string[1:3] == &#39;:\\&#39;),
            ] if x)
            if 2 + path_likeness &lt; min(self.args.filter, 2):
                self.log_info(F&#39;excluding long path because it has no characteristic parts: {text}&#39;)
                return None
            bad_parts = 0
            all_parts = len(path.parts)
            if self.args.filter &gt;= 1:
                date_likeness = sum(1
                    for t in [&#39;yyyy&#39;, &#39;yy&#39;, &#39;mm&#39;, &#39;dd&#39;, &#39;hh&#39;, &#39;ss&#39;]
                    if t in path.parts or t.upper() in path.parts)
                if len(value) &lt; 20 and date_likeness &gt;= all_parts - 1:
                    self.log_info(F&#39;excluding path that looks like a date format: {text}&#39;, clip=True)
                    return None
            if self.args.filter &gt;= 2:
                for k, part in enumerate(path.parts):
                    if not k:
                        drive, colon, slash = part.partition(&#39;:&#39;)
                        if colon and len(drive) == 1 and len(slash) &lt;= 1:
                            continue
                        if part[0] == part[~0] == &#39;%&#39;:
                            continue
                        if len(part) == 1:
                            continue
                    if (
                        LetterWeights.Path(part) &lt; 0.5 + (min(self.args.filter, 4) * 0.1)
                        or (self.args.filter &gt;= 2 and LetterWeights.Path(part[:1]) &lt; 0.5)
                    ):
                        bad_parts += 1
                        self.log_debug(F&#39;bad part {k + 1} in path: {part}&#39;)
            for filter_limit in (2, 3, 4):
                bad_ratio = 2 ** (filter_limit - 1)
                if self.args.filter &gt;= filter_limit and bad_parts * bad_ratio &gt;= all_parts:
                    self.log_info(F&#39;excluding path with bad parts: {text}&#39;, clip=True)
                    return None
        return value

    def process(self, data):
        whitelist = set()

        def check(match: re.Match):
            for name, value in match.groupdict().items():
                if value is not None:
                    break
            else:
                raise RefineryCriticalException(&#39;Received empty match.&#39;)
            if value in whitelist:
                return None
            result = self._check_match(match.string, match.start(), name, value)
            if result is not None:
                return self.labelled(result, pattern=name)
            whitelist.add(value)

        transforms = [check]
        yield from self.matches_filtered(memoryview(data), self.args.pattern, *transforms)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtpdf"><code class="flex name class">
<span>class <span class="ident">xtpdf</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract objects from PDF documents.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/pdf.py#L24-L242" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtpdf(ArchiveUnit):
    &#34;&#34;&#34;
    Extract objects from PDF documents.
    &#34;&#34;&#34;
    # @ArchiveUnit.Requires(&#39;pypdf&gt;=3.1.0&#39;)
    # def _pypdf2():
    #     import pypdf
    #     import pypdf.generic
    #     return pypdf

    @ArchiveUnit.Requires(&#39;pikepdf&lt;=9.5&#39;, [&#39;formats&#39;, &#39;default&#39;, &#39;extended&#39;])
    def _pikepdf():
        import pikepdf
        return pikepdf

    @ArchiveUnit.Requires(&#39;pymupdf&#39;, [&#39;formats&#39;, &#39;default&#39;, &#39;extended&#39;])
    def _mupdf():
        import os
        for setting in (&#39;PYMUPDF_MESSAGE&#39;, &#39;PYMUPDF_LOG&#39;):
            os.environ[setting] = F&#39;path:{os.devnull}&#39;
        import pymupdf
        return pymupdf

    def _walk_pypdf2(self, blob, memo: set[int] | None = None, *path):
        lib = self._pypdf2

        while isinstance(blob, lib.generic.IndirectObject):
            try:
                blob = blob.get_object()
            except Exception:
                break
        if memo is None:
            memo = {id(blob)}
        elif id(blob) in memo:
            return
        else:
            memo.add(id(blob))
        try:
            name = blob[&#39;/F&#39;]
            blob = blob[&#39;/EF&#39;][&#39;/F&#39;]
        except Exception:
            pass
        else:
            def unhex(match):
                return bytes.fromhex(match[1]).decode(&#39;latin1&#39;)
            name = re.sub(&#39;#([0-9a-fA-F]{2})&#39;, unhex, name)
            path = *path[:-1], F&#39;/{name}&#39;
        try:
            def extract():
                with NoLogging():
                    return get_data()
            if TYPE_CHECKING:
                blob = cast(EncodedStreamObject, blob)
            get_data = blob.get_data
        except AttributeError:
            pass
        else:
            yield UnpackResult(&#39;&#39;.join(path), extract, kind=&#39;object&#39;)
            return

        if isinstance(blob, lib.generic.ByteStringObject):
            yield UnpackResult(&#39;&#39;.join(path), blob, kind=&#39;bytes&#39;)
            return
        if isinstance(blob, lib.generic.TextStringObject):
            yield UnpackResult(&#39;&#39;.join(path), blob.encode(self.codec), kind=&#39;string&#39;)
            return

        if isinstance(blob, (
            lib.generic.BooleanObject,
            lib.generic.ByteStringObject,
            lib.generic.FloatObject,
            lib.generic.NameObject,
            lib.generic.NullObject,
            lib.generic.NumberObject,
            lib.generic.RectangleObject,
        )):
            # unhandled PDF objects
            return

        if isinstance(blob, lib.generic.TreeObject):
            blob = list(blob)

        pdf = lib.generic.PdfObject

        if isinstance(blob, list):
            if (
                len(blob) % 2 == 0
                and all(isinstance(key, str) for key in islice(iter(blob), 0, None, 2))
                and all(isinstance(key, pdf) for key in islice(iter(blob), 1, None, 2))
            ):
                blob = dict(zip(*([iter(blob)] * 2)))
            else:
                for key, value in enumerate(blob):
                    yield from self._walk_pypdf2(value, memo, *path, F&#39;/{key}&#39;)
                return

        if not isdict(blob):
            return

        assert isinstance(blob, dict)

        for key, value in blob.items():
            if not isinstance(key, str):
                continue
            if not key.startswith(&#39;/&#39;):
                key = F&#39;/{key}&#39;
            yield from self._walk_pypdf2(value, memo, *path, key)

    def _walk_pike(self, blob: Object, memo: list[Object] | None = None, *keys):
        if memo is None:
            memo = [blob]
        elif blob in memo:
            return
        else:
            memo.append(blob)

        try:
            name = blob[&#39;/F&#39;]
            blob = blob[&#39;/EF&#39;][&#39;/F&#39;]
        except Exception:
            pass
        else:
            def unhex(match):
                return bytes.fromhex(match[1]).decode(&#39;latin1&#39;)
            name = re.sub(&#39;#([0-9a-fA-F]{2})&#39;, unhex, str(name))
            keys = *keys, F&#39;/{name}&#39;

        pike = self._pikepdf
        meta = {}
        path = &#39;&#39;.join(keys)
        done = set()

        if isinstance(blob, pike.Dictionary):
            nested = {}
            for key, value in blob.items():
                if isinstance(value, pike.Name):
                    value = str(value)
                if isinstance(value, (int, float, str, bool)):
                    key = key.lstrip(&#39;/&#39;)
                    meta[key] = value
                    continue
                nested[key] = value
                done.add(key)
            for key, value in nested.items():
                yield from self._walk_pike(value, memo, *keys, key)
            if meta:
                yield UnpackResult(path, blob.to_json(dereference=True))
                return
        elif isinstance(blob, pike.Array):
            for key, value in enumerate(iter(blob)):
                if isinstance(value, pike.Object):
                    yield from self._walk_pike(value, memo, *keys, F&#39;/{key}&#39;)
            return

        try:
            buffer = blob.get_stream_buffer()
        except Exception:
            try:
                buffer = blob.get_raw_stream_buffer()
            except Exception:
                buffer = None
        if buffer or buffer:
            yield UnpackResult(path, bytearray(buffer))
        elif isinstance(blob, pike.String):
            yield UnpackResult(path, bytes(blob))
        elif isinstance(blob, pike.Object):
            yield UnpackResult(path, blob.to_json())

    def unpack(self, data):
        try:
            mu = self._mupdf.open(stream=data, filetype=&#39;pdf&#39;)
        except Exception:
            mu = password = None
        else:
            if password := self.args.pwd or None:
                if mu.is_encrypted:
                    mu.authenticate(password)
                else:
                    self.log_warn(&#39;This PDF document is not protected; ignoring password argument.&#39;)
                    password = &#39;&#39;
            elif mu.is_encrypted:
                raise ValueError(&#39;This PDF is password protected.&#39;)

        with MemoryFile(data, output=bytes) as stream, NoLogging():
            try:
                pdf = self._pikepdf.open(stream, password=(password or &#39;&#39;))
                yield from self._walk_pike(pdf.trailer, None, &#39;raw&#39;)
            except Exception:
                raise
                pdf = self._pypdf2.PdfReader(stream, password=password)
                yield from self._walk_pypdf2(pdf.trailer, None, &#39;raw&#39;)

        if mu is None:
            return

        if (md := mu.metadata) and (md := {k: v for k, v in md.items() if v}):
            md = json.dumps(md, indent=4)
            yield UnpackResult(&#39;parsed/meta.json&#39;, md.encode(self.codec))

        for k in range(len(mu)):
            with NoLogging(NoLogging.Mode.ALL):
                try:
                    page: Page = mu[k]
                    text = page.get_textpage()
                except Exception:
                    continue
            yield UnpackResult(F&#39;parsed/page{k}.html&#39;, text.extractHTML().encode(self.codec))
            yield UnpackResult(F&#39;parsed/page{k}.json&#39;, text.extractJSON().encode(self.codec))
            yield UnpackResult(F&#39;parsed/page{k}.txt&#39;, text.extractText().encode(self.codec))
            for j, image in enumerate(page.get_images(), 1):
                xref = image[0]
                base = mu.extract_image(xref)
                data = base[&#39;image&#39;]
                info = get_cached_file_magic_info(data)
                yield UnpackResult(F&#39;parsed/page{k}/img{j}.{info.extension}&#39;, data)

    @classmethod
    def handles(cls, data) -&gt; bool | None:
        return data[:5] == B&#39;%PDF-&#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtpyi"><code class="flex name class">
<span>class <span class="ident">xtpyi</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', decompile=False, user_code=False, unmarshal=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts and decompiles files from a Python Installer (aka PyInstaller) archive. This unit is a path extractor which extracts data from a hierarchical structure. Each extracted
item is emitted as a separate chunk and has attached to it a meta variable that contains its
path within the source structure. The positional arguments to the command are patterns that can
be used to filter the extracted items by their path. To view only the paths of all chunks, use
the listing switch:</p>
<pre><code>emit something | xtpyi --list
</code></pre>
<p>Otherwise, extracted items are written to the standard output port and usually require a frame
to properly process. In order to dump all extracted data to disk, the following pipeline can be
used:</p>
<pre><code>emit something | xtpyi [| dump {path} ]
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/xtpyi.py#L376-L434" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtpyi(ArchiveUnit, docs=&#39;{0}{s}{PathExtractorUnit}&#39;):
    &#34;&#34;&#34;
    Extracts and decompiles files from a Python Installer (aka PyInstaller) archive.
    &#34;&#34;&#34;
    def __init__(
        self, *paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False,
        path=b&#39;path&#39;, date=b&#39;date&#39;,
        decompile: Param[bool, Arg.Switch(&#39;-c&#39;, help=&#39;Attempt to decompile PYC files.&#39;)] = False,
        user_code: Param[bool, Arg.Switch(&#39;-u&#39;, group=&#39;FILTER&#39;, help=(
            &#39;Extract only source code files from the root of the archive. These usually implement &#39;
            &#39;the actual domain logic. This implies the --decompile option.&#39;))] = False,
        unmarshal: Param[int, Arg(&#39;-y&#39;, action=&#39;count&#39;, group=&#39;FILTER&#39;, help=(
            &#39;(DANGEROUS) Unmarshal embedded PYZ archives. Warning: Maliciously crafted packages can &#39;
            &#39;potentially exploit this to execute code. It is advised to only use this option inside &#39;
            &#39;an isolated environment. Specify twice to decompile unmarshalled Python bytecode.&#39;
        ))] = 0
    ):
        super().__init__(
            *paths,
            list=list,
            join_path=join_path,
            drop_path=drop_path,
            fuzzy=fuzzy,
            exact=exact,
            regex=regex,
            path=path,
            date=date,
            decompile=decompile,
            unmarshal=unmarshal,
            user_code=user_code,
        )

    def unpack(self, data):
        view = memoryview(data)
        positions = [m.start() for m in re.finditer(re.escape(PyInstallerArchiveEpilogue.MagicSignature), view)]
        mode = Unmarshal(min(2, int(self.args.unmarshal)))
        self.log_debug(F&#39;unmarshal mode: {mode.name}&#39;)
        if not positions:
            raise LookupError(&#39;unable to find PyInstaller signature&#39;)
        if len(positions) &gt; 2:
            # first position is expected to be the sentinel value in the unpacker stub
            width = max(len(F&#39;{p:X}&#39;) for p in positions)
            for position in positions:
                self.log_info(F&#39;magic signature found at offset 0x{position:0{width}X}&#39;)
            self.log_warn(F&#39;found {len(positions) - 1} potential PyInstaller epilogue markers; using last one.&#39;)
        decompile = self.args.decompile
        uc_target = PiType.USERCODE if decompile else PiType.SOURCE
        archive = PyInstallerArchiveEpilogue(view, positions[-1], mode, decompile)
        for name, file in archive.files.items():
            if self.args.user_code:
                if file.type != uc_target:
                    continue
                if name.startswith(&#39;pyiboot&#39;):
                    continue
            yield self._pack(name, None, file.data, type=file.type.name)

    @classmethod
    def handles(cls, data: buf) -&gt; bool | None:
        return PyInstallerArchiveEpilogue.MagicSignature in data</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtrtf"><code class="flex name class">
<span>class <span class="ident">xtrtf</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract embedded objects in RTF documents.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/office/xtrtf.py#L11-L58" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtrtf(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract embedded objects in RTF documents.
    &#34;&#34;&#34;
    @PathExtractorUnit.Requires(&#39;oletools&#39;, [&#39;formats&#39;, &#39;office&#39;, &#39;extended&#39;])
    def _oletools():
        import oletools
        import oletools.oleobj
        import oletools.rtfobj
        return oletools

    def unpack(self, data):
        parser = self._oletools.rtfobj.RtfObjParser(data)
        parser.parse()
        width = len(str(len(parser.objects)))
        for k, item in enumerate(parser.objects):
            item: RtfObject
            path = item.filename or F&#39;carve{k:0{width}}.bin&#39;
            data = item.rawdata
            meta = {}
            if item.is_ole:
                if item.format_id == self._oletools.oleobj.OleObject.TYPE_EMBEDDED:
                    meta[&#39;ole_type&#39;] = &#39;EMBEDDED&#39;
                elif item.format_id == self._oletools.oleobj.OleObject.TYPE_LINKED:
                    meta[&#39;ole_type&#39;] = &#39;LINKED&#39;
                if item.is_package:
                    meta[&#39;src_path&#39;] = item.src_path
                    meta[&#39;tmp_path&#39;] = item.temp_path
                if item.clsid is not None:
                    meta[&#39;ole_info&#39;] = item.clsid_desc
                    meta[&#39;ole_guid&#39;] = item.clsid
                meta[&#39;ole_name&#39;] = item.class_name
            if item.oledata:
                data = item.oledata
                pos = item.rawdata.find(data)
                if pos &gt; 0:
                    meta[&#39;raw_header&#39;] = item.rawdata[:pos]
                if item.olepkgdata:
                    data = item.olepkgdata
                    pos = item.oledata.find(data)
                    if pos &gt;= 0:
                        meta[&#39;ole_header&#39;] = item.oledata[:pos]
            yield UnpackResult(path, data, **meta)

    @classmethod
    def handles(cls, data) -&gt; bool:
        import re
        return bool(re.search(bR&#39;^\s{0,500}\{\\rtf&#39;, memoryview(data)[:505]))</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtsim"><code class="flex name class">
<span>class <span class="ident">xtsim</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from Smart Install Maker (SIM) executables. This unit is a path extractor which extracts data from a hierarchical structure. Each extracted
item is emitted as a separate chunk and has attached to it a meta variable that contains its
path within the source structure. The positional arguments to the command are patterns that can
be used to filter the extracted items by their path. To view only the paths of all chunks, use
the listing switch:</p>
<pre><code>emit something | xtsim --list
</code></pre>
<p>Otherwise, extracted items are written to the standard output port and usually require a frame
to properly process. In order to dump all extracted data to disk, the following pipeline can be
used:</p>
<pre><code>emit something | xtsim [| dump {path} ]
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/xtsim.py#L56-L314" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtsim(ArchiveUnit, docs=&#39;{0}{s}{PathExtractorUnit}&#39;):
    &#34;&#34;&#34;
    Extract files from Smart Install Maker (SIM) executables.
    &#34;&#34;&#34;

    _RUNTIME_MAPPING = {
        &#39;4.tmp&#39;  : &#39;header.png&#39;,
        &#39;5.tmp&#39;  : &#39;wizard.bmp&#39;,
        &#39;6.tmp&#39;  : &#39;background.bmp&#39;,
        &#39;7.tmp&#39;  : &#39;folder.png&#39;,
        &#39;8.tmp&#39;  : &#39;group.png&#39;,
        &#39;9.tmp&#39;  : &#39;password.png&#39;,
        &#39;15.tmp&#39; : &#39;license1.rtf&#39;,
        &#39;16.tmp&#39; : &#39;information.rtf&#39;,
        &#39;20.tmp&#39; : &#39;license2.rtf&#39;,
    }

    _DIRECTORY_MASKS = {
        &#39;@$&amp;%01&#39;: &#39;ProgramFiles&#39;,
        &#39;@$&amp;%02&#39;: &#39;WindowsDir&#39;,
        &#39;@$&amp;%03&#39;: &#39;SystemDir&#39;,
        &#39;@$&amp;%04&#39;: &#39;InstallPath&#39;,
        &#39;@$&amp;%05&#39;: &#39;TempDir&#39;,
        &#39;@$&amp;%06&#39;: &#39;Desktop&#39;,
        &#39;@$&amp;%07&#39;: &#39;QuickLaunch&#39;,
        &#39;@$&amp;%08&#39;: &#39;ProgramsDir&#39;,
        &#39;@$&amp;%09&#39;: &#39;StartMenu&#39;,
        &#39;@$&amp;%10&#39;: &#39;MyDocuments&#39;,
        &#39;@$&amp;%11&#39;: &#39;Favorites&#39;,
        &#39;@$&amp;%12&#39;: &#39;SendTo&#39;,
        &#39;@$&amp;%13&#39;: &#39;UserProfile&#39;,
        &#39;@$&amp;%14&#39;: &#39;StartUp&#39;,
        &#39;@$&amp;%15&#39;: &#39;FontsDir&#39;,
        &#39;@$&amp;%16&#39;: &#39;CommonFiles&#39;,
        &#39;@$&amp;%17&#39;: &#39;SystemDrive&#39;,
        &#39;@$&amp;%18&#39;: &#39;CurrentDirectory&#39;,
        &#39;@$&amp;%20&#39;: &#39;UserName&#39;,
        &#39;@$&amp;%21&#39;: &#39;Language&#39;,
        &#39;@$&amp;%22&#39;: &#39;ComputerName&#39;,
        &#39;@$&amp;%26&#39;: &#39;AppData&#39;,
        &#39;@$&amp;%27&#39;: &#39;CommonAppData&#39;,
        &#39;@$&amp;%28&#39;: &#39;CommonDesktop&#39;,
        &#39;@$&amp;%29&#39;: &#39;CommonDocuments&#39;,
        &#39;@$&amp;%30&#39;: &#39;CommonFavourites&#39;,
        &#39;@$&amp;%31&#39;: &#39;CommonPrograms&#39;,
        &#39;@$&amp;%32&#39;: &#39;CommonStartMenu&#39;,
        &#39;@$&amp;%33&#39;: &#39;CommonStartup&#39;,
        &#39;@$&amp;%34&#39;: &#39;Templates&#39;,
        &#39;@$&amp;%35&#39;: &#39;CommonTemplates&#39;,
        &#39;@$&amp;%36&#39;: &#39;ProgramFiles64&#39;,
    }

    def unpack(self, data):
        mem = memoryview(data)
        sim = self.get_offsets(data)

        if sim is None:
            return B&#39;&#39;

        strings = StructReader(mem[sim.strings_offset:sim.runtime_offset])
        runtime = StructReader(mem[sim.runtime_offset:sim.content_offset])
        content = StructReader(mem[sim.content_offset:sim.archive_end])

        header = [strings.read_c_string() for _ in range(sim.nr_of_strings)]
        tables: dict[str, list[list[str | int]]] = {}
        unknown_tables: dict[str, list[list[str | int]]] = {}

        def sc(k: int):
            return int(header[k])

        for size, index, name in [
            (4, 98, None),
            (7, 50, &#39;registry&#39;),    # (2=HKLM/1=HKCU,key)
            (3, 96, None),
            (2, 31, &#39;fonts&#39;),
            (8, 54, &#39;shortcuts&#39;),   # (?,0=Menu/1=Desktop,filename,target_path,comment,icon_path1,icon_path2)
            (3, 67, &#39;filenames&#39;),
            (2, 93, None),
            (6, 40, &#39;install&#39;),     #
            (6, 25, &#39;uninstall&#39;),
            (6, 24, &#39;ini&#39;),         # 34991da998ece07d4a941394c6630ce74955fb4800e5915f6766180d12a8dc61
            (2, 45, None),
            (2, 20, None),
            (4, 26, &#39;languages&#39;),
        ]:
            count = sc(index)
            if not count:
                continue
            table = [[
                strings.read_c_string() for _ in range(size)
            ] for _ in range(count)]
            if name is None:
                unknown_tables[F&#39;T{index}&#39;] = table
            else:
                tables[name] = table

        unknown_marker = strings.read_c_string()

        language_count = sc(26)
        message_matrix = [[
            strings.read_c_string() for _ in range(sc(57))
        ] for _ in range(language_count)]

        len_chunks = sc(117)
        chunk_size = sc(95)
        chunk_rest = sc(118)

        def check_empty_reader(r: StructReader, name: str):
            if _c := r.remaining_bytes:
                self.log_warn(F&#39;{name} reader had 0x{_c:08X} bytes remaining:&#39;, r.peek(), clip=True)

        check_empty_reader(strings, &#39;strings&#39;)

        lngid = tables[&#39;languages&#39;][0]
        if not lngid[2].isdigit():
            lname: bytes = lngid[1]
            lname = lname.decode(&#39;latin1&#39;)
            lngid = max(LCID, key=lambda k: longest_common_substring(LCID[k], lname))
        else:
            lngid = int(lngid[2])

        codec = DEFAULT_CODEPAGE.get(lngid, &#39;latin1&#39;)

        def decode(cell: bytes, codec: str):
            try:
                cell = cell.decode(codec)
            except UnicodeDecodeError:
                cell = cell.decode(&#39;latin1&#39;)
                self.log_debug(&#39;failed to decode string:&#39;, cell, clip=True)
            if cell.isdigit():
                return int(cell)
            if not cell:
                return None
            for key, val in self._DIRECTORY_MASKS.items():
                cell = cell.replace(key, F&#39;${val}&#39;)
            return cell

        header[:] = [decode(s, codec) for s in header]

        for t in (tables, unknown_tables):
            for name, table in t.items():
                for row in table:
                    row[:] = [decode(cell, codec) for cell in row]

        messages = {}

        for array, lng in zip(message_matrix, tables[&#39;languages&#39;]):
            lng_codec = DEFAULT_CODEPAGE.get(lng[2], &#39;latin1&#39;)
            messages[lng[1]] = [decode(cell, lng_codec) for cell in array]

        tables[&#39;messages&#39;] = messages
        tables[&#39;header&#39;] = header

        if unknown_tables:
            tables[&#39;unknown_tables&#39;] = unknown_tables
        if unknown_marker:
            tables[&#39;unknown_marker&#39;] = decode(unknown_marker, codec)

        yield self._pack(&#39;setup.json&#39;, None,
            json.dumps(tables, indent=4).encode(self.codec))

        def runtime_path(name: str):
            root, backslash, temp = name.rpartition(&#39;\\&#39;)
            if backslash and root == &#39;$inst&#39; and (t := self._RUNTIME_MAPPING.get(temp)):
                name = t
            return F&#39;runtime/{name}&#39;

        if sim.runtime_is_cab:
            runtime_cab = Cabinet(runtime.read(), no_magic=True)
            for file in runtime_cab.process().get_files():
                yield self._pack(runtime_path(file.name), file.timestamp, lambda f=file: f.decompress())
        else:
            for _ in range(sim.nr_of_runtime):
                name = decode(runtime.read_c_string(), codec)
                path = runtime_path(name)
                size = int(runtime.read_c_string())
                yield self._pack(path, None, runtime.read(size))
            check_empty_reader(runtime, &#39;runtime&#39;)

        def no_abs_path(p: str):
            drive, d, rest = p.partition(&#39;:\\&#39;)
            if d and len(drive) == 1:
                return F&#39;$Drive{drive.upper()}\\{rest}&#39;
            return p

        if len_chunks + chunk_rest == 0:
            for file in tables[&#39;filenames&#39;]:
                path = no_abs_path(file[1])
                content.u32() # unknown
                size = content.u32()
                content.u32() # unknown
                content.u32() # unknown
                content.u32() # unknown
                content.u32() # unknown
                yield self._pack(F&#39;data/{path}&#39;, None, content.read(size))
        else:
            content_cab = Cabinet(no_magic=True)
            content_cab.extend(content.read(chunk_size) for _ in range(len_chunks))
            if chunk_rest &gt; 0:
                content_cab.append(content.read(chunk_rest))
            for file in content_cab.process().get_files():
                try:
                    path = tables[&#39;filenames&#39;][int(file.name)][1]
                except Exception:
                    path = file.name
                path = F&#39;content/{no_abs_path(path)}&#39;
                yield self._pack(path, file.timestamp, lambda f=file: f.decompress())

        check_empty_reader(content, &#39;content&#39;)

    @classmethod
    def get_offsets(cls, data: bytes | bytearray) -&gt; SIMOffsets | None:
        if len(data) &lt; 0x1000:
            return None

        def sane(offsets: SIMOffsets):
            if offsets.sim_signature != _SIGBYTE:
                return False
            for offset in (
                offsets.strings_offset,
                offsets.runtime_offset,
                offsets.content_offset,
            ):
                if offset not in range(0x1000, 0x100000000):
                    return False
            if offsets.strings_offset &gt;= offsets.runtime_offset:
                return False
            return offsets.content_offset &gt;= offsets.runtime_offset + offsets.runtime_length

        end = len(data) - 0x24
        offsets = SIMOffsets(end, *struct.unpack(&#39;&lt;QQQQ?BBB&#39;, data[end:]))

        if sane(offsets):
            pos = offsets.strings_offset
            end = pos + len(_SIMNAME)
            if data[pos:end] == _SIMNAME:
                return offsets
            pos = data.rfind(_SIMNAME)
            if pos &gt; 0:
                return offsets.rebase(pos)

        view = memoryview(data)

        for stub in re.finditer(rb&#39;MZ.{78}This program must be run under Win&#39;, data):
            pos_zero = stub.start()
            pos_data = data.find(_SIMNAME, pos_zero)
            if pos_data &lt; 0:
                continue
            pattern = re.escape((pos_data - pos_zero).to_bytes(8, &#39;little&#39;)) + B&#39;.{27}\\xF1&#39;
            if match := re.search(pattern, view[pos_zero:]):
                end = match.start()
                offsets = SIMOffsets(end, *struct.unpack(&#39;&lt;QQQQ?BBB&#39;, match[0]))
                if sane(offsets):
                    return offsets.rebase(pos_zero)

    @classmethod
    def handles(cls, data) -&gt; bool | None:
        if isinstance(data, (bytes, bytearray)):
            return cls.get_offsets(data) is not None</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtsql"><code class="flex name class">
<span>class <span class="ident">xtsql</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from SQLite3 databases.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/xtsql.py#L11-L75" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtsql(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract files from SQLite3 databases.
    &#34;&#34;&#34;
    def unpack(self, data: bytearray):
        def _json(object):
            with BytesAsStringEncoder as encoder:
                return encoder.dumps(object).encode(self.codec)

        if sys.version_info[:2] &lt; (3, 11):
            raise NotImplementedError(F&#39;python 3.11 is required to use {self.__class__.__name__}.&#39;)

        database = sqlite3.connect(&#39;:memory:&#39;)
        database.text_factory = bytes
        database.deserialize(data)
        cursor = database.cursor()
        result: dict[str, list[dict[str, int | float | str | bytes]]] = {}

        listing: list[tuple[bytes, bytes]] = cursor.execute(
            &#34;SELECT name, sql FROM sqlite_master WHERE type=&#39;table&#39;;&#34;).fetchall()

        for tbl, spec in listing:
            table = tbl.decode(&#39;utf8&#39;)
            result[table] = t = []
            ct, _tbl, names = spec.partition(tbl)
            ct = ct.rstrip(B&#39;&#34;&#39;)
            names = names.lstrip(B&#39;&#34;&#39;)
            names = names.strip()
            names, _, _ = names.rpartition(B&#39;)&#39;)
            if (
                tbl != _tbl
                or ct.strip().upper().split() != [B&#39;CREATE&#39;, B&#39;TABLE&#39;]
                or not names.startswith(B&#39;(&#39;)
            ):
                raise ValueError(F&#39;Unexpeted SQL statement for {table} in master table: {spec}&#39;)
            names = [next(iter(name.strip().split()))
                for name in names[1:-1].decode().split(&#39;,&#39;)]
            for row in cursor.execute(F&#39;SELECT * FROM {table}&#39;).fetchall():
                t.append(dict(zip(names, row)))

        yield UnpackResult(&#39;db&#39;, functools.partial(_json, result))

        for table, rows in result.items():

            yield UnpackResult(F&#39;db/{table}&#39;, functools.partial(_json, rows))

            for k, row in enumerate(rows):

                root = F&#39;db/{table}/{k}&#39;
                yield UnpackResult(root, functools.partial(_json, row))

                for name, value in row.items():
                    path = F&#39;{root}/{name}&#39;
                    if value is None:
                        continue
                    if isinstance(value, (int, float)):
                        value = str(value)
                    if isinstance(value, str):
                        value = value.encode(self.codec)
                    if isinstance(value, bytes):
                        yield UnpackResult(path, value)

    @classmethod
    def handles(cls, data):
        return memoryview(data)[:15] == B&#39;SQLite format 3&#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.xttar"><code class="flex name class">
<span>class <span class="ident">xttar</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from a Tar archive. This unit is a path extractor which extracts data from a hierarchical structure. Each extracted
item is emitted as a separate chunk and has attached to it a meta variable that contains its
path within the source structure. The positional arguments to the command are patterns that can
be used to filter the extracted items by their path. To view only the paths of all chunks, use
the listing switch:</p>
<pre><code>emit something | xttar --list
</code></pre>
<p>Otherwise, extracted items are written to the standard output port and usually require a frame
to properly process. In order to dump all extracted data to disk, the following pipeline can be
used:</p>
<pre><code>emit something | xttar [| dump {path} ]
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/xttar.py#L10-L35" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xttar(ArchiveUnit, docs=&#39;{0}{s}{PathExtractorUnit}&#39;):
    &#34;&#34;&#34;
    Extract files from a Tar archive.
    &#34;&#34;&#34;
    def unpack(self, data: bytearray):
        with MemoryFile(data) as stream:
            try:
                archive = tarfile.open(fileobj=stream)
            except Exception:
                ustar = data.find(B&#39;ustar&#39;)
                if ustar &lt; 257:
                    raise
                stream.seek(ustar - 257)
                archive = tarfile.open(fileobj=stream)
            for info in archive.getmembers():
                if not info.isfile():
                    continue
                extractor = archive.extractfile(info)
                if extractor is None:
                    continue
                date = datetime.datetime.fromtimestamp(info.mtime)
                yield self._pack(info.name, date, lambda e=extractor: e.read())

    @classmethod
    def handles(cls, data) -&gt; bool:
        return data[257:262] == B&#39;ustar&#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtvba"><code class="flex name class">
<span>class <span class="ident">xtvba</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract VBA macro code from Office documents.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/office/xtvba.py#L10-L44" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtvba(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract VBA macro code from Office documents.
    &#34;&#34;&#34;
    @PathExtractorUnit.Requires(&#39;oletools&#39;, [&#39;formats&#39;, &#39;office&#39;, &#39;extended&#39;])
    def _olevba():
        with NoLogging(NoLogging.Mode.ALL):
            import oletools.olevba
            return oletools.olevba

    def unpack(self, data):
        sentinel = str(uuid4())
        try:
            parser = self._olevba.VBA_Parser(sentinel, data=bytes(data), relaxed=True)
        except self._olevba.FileOpenError:
            raise ValueError(&#39;Input data not recognized by VBA parser&#39;)
        for p1, stream_path, p2, code in parser.extract_all_macros():
            code: str
            if not stream_path:
                if p1 == sentinel:
                    continue
                if p2 == sentinel:
                    continue
            yield UnpackResult(stream_path, code.encode(self.codec))

    @classmethod
    def handles(cls, data):
        if data[:8] == b&#39;\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1&#39;:
            return True
        if data[:2] == B&#39;PK&#39;:
            return buffer_contains(data, B&#39;xl/vbaProject.bin&#39;)
        return any(buffer_contains(data, ns) for ns in [
            b&#39;http://schemas.microsoft.com/office/word/2003/wordml&#39;,
            b&#39;http://schemas.microsoft.com/office/2006/xmlPackage&#39;,
        ])</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtw"><code class="flex name class">
<span>class <span class="ident">xtw</span></span>
<span>(</span><span>stripspace=False, duplicates=False, longest=False, take=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract Wallets: Extracts anything that looks like a cryptocurrency wallet address.
This works similar to the <code><a title="refinery.xtp" href="index.html#refinery.xtp">xtp</a></code> unit.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/pattern/xtw.py#L10-L31" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtw(PatternExtractor):
    &#34;&#34;&#34;
    Extract Wallets: Extracts anything that looks like a cryptocurrency wallet address.
    This works similar to the `refinery.xtp` unit.
    &#34;&#34;&#34;

    def __init__(self, stripspace=False, duplicates=False, longest=False, take=None):
        self.superinit(super(), **vars(), ascii=True, utf16=True)

    def process(self, data):
        pattern = &#39;|&#39;.join(FR&#39;(?P&lt;{p.name}&gt;\b{p.value}\b)&#39; for p in wallets)
        pattern = FR&#39;\b{pattern}\b&#39;.encode(&#39;latin1&#39;)

        def check(match: re.Match[bytes]):
            for name, value in match.groupdict().items():
                if value is not None:
                    break
            else:
                raise RefineryCriticalException(&#39;Received empty match.&#39;)
            return self.labelled(value, kind=name)

        yield from self.matches_filtered(memoryview(data), pattern, check)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtxml"><code class="flex name class">
<span>class <span class="ident">xtxml</span></span>
<span>(</span><span>*paths, format=None, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract values from an XML document.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/xml.py#L11-L45" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtxml(XMLToPathExtractorUnit):
    &#34;&#34;&#34;
    Extract values from an XML document.
    &#34;&#34;&#34;
    def unpack(self, data):
        root = xml.parse(data.strip())
        meta = metavars(data)
        path = self._make_path_builder(meta, root)

        def walk(node: xml.XMLNode, *parts: str):
            def extract(node: xml.XMLNode = node):
                if not node.children:
                    return node.content.encode(self.codec)
                with MemoryFile() as stream:
                    node.write(stream)
                    return bytes(stream.getvalue() | ppxml)

            attributes = {
                self._normalize_key(k): self._normalize_val(v)
                for k, v in node.attributes.items()
            }

            if not all(is_valid_variable_name(k) for k in attributes):
                attributes = {F&#39;_{k}&#39;: v for k, v in attributes.items()}

            yield UnpackResult(&#39;/&#39;.join(parts), extract, **attributes)

            for child in node.children:
                yield from walk(child, *parts, path(child))

        yield from walk(root, path(root))

    @classmethod
    def handles(cls, data):
        return is_likely_xml(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtxs"><code class="flex name class">
<span>class <span class="ident">xtxs</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract data from Microsoft Access Databases.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/office/xtxs.py#L8-L51" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtxs(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract data from Microsoft Access Databases.
    &#34;&#34;&#34;

    @PathExtractorUnit.Requires(&#39;access-parser&#39;, [&#39;formats&#39;, &#39;office&#39;, &#39;extended&#39;])
    def _access_parser():
        import access_parser
        return access_parser

    def unpack(self, data):

        with VirtualFileSystem() as vfs:
            file = vfs.new(data, &#39;accdb&#39;)
            xsdb = self._access_parser.AccessParser(file.path)

        for name in xsdb.catalog:
            with NoLogging():
                table = xsdb.parse_table(name)
            if not table:
                continue
            length = max(len(cells) for cells in table.values())
            for k in range(length):
                for header, column in table.items():
                    try:
                        entry = column[k]
                    except IndexError:
                        continue
                    if entry is None:
                        continue

                    if isinstance(entry, (int, float)):
                        entry = str(entry)
                    if isinstance(entry, str):
                        entry = entry.encode(self.codec)
                    if isinstance(entry, bytes):
                        yield UnpackResult(F&#39;{name}/{k}/{header}&#39;, entry)

    @classmethod
    def handles(cls, data) -&gt; bool | None:
        if data[:19] == b&#39;\0\01\0\0Standard ACE DB&#39;:
            return True
        if data[:19] == b&#39;\0\01\0\0Standard Jet DB&#39;:
            return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtzip"><code class="flex name class">
<span>class <span class="ident">xtzip</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from a Zip archive. This unit is a path extractor which extracts data from a hierarchical structure. Each extracted
item is emitted as a separate chunk and has attached to it a meta variable that contains its
path within the source structure. The positional arguments to the command are patterns that can
be used to filter the extracted items by their path. To view only the paths of all chunks, use
the listing switch:</p>
<pre><code>emit something | xtzip --list
</code></pre>
<p>Otherwise, extracted items are written to the standard output port and usually require a frame
to properly process. In order to dump all extracted data to disk, the following pipeline can be
used:</p>
<pre><code>emit something | xtzip [| dump {path} ]
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/xtzip.py#L37-L162" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtzip(ArchiveUnit, docs=&#39;{0}{s}{PathExtractorUnit}&#39;):
    &#34;&#34;&#34;
    Extract files from a Zip archive.
    &#34;&#34;&#34;
    @ArchiveUnit.Requires(&#39;chardet&#39;, [&#39;default&#39;, &#39;extended&#39;])
    def _chardet():
        import chardet
        return chardet

    @ArchiveUnit.Requires(&#39;pyzipper&#39;, [&#39;arc&#39;, &#39;default&#39;, &#39;extended&#39;])
    def _pyzipper():
        import pyzipper
        return pyzipper

    @classmethod
    def _carver(cls):
        return carve_zip

    def unpack(self, data: bytearray):
        from zipfile import BadZipFile, ZipFile, ZipInfo

        def password_invalid(password: bytes | None):
            nonlocal archive, fallback
            if password:
                archive.setpassword(password)
            try:
                archive.testzip()
                files = (t for t in archive.infolist() if t.filename and not t.is_dir())
                files = sorted(files, key=lambda info: info.file_size)
                for info in files:
                    self.log_debug(&#39;testing password against:&#39;, info.filename)
                    try:
                        with archive.open(info.filename, &#34;r&#34;) as test:
                            while test.read(1024):
                                pass
                    except BadZipFile:
                        continue
                    else:
                        break
            except NotImplementedError:
                if fallback:
                    raise
                self.log_debug(&#39;compression method unsupported, switching to pyzipper&#39;)
                archive = self._pyzipper.AESZipFile(MemoryFile(data))
                fallback = True
                return password_invalid(password)
            except RuntimeError as E:
                if &#39;password&#39; not in str(E):
                    raise
                return True
            else:
                if password:
                    self.log_debug(&#39;using password:&#39;, password)
                return False

        password = bytes(self.args.pwd)
        fallback = False
        archive = ZipFile(MemoryFile(data))
        passwords = [password]

        if not password:
            passwords.extend(p.encode(self.codec) for p in self._COMMON_PASSWORDS)
        for p in passwords:
            if not password_invalid(p):
                break

        for info in archive.infolist():
            def xt(archive: ZipFile = archive, info: ZipInfo = info, data=memoryview(data)):
                try:
                    return archive.read(info.filename)
                except RuntimeError as E:
                    if &#39;password&#39; not in str(E):
                        raise
                    msg = &#39;invalid password; use -L to extract raw encrypted data&#39;
                    rec = _FileRecord(data[info.header_offset:])
                    raise RefineryPartialResult(msg, rec.data) from E

            if info.filename:
                if info.is_dir():
                    continue

            # courtesy of https://stackoverflow.com/a/37773438/9130824
            filename = info.filename
            if info.flag_bits &amp; ZIP_FILENAME_UTF8_FLAG == 0:
                filename_bytes = filename.encode(&#39;437&#39;)
                try:
                    guessed_encoding = self._chardet.detect(filename_bytes)[&#39;encoding&#39;]
                except ImportError:
                    guessed_encoding = None
                guessed_encoding = guessed_encoding or &#39;cp1252&#39;
                filename = filename_bytes.decode(guessed_encoding, &#39;replace&#39;)

            try:
                date = datetime(*info.date_time)
            except Exception as e:
                self.log_info(F&#39;{e!s} - unable to determine date from tuple {info.date_time} for: {filename}&#39;)
                date = None

            yield self._pack(filename, date, xt)

    @classmethod
    def handles(cls, data):
        if data[:4] in (
            B&#39;PK\x03\x04&#39;,
            B&#39;PK\x07\x08&#39;,
        ):
            return True
        if not is_likely_pe(data):
            return False
        memory = memoryview(data)
        if 0 &lt;= buffer_offset(memory[-0x400:], ZipEndOfCentralDirectory.SIGNATURE):
            return True
        pe = lief.load_pe_fast(data)
        offset = get_pe_size(pe)
        if 0 &lt;= buffer_offset(memory[offset:], B&#39;PK\x03\x04&#39;) &lt; 0x1000:
            return True
        if not pe.has_debug:
            return False
        for entry in pe.debug:
            if not isinstance(entry, lief.PE.CodeViewPDB):
                continue
            path = entry.filename
            if not isinstance(path, str):
                path = codecs.decode(path, &#39;latin1&#39;)
            if &#39;sfxzip32&#39; in path and &#39;WinRAR&#39; in path:
                return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtzpaq"><code class="flex name class">
<span>class <span class="ident">xtzpaq</span></span>
<span>(</span><span>*paths, index=False, pwd=b'', date=b'date', path=b'path', regex=False, exact=False, fuzzy=0, drop_path=False, join_path=False, list=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from a ZPAQ archive. This unit is a path extractor which extracts data from a hierarchical structure. Each extracted
item is emitted as a separate chunk and has attached to it a meta variable that contains its
path within the source structure. The positional arguments to the command are patterns that can
be used to filter the extracted items by their path. To view only the paths of all chunks, use
the listing switch:</p>
<pre><code>emit something | xtzpaq --list
</code></pre>
<p>Otherwise, extracted items are written to the standard output port and usually require a frame
to properly process. In order to dump all extracted data to disk, the following pipeline can be
used:</p>
<pre><code>emit something | xtzpaq [| dump {path} ]
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/formats/archive/xtzpaq.py#L1221-L1493" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtzpaq(ArchiveUnit, docs=&#39;{0}{s}{PathExtractorUnit}&#39;):
    &#34;&#34;&#34;
    Extract files from a ZPAQ archive.
    &#34;&#34;&#34;

    _MAGIC = B&#39;7kSt\xA01\x83\xD3\x8C\xB2\x28\xB0\xD3zPQ&#39;

    def __init__(
        self, *paths,
        index: Param[bool, Arg.Switch(&#39;-i&#39;, help=&#39;Archive is an index (no d-blocks).&#39;)] = False,
        **more
    ):
        for _code, _size in {
            _TCU32: 4,
            _TCI32: 4,
            _TCU16: 2,
            _TCI16: 2,
        }.items():
            _item_size = array(_code).itemsize
            if _item_size == _size:
                continue
            raise RuntimeError(
                F&#39;Expected array type &#34;{_code}&#34; to have entries of size {_size}, but the API &#39;
                F&#39;reports a size of {_item_size}.&#39;)

        super().__init__(*paths, index=index, **more)

    @classmethod
    def handles(cls, data) -&gt; bool | None:
        return data[:len(cls._MAGIC)] == cls._MAGIC

    def unpack(self, data: bytearray):
        def mkdate(date) -&gt; datetime:
            date = int(date)
            year = date // 1000000 // 10000
            month = date // 100000000 % 100
            day = date // 1000000 % 100
            hour = date // 10000 % 100
            minute = date // 100 % 100
            second = date % 100
            return datetime(year, month, day, hour, minute, second, 0)

        @dataclass
        class DT:
            date: int = 0
            attr: int = 0
            name: str = &#34;&#34;
            frag: list[int] = field(default_factory=list)

            @property
            def dt(self) -&gt; datetime | None:
                if self.date &gt; 0:
                    return mkdate(self.date)

        # TODO: implement password-protected archives
        # key = self.args.pwd
        index = self.args.index
        bsize: dict[int, int] = {}  # frag ID -&gt; d block compressed size
        dt: dict[str, DT] = {}      # filename -&gt; date, attr, frags
        frag: list[bytes] = []      # ID -&gt; hash[20] size[4] data
        csize = 0                   # expected offset of next non d block
        streaming = False
        journaling = False

        done = False
        dc = Decompressor(data)
        src = dc.dec.src
        offset = 0

        while not done and dc.read_block():
            while not done:
                filename = dc.read_filename()
                if filename is None:
                    break
                self.log_info(&#39;reading file&#39;, filename)
                comment = dc.read_comment()
                jsize = 0
                if comment and len(comment) &gt;= 4 and comment[-4:] == &#34;jDC\x01&#34;:
                    num = re.search(&#39;^\\d+&#39;, comment)
                    if not num:
                        raise RuntimeError(&#39;missing size in comment&#39;)
                    jsize = int(num[0])
                    if streaming:
                        raise RuntimeError(&#39;journaling block after streaming one&#39;)
                    journaling = True
                    self.log_info(&#39;archive type is journaling&#39;)
                else:
                    if journaling:
                        raise RuntimeError(&#39;streaming block after journaling one&#39;)
                    if index:
                        raise RuntimeError(&#39;streaming block in index&#39;)
                    streaming = True
                    self.log_info(&#39;archive type is streaming&#39;)

                # Test journaling filename. The format must be
                # jDC[YYYYMMDDHHMMSS][t][NNNNNNNNNN]
                # where YYYYMMDDHHMMSS is the date, t is the type {c,d,h,i}, and
                # NNNNNNNNNN is the 10 digit first fragment ID for types c,d,h.
                # They must be in ascending lexicographical order.

                frag_id = 0
                block_type = None

                if journaling:
                    if len(filename) != 28:
                        raise RuntimeError(&#39;filename size not 28&#39;)
                    if filename[:3] != &#39;jDC&#39;:
                        raise RuntimeError(&#39;filename not jDC&#39;)
                    block_type = filename[17]
                    if block_type not in &#39;cdhi&#39;:
                        raise RuntimeError(&#39;type not c,d,h,i&#39;)
                    try:
                        mkdate(filename[3:17])
                    except Exception as E:
                        raise RuntimeError(&#39;invalid date&#39;) from E
                    frag_id = int(filename[18:28])
                    if not 1 &lt;= frag_id &lt;= 4294967295:
                        raise RuntimeError(&#39;fragment ID out of range&#39;)

                seg = MemoryFile(size_limit=jsize)
                dc.set_output(seg)
                sha1 = hashlib.sha1()
                dc.set_hasher(sha1)
                dc.decompress_data()

                if journaling and len(seg) != jsize:
                    raise RuntimeError(&#39;incomplete output&#39;)

                checksum = dc.read_segment_end()
                if checksum is None:
                    self.log_debug(&#39;no checksum&#39;)
                elif checksum != sha1.digest():
                    raise RuntimeError(&#39;SHA1 mismatch&#39;)

                # check csize at first non-d block
                if csize and block_type and block_type in &#39;chi&#39;:
                    if csize != offset:
                        raise RuntimeError(F&#39;csize={csize} does not point to offset={offset}&#39;)
                    csize = 0

                # get csize from c block
                seglen = len(seg)
                seg = StructReader(seg.getvalue())
                if block_type == &#39;c&#39;:
                    if seglen &lt; 8:
                        raise RuntimeError(&#34;c block too small&#34;)
                    csize = seg.u64()
                    offset = src.tell() + 1
                    self.log_debug(F&#39;csize={csize} at offset={offset}&#39;)
                    if csize &gt;&gt; 63:
                        self.log_warn(&#39;incomplete transaction at end of archive&#39;)
                        done = True
                    elif index and csize != 0:
                        raise RuntimeError(&#39;nonzero csize in index&#39;)
                    # Set csize to expected offset of first non d block
                    # assuming 1 more byte for unread end of block marker.
                    csize += offset

                if block_type == &#39;d&#39;:
                    if index:
                        raise RuntimeError(&#39;d block in index&#39;)
                    bsize[frag_id] = src.tell() + 1 - offset  # compressed size
                    self.log_debug(F&#39; {bsize[frag_id]} -&gt; {len(seg)}&#39;)
                    # Test frag size list at end. The format is f[id..id+n-1] fid n
                    # where fid may be id or 0. sizes must sum to the rest of block.
                    if seglen &lt; 8:
                        raise RuntimeError(&#39;d block too small&#39;)
                    seg.seekset(-8)
                    fid = seg.u32() or frag_id
                    n = seg.u32()
                    if fid != frag_id:
                        raise RuntimeError(&#39;missing ID&#39;)
                    if n &gt; (seglen - 8) // 4:
                        raise RuntimeError(&#39;frag list too big&#39;)
                    fragsum = 0  # computed sum of frag sizes
                    seg.seekset(-4 * (n + 2))
                    for _ in range(n):
                        fragsum += seg.u32()
                    if fragsum + n * 4 + 8 != seglen:
                        raise RuntimeError(&#39;bad frag size list&#39;)
                    # Save frag hashes and sizes. For output, save data too.
                    seg.seekset(fragsum)
                    buffer = seg.getvalue()
                    assert seg.remaining_bytes == n * 4 + 8
                    for i in range(n):
                        while len(frag) &lt;= frag_id + i:
                            frag.append(B&#39;&#39;)
                        if frag[frag_id + i]:
                            raise RuntimeError(&#39;duplicate frag ID&#39;)
                        f = seg.u32()
                        h = hashlib.sha1(buffer[:f]).digest()
                        frag[frag_id + i] = h + f.to_bytes(4, &#39;little&#39;) + buffer[:f]
                        buffer = buffer[f:]

                    assert len(buffer) == n * 4 + 8
                    assert seg.remaining_bytes == 8

                # Test and save h block. Format is: bsize (sha1[20] size)...
                # where bsize is the compressed size of the d block with the same id,
                # and each size corresonds to a fragment in that block. The list
                # must match the list in the d block if present.

                if block_type == &#39;h&#39;:
                    if seglen % 24 != 4:
                        raise RuntimeError(&#39;bad h block size&#39;)
                    b = seg.u32()
                    self.log_debug(F&#39;[{frag_id}..{frag_id + seglen // 24}[ {b}&#39;)
                    fragsum = 0 # uncompressed size of all frags
                    for i in range(seglen // 24):
                        fd = seg.read(24)
                        if index:
                            while len(frag) &lt;= frag_id + i:
                                frag.append(B&#39;&#39;)
                            if frag[frag_id + i]:
                                raise RuntimeError(&#39;data in index&#39;)
                            frag[frag_id + i] = fd
                        elif frag_id + i &gt;= len(frag) or len(frag[frag_id + i]) &lt; 24:
                            raise RuntimeError(&#39;no matching d block&#39;)
                        elif frag[frag_id + i][:24] != fd:
                            raise RuntimeError(&#39;frag size or hash mismatch&#39;)
                        fragsum += int.from_bytes(fd[20:24], &#39;little&#39;)

                # Test i blocks and save files to extract. Format is:
                #   date filename 0 na attr[0..na) ni ptr[0..ni)   (to update)
                #   0    filename                                  (to delete)
                # Date is 64 bits in YYYYMMDDHHMMSS format.

                if block_type == &#39;i&#39;:
                    while not seg.eof:
                        f = DT(seg.u64())
                        f.name = seg.read_c_string(&#39;utf8&#39;)
                        if f.date &gt; 0:
                            na = seg.u32()
                            if na &gt; 65535:
                                raise ValueError(&#39;attr size &gt; 65535&#39;)
                            f.attr = seg.read_integer(na * 8)
                            ni = seg.u32()
                            for i in range(ni):
                                a = seg.u32()
                                f.frag.append(a)
                                if index:
                                    continue
                                elif not 1 &lt;= a &lt; len(frag):
                                    raise RuntimeError(&#39;frag ID out of range&#39;)
                                elif not frag[a]:
                                    raise LookupError(&#39;missing frag data&#39;)
                        dt[f.name] = f

                if streaming:
                    yield self._pack(filename, None, seg.getvalue())

            offset = src.tell()

        self.log_debug(F&#39;{offset} bytes of archive tested&#39;)

        if not journaling:
            return

        for name, f in dt.items():
            if not f.date:
                continue
            size = sum(
                int.from_bytes(frag[fp][20:24], &#39;little&#39;)
                for fp in f.frag
                if 0 &lt; fp &lt; len(frag) and len(frag[fp]) &gt;= 24
            )
            out = MemoryFile()
            for fp in f.frag:
                if fp &lt; len(frag):
                    out.write(memoryview(frag[fp])[24:])
            if len(out) != size:
                self.log_warn(&#39;invalid size during unpacking&#39;)
            yield self._pack(name, f.dt, out.getvalue())</code></pre>
</details>
</dd>
<dt id="refinery.shell.xxh"><code class="flex name class">
<span>class <span class="ident">xxh</span></span>
<span>(</span><span>seed=0, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Implements the xxHash hashing algorithm.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/hash/xxhash.py#L8-L20" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xxh(HashUnit):
    &#34;&#34;&#34;
    Implements the xxHash hashing algorithm.
    &#34;&#34;&#34;
    def __init__(
        self,
        seed: Param[int, Arg.Number(metavar=&#39;seed&#39;, help=&#39;specify the seed value; the default is {default}&#39;)] = 0,
        text=False
    ):
        super().__init__(text, seed=seed)

    def _algorithm(self, data):
        return xxhash(data, self.args.seed).digest()</code></pre>
</details>
</dd>
<dt id="refinery.shell.xxtea"><code class="flex name class">
<span>class <span class="ident">xxtea</span></span>
<span>(</span><span>key, iv=b'', padding=None, mode=None, raw=False, swap=False, block_size=1)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/crypto/cipher/xxtea.py#L53-L85" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xxtea(TEAUnit, cipher=BlockCipherFactory(XXTEA)):

    block_size: int = 4

    def __init__(
        self, key, iv=b&#39;&#39;, padding=None, mode=None, raw=False, swap=False,
        block_size: Param[int, Arg.Number(&#39;-b&#39;, help=(
            &#39;Cipher block size in 32-bit words. The default value {default} implies that the input &#39;
            &#39;is treated as a single block, which is common behaviour of many implementations.&#39;))] = 1
    ):
        super().__init__(
            key, iv=iv, padding=padding, mode=mode, raw=raw, swap=swap, block_size=block_size)

    def _prepare_block(self, data: bytes):
        if self.args.block_size &lt;= 1:
            blocks, remainder = divmod(len(data), 4)
            if remainder:
                blocks += 1
            self.block_size = blocks * 4
        else:
            self.block_size = self.args.block_size * 4

    def encrypt(self, data: bytes) -&gt; bytes:
        self._prepare_block(data)
        return super().encrypt(data)

    def decrypt(self, data: bytes) -&gt; bytes:
        self._prepare_block(data)
        return super().decrypt(data)

    def _new_cipher(self, **optionals) -&gt; CipherInterface:
        return StandardBlockCipherUnit._new_cipher(self,
            big_endian=self.args.swap, block_size=self.block_size, **optionals)</code></pre>
</details>
</dd>
<dt id="refinery.shell.z85"><code class="flex name class">
<span>class <span class="ident">z85</span></span>
</code></dt>
<dd>
<section class="desc"><p>Z85 encoding and decoding, an alternative variant of Base85 with a different alphabet.
This variant derives its name from the developer, ZeroMQ.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/encoding/z85.py#L23-L37" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class z85(Unit):
    &#34;&#34;&#34;
    Z85 encoding and decoding, an alternative variant of Base85 with a different alphabet.
    This variant derives its name from the developer, ZeroMQ.
    &#34;&#34;&#34;
    def reverse(self, data):
        return base64.b85encode(data).translate(_z85_encode_translation)

    def process(self, data: bytearray):
        return base64.b85decode(data.translate(_z85_decode_translation))

    @classmethod
    def handles(cls, data):
        from refinery.lib.patterns import formats
        return formats.z85s.value.bin.fullmatch(data) is not None</code></pre>
</details>
</dd>
<dt id="refinery.shell.zl"><code class="flex name class">
<span>class <span class="ident">zl</span></span>
<span>(</span><span>level=9, window=15, zlib_header=False, gzip_header=False)</span>
</code></dt>
<dd>
<section class="desc"><p>ZLib compression and decompression.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/zl.py#L11-L106" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class zl(Unit):
    &#34;&#34;&#34;
    ZLib compression and decompression.
    &#34;&#34;&#34;

    def __init__(
        self,
        level: Param[int, Arg.Number(&#39;-l&#39;, bound=(0, 0X9), help=&#39;Specify a compression level between 0 and 9.&#39;)] = 9,
        window: Param[int, Arg.Number(&#39;-w&#39;, bound=(8, 0XF), help=&#39;Manually specify the window size between 8 and 15.&#39;)] = 15,
        zlib_header: Param[bool, Arg.Switch(&#39;-z&#39;, group=&#39;MODE&#39;, help=&#39;Use a ZLIB header.&#39;)] = False,
        gzip_header: Param[bool, Arg.Switch(&#39;-g&#39;, group=&#39;MODE&#39;, help=&#39;Use a GZIP header.&#39;)] = False
    ):
        if zlib_header and gzip_header:
            raise ValueError(&#39;You can only specify one header type (ZLIB or GZIP).&#39;)
        return super().__init__(level=level, window=window, zlib_header=zlib_header, gzip_header=gzip_header)

    def _decompress_data(self, data, mode: int, step: int):
        zl = zlib.decompressobj(mode)
        memory = memoryview(data)
        result = bytearray()
        while not zl.eof:
            read = min(step, len(memory))
            try:
                chunk = zl.decompress(memory[:read])
            except zlib.error as e:
                if not result:
                    raise
                raise RefineryPartialResult(exception_to_string(e), result) from e
            else:
                result.extend(chunk)
                consumed = read - len(zl.unused_data)
                if not memory or consumed == 0:
                    break
                memory = memory[consumed:]
        return result, memory

    def process(self, data):
        if data[0] == 0x78 or data[0:2] == B&#39;\x1F\x8B&#39; or self.args.zlib_header or self.args.gzip_header:
            modes = [self.args.window | 0x20, -self.args.window]
        else:
            modes = [-self.args.window, self.args.window | 0x20]
        modes.extend([0x10 | self.args.window, 0])
        view = memoryview(data)
        rest = view
        step = 32 if self.leniency &gt; 0 else len(data)
        count = 0
        error = None
        for k in itertools.count(1):
            error = None
            for mode in modes:
                msg = F&#39;decompressing chunk {k} with mode {mode &amp; 0xFF:02X}&#39;
                try:
                    out, rest = self._decompress_data(view, mode, step)
                    yield out
                except Exception as e:
                    self.log_info(F&#39;{msg} failed: {e!s}&#39;)
                    error = error or e
                else:
                    self.log_info(F&#39;{msg} ok, remaining data:&#39;, rest, clip=True)
                    count += 1
                    error = None
                    modes = [mode]
                    break
            if error or not rest or len(rest) == len(view):
                break
            if len(rest) &gt; len(view):
                raise RuntimeError(&#39;Decompressor returned more tail data than input data.&#39;)
            view = rest
        if count &lt;= 0:
            raise error or ValueError(&#39;Could not detect any zlib stream.&#39;)
        if rest:
            from refinery.lib.meta import SizeInt
            size = SizeInt(len(rest))
            raise RefineryPartialResult(F&#39;{size!r} excess data after compressed stream&#39;, rest)

    def reverse(self, data):
        mode = -self.args.window
        if self.args.zlib_header:
            mode = -mode
        if self.args.gzip_header:
            mode = -mode | 0x10
        self.log_debug(F&#39;using mode {mode:+2d} for compression&#39;)
        zl = zlib.compressobj(self.args.level, zlib.DEFLATED, mode)
        zz = zl.compress(data)
        return zz + zl.flush(zlib.Z_FINISH)

    @classmethod
    def handles(cls, data):
        for sig in (
            B&#39;\x1F\x8B&#39;,  # gzip header
            B&#39;\x78\x01&#39;,  # zlib low compression
            B&#39;\x78\x9C&#39;,  # zlib medium compression
            B&#39;\x78\xDA&#39;,  # zlib high compression
        ):
            if data[:2] == sig:
                return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.zstd"><code class="flex name class">
<span>class <span class="ident">zstd</span></span>
</code></dt>
<dd>
<section class="desc"><p>ZStandard (ZSTD) compression and decompression.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/2c887cef6f2560025a1ac871b535f0d84fa3f321/refinery/units/compression/zstd.py#L6-L28" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class zstd(Unit):
    &#34;&#34;&#34;
    ZStandard (ZSTD) compression and decompression.
    &#34;&#34;&#34;
    @Unit.Requires(&#39;pyzstd&#39;, [&#39;all&#39;])
    def _pyzstd():
        import pyzstd
        return pyzstd

    def process(self, data):
        zd = self._pyzstd.ZstdDecompressor()
        out = zd.decompress(data)
        if zd.needs_input:
            raise RefineryPartialResult(&#39;Incomplete ZSTD stream.&#39;, out)
        return out

    def reverse(self, data):
        zc = self._pyzstd.ZstdCompressor()
        return zc.compress(data) + zc.flush()

    @classmethod
    def handles(cls, data) -&gt; bool:
        return data[:4] == B&#39;\x28\xB5\x2F\xFD&#39;</code></pre>
</details>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#shell-like-unit-interface">Shell-Like Unit Interface</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="refinery" href="index.html">refinery</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Units</a></h3>
<ul>
<li>
<h4><code><a title="refinery.shell.a3x" href="#refinery.shell.a3x">a3x</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.a85" href="#refinery.shell.a85">a85</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.add" href="#refinery.shell.add">add</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.adler32" href="#refinery.shell.adler32">adler32</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.aes" href="#refinery.shell.aes">aes</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.alu" href="#refinery.shell.alu">alu</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.aplib" href="#refinery.shell.aplib">aplib</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.argon2id" href="#refinery.shell.argon2id">argon2id</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.asm" href="#refinery.shell.asm">asm</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.atbash" href="#refinery.shell.atbash">atbash</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.autoxor" href="#refinery.shell.autoxor">autoxor</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.b2f" href="#refinery.shell.b2f">b2f</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.b32" href="#refinery.shell.b32">b32</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.b58" href="#refinery.shell.b58">b58</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.b62" href="#refinery.shell.b62">b62</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.b64" href="#refinery.shell.b64">b64</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.b65536" href="#refinery.shell.b65536">b65536</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.b85" href="#refinery.shell.b85">b85</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.b92" href="#refinery.shell.b92">b92</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.base" href="#refinery.shell.base">base</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.bat" href="#refinery.shell.bat">bat</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.bitrev" href="#refinery.shell.bitrev">bitrev</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.bitsnip" href="#refinery.shell.bitsnip">bitsnip</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.blabla" href="#refinery.shell.blabla">blabla</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.blowfish" href="#refinery.shell.blowfish">blowfish</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.blz" href="#refinery.shell.blz">blz</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.brotli" href="#refinery.shell.brotli">brotli</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.bruteforce" href="#refinery.shell.bruteforce">bruteforce</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.byteswap" href="#refinery.shell.byteswap">byteswap</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.bz2" href="#refinery.shell.bz2">bz2</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.camellia" href="#refinery.shell.camellia">camellia</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.carve" href="#refinery.shell.carve">carve</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.carve_7z" href="#refinery.shell.carve_7z">carve_7z</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.carve_der" href="#refinery.shell.carve_der">carve_der</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.carve_json" href="#refinery.shell.carve_json">carve_json</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.carve_lnk" href="#refinery.shell.carve_lnk">carve_lnk</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.carve_pe" href="#refinery.shell.carve_pe">carve_pe</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.carve_rtf" href="#refinery.shell.carve_rtf">carve_rtf</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.carve_xml" href="#refinery.shell.carve_xml">carve_xml</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.carve_zip" href="#refinery.shell.carve_zip">carve_zip</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.cast" href="#refinery.shell.cast">cast</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.cca" href="#refinery.shell.cca">cca</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ccp" href="#refinery.shell.ccp">ccp</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.chacha" href="#refinery.shell.chacha">chacha</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.chacha20" href="#refinery.shell.chacha20">chacha20</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.chacha20poly1305" href="#refinery.shell.chacha20poly1305">chacha20poly1305</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.chaskey" href="#refinery.shell.chaskey">chaskey</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.chop" href="#refinery.shell.chop">chop</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.clower" href="#refinery.shell.clower">clower</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.cm" href="#refinery.shell.cm">cm</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.codebook" href="#refinery.shell.codebook">codebook</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.cp1252" href="#refinery.shell.cp1252">cp1252</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.crc32" href="#refinery.shell.crc32">crc32</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.csb" href="#refinery.shell.csb">csb</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.csd" href="#refinery.shell.csd">csd</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.csv" href="#refinery.shell.csv">csv</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.cswap" href="#refinery.shell.cswap">cswap</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.cupper" href="#refinery.shell.cupper">cupper</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.d2p" href="#refinery.shell.d2p">d2p</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.datefix" href="#refinery.shell.datefix">datefix</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.decompress" href="#refinery.shell.decompress">decompress</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dedup" href="#refinery.shell.dedup">dedup</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.defang" href="#refinery.shell.defang">defang</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_js_arithmetic" href="#refinery.shell.deob_js_arithmetic">deob_js_arithmetic</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_js_arrays" href="#refinery.shell.deob_js_arrays">deob_js_arrays</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_js_comments" href="#refinery.shell.deob_js_comments">deob_js_comments</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_js_concat" href="#refinery.shell.deob_js_concat">deob_js_concat</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_js_getattr" href="#refinery.shell.deob_js_getattr">deob_js_getattr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_js_tuples" href="#refinery.shell.deob_js_tuples">deob_js_tuples</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1" href="#refinery.shell.deob_ps1">deob_ps1</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_b64convert" href="#refinery.shell.deob_ps1_b64convert">deob_ps1_b64convert</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_brackets" href="#refinery.shell.deob_ps1_brackets">deob_ps1_brackets</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_cases" href="#refinery.shell.deob_ps1_cases">deob_ps1_cases</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_concat" href="#refinery.shell.deob_ps1_concat">deob_ps1_concat</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_encodings" href="#refinery.shell.deob_ps1_encodings">deob_ps1_encodings</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_escape" href="#refinery.shell.deob_ps1_escape">deob_ps1_escape</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_format" href="#refinery.shell.deob_ps1_format">deob_ps1_format</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_invoke" href="#refinery.shell.deob_ps1_invoke">deob_ps1_invoke</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_secstr" href="#refinery.shell.deob_ps1_secstr">deob_ps1_secstr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_stringreplace" href="#refinery.shell.deob_ps1_stringreplace">deob_ps1_stringreplace</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_typecast" href="#refinery.shell.deob_ps1_typecast">deob_ps1_typecast</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_uncurly" href="#refinery.shell.deob_ps1_uncurly">deob_ps1_uncurly</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_vba" href="#refinery.shell.deob_vba">deob_vba</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_vba_arithmetic" href="#refinery.shell.deob_vba_arithmetic">deob_vba_arithmetic</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_vba_brackets" href="#refinery.shell.deob_vba_brackets">deob_vba_brackets</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_vba_char_function" href="#refinery.shell.deob_vba_char_function">deob_vba_char_function</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_vba_chr_literals" href="#refinery.shell.deob_vba_chr_literals">deob_vba_chr_literals</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_vba_comments" href="#refinery.shell.deob_vba_comments">deob_vba_comments</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_vba_concat" href="#refinery.shell.deob_vba_concat">deob_vba_concat</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_vba_constants" href="#refinery.shell.deob_vba_constants">deob_vba_constants</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_vba_dummy_variables" href="#refinery.shell.deob_vba_dummy_variables">deob_vba_dummy_variables</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_vba_stringreplace" href="#refinery.shell.deob_vba_stringreplace">deob_vba_stringreplace</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_vba_stringreverse" href="#refinery.shell.deob_vba_stringreverse">deob_vba_stringreverse</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.des" href="#refinery.shell.des">des</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.des3" href="#refinery.shell.des3">des3</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deskd" href="#refinery.shell.deskd">deskd</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dexstr" href="#refinery.shell.dexstr">dexstr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.djb2" href="#refinery.shell.djb2">djb2</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dnarrays" href="#refinery.shell.dnarrays">dnarrays</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dnasm" href="#refinery.shell.dnasm">dnasm</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dnblob" href="#refinery.shell.dnblob">dnblob</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dnds" href="#refinery.shell.dnds">dnds</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dnfields" href="#refinery.shell.dnfields">dnfields</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dnhdr" href="#refinery.shell.dnhdr">dnhdr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dnmr" href="#refinery.shell.dnmr">dnmr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dnopc" href="#refinery.shell.dnopc">dnopc</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dnrc" href="#refinery.shell.dnrc">dnrc</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dnsdomain" href="#refinery.shell.dnsdomain">dnsdomain</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dnsfx" href="#refinery.shell.dnsfx">dnsfx</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dnstr" href="#refinery.shell.dnstr">dnstr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.docmeta" href="#refinery.shell.docmeta">docmeta</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.doctxt" href="#refinery.shell.doctxt">doctxt</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.drp" href="#refinery.shell.drp">drp</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dsjava" href="#refinery.shell.dsjava">dsjava</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dsphp" href="#refinery.shell.dsphp">dsphp</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dump" href="#refinery.shell.dump">dump</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.eat" href="#refinery.shell.eat">eat</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ef" href="#refinery.shell.ef">ef</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.emit" href="#refinery.shell.emit">emit</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.esc" href="#refinery.shell.esc">esc</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.escps" href="#refinery.shell.escps">escps</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.escvb" href="#refinery.shell.escvb">escvb</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.evtx" href="#refinery.shell.evtx">evtx</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.fernet" href="#refinery.shell.fernet">fernet</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.flz" href="#refinery.shell.flz">flz</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.gost" href="#refinery.shell.gost">gost</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.group" href="#refinery.shell.group">group</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.groupby" href="#refinery.shell.groupby">groupby</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.hc128" href="#refinery.shell.hc128">hc128</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.hc256" href="#refinery.shell.hc256">hc256</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.hex" href="#refinery.shell.hex">hex</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.hexload" href="#refinery.shell.hexload">hexload</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.hkdf" href="#refinery.shell.hkdf">hkdf</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.hmac" href="#refinery.shell.hmac">hmac</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.htmlesc" href="#refinery.shell.htmlesc">htmlesc</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.httprequest" href="#refinery.shell.httprequest">httprequest</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.httpresponse" href="#refinery.shell.httpresponse">httpresponse</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.iemap" href="#refinery.shell.iemap">iemap</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.iff" href="#refinery.shell.iff">iff</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.iffc" href="#refinery.shell.iffc">iffc</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.iffp" href="#refinery.shell.iffp">iffp</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.iffs" href="#refinery.shell.iffs">iffs</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.iffx" href="#refinery.shell.iffx">iffx</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ifps" href="#refinery.shell.ifps">ifps</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ifpsstr" href="#refinery.shell.ifpsstr">ifpsstr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.imgdb" href="#refinery.shell.imgdb">imgdb</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.imgto" href="#refinery.shell.imgto">imgto</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.imgtp" href="#refinery.shell.imgtp">imgtp</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.imphash" href="#refinery.shell.imphash">imphash</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.innopwd" href="#refinery.shell.innopwd">innopwd</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.isaac" href="#refinery.shell.isaac">isaac</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.jamv" href="#refinery.shell.jamv">jamv</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.jcalg" href="#refinery.shell.jcalg">jcalg</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.jvdasm" href="#refinery.shell.jvdasm">jvdasm</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.jvstr" href="#refinery.shell.jvstr">jvstr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.kblob" href="#refinery.shell.kblob">kblob</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.keccak" href="#refinery.shell.keccak">keccak</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.kramer" href="#refinery.shell.kramer">kramer</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.lnk" href="#refinery.shell.lnk">lnk</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.loop" href="#refinery.shell.loop">loop</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.lz4" href="#refinery.shell.lz4">lz4</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.lzf" href="#refinery.shell.lzf">lzf</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.lzg" href="#refinery.shell.lzg">lzg</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.lzip" href="#refinery.shell.lzip">lzip</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.lzjb" href="#refinery.shell.lzjb">lzjb</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.lzma" href="#refinery.shell.lzma">lzma</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.lznt1" href="#refinery.shell.lznt1">lznt1</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.lzo" href="#refinery.shell.lzo">lzo</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.lzw" href="#refinery.shell.lzw">lzw</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.lzx" href="#refinery.shell.lzx">lzx</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.m2h" href="#refinery.shell.m2h">m2h</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.m2h64a" href="#refinery.shell.m2h64a">m2h64a</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.m2h64b" href="#refinery.shell.m2h64b">m2h64b</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.m2ha" href="#refinery.shell.m2ha">m2ha</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.m3h" href="#refinery.shell.m3h">m3h</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.m3h32" href="#refinery.shell.m3h32">m3h32</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.m3h64" href="#refinery.shell.m3h64">m3h64</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.machometa" href="#refinery.shell.machometa">machometa</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.map" href="#refinery.shell.map">map</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.maru" href="#refinery.shell.maru">maru</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.max_" href="#refinery.shell.max_">max_</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.md2" href="#refinery.shell.md2">md2</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.md4" href="#refinery.shell.md4">md4</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.mimewords" href="#refinery.shell.mimewords">mimewords</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.min_" href="#refinery.shell.min_">min_</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.morse" href="#refinery.shell.morse">morse</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.mscdk" href="#refinery.shell.mscdk">mscdk</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.mscf" href="#refinery.shell.mscf">mscf</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.msgpack" href="#refinery.shell.msgpack">msgpack</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.mspdb" href="#refinery.shell.mspdb">mspdb</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.mvg" href="#refinery.shell.mvg">mvg</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.n40" href="#refinery.shell.n40">n40</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.neg" href="#refinery.shell.neg">neg</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.netbios" href="#refinery.shell.netbios">netbios</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ngrams" href="#refinery.shell.ngrams">ngrams</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.nop" href="#refinery.shell.nop">nop</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.nrv2b" href="#refinery.shell.nrv2b">nrv2b</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.nrv2d" href="#refinery.shell.nrv2d">nrv2d</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.nrv2e" href="#refinery.shell.nrv2e">nrv2e</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ntlm" href="#refinery.shell.ntlm">ntlm</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.officecrypt" href="#refinery.shell.officecrypt">officecrypt</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.opc" href="#refinery.shell.opc">opc</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.p1" href="#refinery.shell.p1">p1</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.p2" href="#refinery.shell.p2">p2</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.p3" href="#refinery.shell.p3">p3</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pack" href="#refinery.shell.pack">pack</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pad" href="#refinery.shell.pad">pad</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pbkdf1" href="#refinery.shell.pbkdf1">pbkdf1</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pbkdf2" href="#refinery.shell.pbkdf2">pbkdf2</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pbuf" href="#refinery.shell.pbuf">pbuf</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pcap" href="#refinery.shell.pcap">pcap</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pcap_http" href="#refinery.shell.pcap_http">pcap_http</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pdfcrypt" href="#refinery.shell.pdfcrypt">pdfcrypt</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pecdb" href="#refinery.shell.pecdb">pecdb</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pedebloat" href="#refinery.shell.pedebloat">pedebloat</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.peek" href="#refinery.shell.peek">peek</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pefix" href="#refinery.shell.pefix">pefix</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pemeta" href="#refinery.shell.pemeta">pemeta</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.peoverlay" href="#refinery.shell.peoverlay">peoverlay</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.perc" href="#refinery.shell.perc">perc</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pesig" href="#refinery.shell.pesig">pesig</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pestrip" href="#refinery.shell.pestrip">pestrip</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pf" href="#refinery.shell.pf">pf</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pick" href="#refinery.shell.pick">pick</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pkcs7" href="#refinery.shell.pkcs7">pkcs7</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pkcs7sig" href="#refinery.shell.pkcs7sig">pkcs7sig</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pkw" href="#refinery.shell.pkw">pkw</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pop" href="#refinery.shell.pop">pop</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ppjscript" href="#refinery.shell.ppjscript">ppjscript</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ppjson" href="#refinery.shell.ppjson">ppjson</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ppxml" href="#refinery.shell.ppxml">ppxml</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.push" href="#refinery.shell.push">push</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.put" href="#refinery.shell.put">put</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pyc" href="#refinery.shell.pyc">pyc</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pym" href="#refinery.shell.pym">pym</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pymstr" href="#refinery.shell.pymstr">pymstr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.qb" href="#refinery.shell.qb">qb</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.qf" href="#refinery.shell.qf">qf</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.qlz" href="#refinery.shell.qlz">qlz</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.qr" href="#refinery.shell.qr">qr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rabbit" href="#refinery.shell.rabbit">rabbit</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rc2" href="#refinery.shell.rc2">rc2</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rc4" href="#refinery.shell.rc4">rc4</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rc4mod" href="#refinery.shell.rc4mod">rc4mod</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rc5" href="#refinery.shell.rc5">rc5</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rc6" href="#refinery.shell.rc6">rc6</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.recode" href="#refinery.shell.recode">recode</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.reduce" href="#refinery.shell.reduce">reduce</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rep" href="#refinery.shell.rep">rep</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.repl" href="#refinery.shell.repl">repl</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.resplit" href="#refinery.shell.resplit">resplit</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.resub" href="#refinery.shell.resub">resub</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rev" href="#refinery.shell.rev">rev</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rex" href="#refinery.shell.rex">rex</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rijndael" href="#refinery.shell.rijndael">rijndael</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ripemd128" href="#refinery.shell.ripemd128">ripemd128</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ripemd160" href="#refinery.shell.ripemd160">ripemd160</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rmv" href="#refinery.shell.rmv">rmv</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rncrypt" href="#refinery.shell.rncrypt">rncrypt</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rot" href="#refinery.shell.rot">rot</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rotl" href="#refinery.shell.rotl">rotl</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rotr" href="#refinery.shell.rotr">rotr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rsa" href="#refinery.shell.rsa">rsa</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rsakey" href="#refinery.shell.rsakey">rsakey</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rtfc" href="#refinery.shell.rtfc">rtfc</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.run" href="#refinery.shell.run">run</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.salsa" href="#refinery.shell.salsa">salsa</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.salsa20" href="#refinery.shell.salsa20">salsa20</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.scope" href="#refinery.shell.scope">scope</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.seal" href="#refinery.shell.seal">seal</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.secstr" href="#refinery.shell.secstr">secstr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.sep" href="#refinery.shell.sep">sep</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.serpent" href="#refinery.shell.serpent">serpent</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.shl" href="#refinery.shell.shl">shl</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.shr" href="#refinery.shell.shr">shr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.sm4" href="#refinery.shell.sm4">sm4</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.snip" href="#refinery.shell.snip">snip</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.sorted" href="#refinery.shell.sorted">sorted</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.sosemanuk" href="#refinery.shell.sosemanuk">sosemanuk</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.speck" href="#refinery.shell.speck">speck</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.sqlite" href="#refinery.shell.sqlite">sqlite</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.stego" href="#refinery.shell.stego">stego</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.stretch" href="#refinery.shell.stretch">stretch</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.struct" href="#refinery.shell.struct">struct</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.sub" href="#refinery.shell.sub">sub</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.subfiles" href="#refinery.shell.subfiles">subfiles</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.swap" href="#refinery.shell.swap">swap</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.szdd" href="#refinery.shell.szdd">szdd</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.tea" href="#refinery.shell.tea">tea</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.termfit" href="#refinery.shell.termfit">termfit</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.terminate" href="#refinery.shell.terminate">terminate</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.transpose" href="#refinery.shell.transpose">transpose</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.trim" href="#refinery.shell.trim">trim</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.u16" href="#refinery.shell.u16">u16</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ucrypt" href="#refinery.shell.ucrypt">ucrypt</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.url" href="#refinery.shell.url">url</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.urlfix" href="#refinery.shell.urlfix">urlfix</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.urlguards" href="#refinery.shell.urlguards">urlguards</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.urn" href="#refinery.shell.urn">urn</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.uuenc" href="#refinery.shell.uuenc">uuenc</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.vaddr" href="#refinery.shell.vaddr">vaddr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.vbapc" href="#refinery.shell.vbapc">vbapc</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.vbastr" href="#refinery.shell.vbastr">vbastr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.vigenere" href="#refinery.shell.vigenere">vigenere</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.vmemref" href="#refinery.shell.vmemref">vmemref</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.vsect" href="#refinery.shell.vsect">vsect</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.vsnip" href="#refinery.shell.vsnip">vsnip</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.vstack" href="#refinery.shell.vstack">vstack</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.winreg" href="#refinery.shell.winreg">winreg</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.wshenc" href="#refinery.shell.wshenc">wshenc</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xchacha" href="#refinery.shell.xchacha">xchacha</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xfcc" href="#refinery.shell.xfcc">xfcc</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xj0" href="#refinery.shell.xj0">xj0</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xjl" href="#refinery.shell.xjl">xjl</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xkey" href="#refinery.shell.xkey">xkey</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xlmdeobf" href="#refinery.shell.xlmdeobf">xlmdeobf</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xlxtr" href="#refinery.shell.xlxtr">xlxtr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xor" href="#refinery.shell.xor">xor</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xsalsa" href="#refinery.shell.xsalsa">xsalsa</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xt" href="#refinery.shell.xt">xt</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xt7z" href="#refinery.shell.xt7z">xt7z</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtace" href="#refinery.shell.xtace">xtace</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtasar" href="#refinery.shell.xtasar">xtasar</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtcab" href="#refinery.shell.xtcab">xtcab</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtchm" href="#refinery.shell.xtchm">xtchm</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtcpio" href="#refinery.shell.xtcpio">xtcpio</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtdoc" href="#refinery.shell.xtdoc">xtdoc</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtea" href="#refinery.shell.xtea">xtea</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtgz" href="#refinery.shell.xtgz">xtgz</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xthtml" href="#refinery.shell.xthtml">xthtml</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtinno" href="#refinery.shell.xtinno">xtinno</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtiso" href="#refinery.shell.xtiso">xtiso</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtiss" href="#refinery.shell.xtiss">xtiss</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtjson" href="#refinery.shell.xtjson">xtjson</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtmacho" href="#refinery.shell.xtmacho">xtmacho</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtmagtape" href="#refinery.shell.xtmagtape">xtmagtape</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtmail" href="#refinery.shell.xtmail">xtmail</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtmsi" href="#refinery.shell.xtmsi">xtmsi</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtnode" href="#refinery.shell.xtnode">xtnode</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtnsis" href="#refinery.shell.xtnsis">xtnsis</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtnuitka" href="#refinery.shell.xtnuitka">xtnuitka</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtone" href="#refinery.shell.xtone">xtone</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtp" href="#refinery.shell.xtp">xtp</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtpdf" href="#refinery.shell.xtpdf">xtpdf</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtpyi" href="#refinery.shell.xtpyi">xtpyi</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtrtf" href="#refinery.shell.xtrtf">xtrtf</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtsim" href="#refinery.shell.xtsim">xtsim</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtsql" href="#refinery.shell.xtsql">xtsql</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xttar" href="#refinery.shell.xttar">xttar</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtvba" href="#refinery.shell.xtvba">xtvba</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtw" href="#refinery.shell.xtw">xtw</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtxml" href="#refinery.shell.xtxml">xtxml</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtxs" href="#refinery.shell.xtxs">xtxs</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtzip" href="#refinery.shell.xtzip">xtzip</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtzip.unpack" href="#refinery.shell.xtzip.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtzpaq" href="#refinery.shell.xtzpaq">xtzpaq</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xxh" href="#refinery.shell.xxh">xxh</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xxtea" href="#refinery.shell.xxtea">xxtea</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.z85" href="#refinery.shell.z85">z85</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.zl" href="#refinery.shell.zl">zl</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.zstd" href="#refinery.shell.zstd">zstd</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>
hljs.configure({languages: []})
hljs.initHighlightingOnLoad()
</script>
</body>
</html>
